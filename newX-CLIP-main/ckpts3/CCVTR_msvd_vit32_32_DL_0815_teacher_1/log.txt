2025-08-15 10:51:10,700:INFO: device: cuda:1 n_gpu: 2
2025-08-15 10:51:10,718:INFO: Effective parameters:
2025-08-15 10:51:10,719:INFO:   <<< batch_size: 32
2025-08-15 10:51:10,719:INFO:   <<< batch_size_val: 36
2025-08-15 10:51:10,719:INFO:   <<< cache_dir: 
2025-08-15 10:51:10,719:INFO:   <<< coef_lr: 0.001
2025-08-15 10:51:10,719:INFO:   <<< cross_model: cross-base
2025-08-15 10:51:10,719:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 10:51:10,719:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 10:51:10,719:INFO:   <<< datatype: msvd
2025-08-15 10:51:10,719:INFO:   <<< do_eval: False
2025-08-15 10:51:10,719:INFO:   <<< do_lower_case: False
2025-08-15 10:51:10,719:INFO:   <<< do_pretrain: False
2025-08-15 10:51:10,719:INFO:   <<< do_train: True
2025-08-15 10:51:10,719:INFO:   <<< epochs: 1
2025-08-15 10:51:10,719:INFO:   <<< eval_frame_order: 0
2025-08-15 10:51:10,719:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 10:51:10,720:INFO:   <<< feature_framerate: 1
2025-08-15 10:51:10,720:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 10:51:10,720:INFO:   <<< fp16: False
2025-08-15 10:51:10,720:INFO:   <<< fp16_opt_level: O1
2025-08-15 10:51:10,720:INFO:   <<< freeze_layer_num: 9
2025-08-15 10:51:10,720:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 10:51:10,720:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 10:51:10,720:INFO:   <<< init_model: None
2025-08-15 10:51:10,720:INFO:   <<< linear_patch: 2d
2025-08-15 10:51:10,720:INFO:   <<< local_rank: 0
2025-08-15 10:51:10,720:INFO:   <<< loose_type: True
2025-08-15 10:51:10,720:INFO:   <<< lr: 0.0001
2025-08-15 10:51:10,720:INFO:   <<< lr_decay: 0.9
2025-08-15 10:51:10,720:INFO:   <<< margin: 0.1
2025-08-15 10:51:10,720:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 10:51:10,720:INFO:   <<< max_frames: 12
2025-08-15 10:51:10,720:INFO:   <<< max_words: 32
2025-08-15 10:51:10,720:INFO:   <<< n_display: 5
2025-08-15 10:51:10,720:INFO:   <<< n_gpu: 1
2025-08-15 10:51:10,721:INFO:   <<< n_pair: 1
2025-08-15 10:51:10,721:INFO:   <<< negative_weighting: 1
2025-08-15 10:51:10,721:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 10:51:10,721:INFO:   <<< num_thread_reader: 4
2025-08-15 10:51:10,721:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 10:51:10,721:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 10:51:10,721:INFO:   <<< rank: 0
2025-08-15 10:51:10,721:INFO:   <<< resume_model: None
2025-08-15 10:51:10,721:INFO:   <<< sampled_use_mil: False
2025-08-15 10:51:10,721:INFO:   <<< seed: 42
2025-08-15 10:51:10,721:INFO:   <<< sim_header: meanP
2025-08-15 10:51:10,721:INFO:   <<< slice_framepos: 0
2025-08-15 10:51:10,721:INFO:   <<< task_type: retrieval
2025-08-15 10:51:10,721:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 10:51:10,721:INFO:   <<< train_csv: data/.train.csv
2025-08-15 10:51:10,721:INFO:   <<< train_frame_order: 0
2025-08-15 10:51:10,721:INFO:   <<< use_mil: False
2025-08-15 10:51:10,721:INFO:   <<< val_csv: data/.val.csv
2025-08-15 10:51:10,722:INFO:   <<< video_dim: 1024
2025-08-15 10:51:10,722:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 10:51:10,722:INFO:   <<< warmup_proportion: 0.1
2025-08-15 10:51:10,722:INFO:   <<< world_size: 2
2025-08-15 10:51:10,722:INFO: device: cuda:0 n_gpu: 2
2025-08-15 10:51:11,649:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 10:51:11,650:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 10:51:11,650:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 10:51:11,650:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 10:51:11,650:WARNING: Test retrieval by loose type.
2025-08-15 10:51:11,650:WARNING: 	 embed_dim: 512
2025-08-15 10:51:11,650:WARNING: 	 image_resolution: 224
2025-08-15 10:51:11,651:WARNING: 	 vision_layers: 12
2025-08-15 10:51:11,651:WARNING: 	 vision_width: 768
2025-08-15 10:51:11,651:WARNING: 	 vision_patch_size: 32
2025-08-15 10:51:11,651:WARNING: 	 context_length: 77
2025-08-15 10:51:11,651:WARNING: 	 vocab_size: 49408
2025-08-15 10:51:11,651:WARNING: 	 transformer_width: 512
2025-08-15 10:51:11,651:WARNING: 	 transformer_heads: 8
2025-08-15 10:51:11,651:WARNING: 	 transformer_layers: 12
2025-08-15 10:51:11,651:WARNING: 		 linear_patch: 2d
2025-08-15 10:51:11,651:WARNING: 	 cut_top_layer: 0
2025-08-15 10:51:13,340:WARNING: 	 sim_header: meanP
2025-08-15 10:51:18,159:INFO: --------------------
2025-08-15 10:51:18,160:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_i.bias
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_m.bias
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.patch_embed_r.bias
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_i.bias
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_m.bias
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm1_r.bias
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm2_i.bias
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.bias
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.0.bias
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.0.ffn.2.bias
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_i.bias
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_m.bias
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm1_r.bias
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm2_i.bias
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.bias
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.0.bias
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.blocks.1.ffn.2.bias
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.norm.bias
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
   teacher.temporal_fusion.gating_module.0.weight
   teacher.temporal_fusion.gating_module.0.bias
   teacher.temporal_fusion.gating_module.2.weight
   teacher.temporal_fusion.gating_module.2.bias
2025-08-15 10:51:18,160:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 10:51:20,823:INFO: ***** Running test *****
2025-08-15 10:51:20,823:INFO:   Num examples = 27763
2025-08-15 10:51:20,823:INFO:   Batch size = 36
2025-08-15 10:51:20,823:INFO:   Num steps = 772
2025-08-15 10:51:20,823:INFO: ***** Running val *****
2025-08-15 10:51:20,823:INFO:   Num examples = 4290
2025-08-15 10:51:21,361:INFO: ***** Running training *****
2025-08-15 10:51:21,361:INFO:   Num examples = 48774
2025-08-15 10:51:21,361:INFO:   Batch size = 32
2025-08-15 10:51:21,361:INFO:   Num steps = 1524
2025-08-15 10:51:44,553:INFO: Epoch: 1/1, Step: 5/1524, Lr: , Loss: 2.261402, Time/step: 4.631103
2025-08-15 10:51:56,241:INFO: Epoch: 1/1, Step: 10/1524, Lr: , Loss: 1.966665, Time/step: 2.337293
2025-08-15 10:52:09,794:INFO: Epoch: 1/1, Step: 15/1524, Lr: , Loss: 2.127441, Time/step: 2.710604
2025-08-15 10:52:22,005:INFO: Epoch: 1/1, Step: 20/1524, Lr: , Loss: 1.852578, Time/step: 2.441937
2025-08-15 10:52:34,562:INFO: Epoch: 1/1, Step: 25/1524, Lr: , Loss: 2.052947, Time/step: 2.511406
2025-08-15 10:52:44,866:INFO: Epoch: 1/1, Step: 30/1524, Lr: , Loss: 1.775782, Time/step: 2.060578
2025-08-15 10:53:05,698:INFO: Epoch: 1/1, Step: 35/1524, Lr: , Loss: 1.586184, Time/step: 4.166324
2025-08-15 10:53:36,041:INFO: Epoch: 1/1, Step: 40/1524, Lr: , Loss: 1.641186, Time/step: 6.068519
2025-08-15 10:53:57,839:INFO: Epoch: 1/1, Step: 45/1524, Lr: , Loss: 1.293076, Time/step: 4.359420
2025-08-15 10:54:10,275:INFO: Epoch: 1/1, Step: 50/1524, Lr: , Loss: 1.303648, Time/step: 2.487135
2025-08-15 10:54:33,562:INFO: Epoch: 1/1, Step: 55/1524, Lr: , Loss: 1.128652, Time/step: 4.657258
2025-08-15 10:55:06,439:INFO: device: cuda:1 n_gpu: 2
2025-08-15 10:55:06,495:INFO: Effective parameters:
2025-08-15 10:55:06,495:INFO:   <<< batch_size: 64
2025-08-15 10:55:06,495:INFO:   <<< batch_size_val: 32
2025-08-15 10:55:06,495:INFO:   <<< cache_dir: 
2025-08-15 10:55:06,495:INFO:   <<< coef_lr: 0.001
2025-08-15 10:55:06,495:INFO:   <<< cross_model: cross-base
2025-08-15 10:55:06,495:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 10:55:06,496:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 10:55:06,496:INFO:   <<< datatype: msvd
2025-08-15 10:55:06,496:INFO:   <<< do_eval: False
2025-08-15 10:55:06,496:INFO:   <<< do_lower_case: False
2025-08-15 10:55:06,496:INFO:   <<< do_pretrain: False
2025-08-15 10:55:06,496:INFO:   <<< do_train: True
2025-08-15 10:55:06,496:INFO:   <<< epochs: 1
2025-08-15 10:55:06,496:INFO:   <<< eval_frame_order: 0
2025-08-15 10:55:06,496:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 10:55:06,496:INFO:   <<< feature_framerate: 1
2025-08-15 10:55:06,497:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 10:55:06,497:INFO:   <<< fp16: False
2025-08-15 10:55:06,497:INFO:   <<< fp16_opt_level: O1
2025-08-15 10:55:06,497:INFO:   <<< freeze_layer_num: 9
2025-08-15 10:55:06,497:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 10:55:06,497:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 10:55:06,497:INFO:   <<< init_model: None
2025-08-15 10:55:06,497:INFO:   <<< linear_patch: 2d
2025-08-15 10:55:06,497:INFO:   <<< local_rank: 0
2025-08-15 10:55:06,497:INFO:   <<< loose_type: True
2025-08-15 10:55:06,498:INFO:   <<< lr: 0.0001
2025-08-15 10:55:06,498:INFO:   <<< lr_decay: 0.9
2025-08-15 10:55:06,498:INFO:   <<< margin: 0.1
2025-08-15 10:55:06,498:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 10:55:06,498:INFO:   <<< max_frames: 12
2025-08-15 10:55:06,498:INFO:   <<< max_words: 32
2025-08-15 10:55:06,498:INFO:   <<< n_display: 5
2025-08-15 10:55:06,498:INFO:   <<< n_gpu: 1
2025-08-15 10:55:06,498:INFO:   <<< n_pair: 1
2025-08-15 10:55:06,498:INFO:   <<< negative_weighting: 1
2025-08-15 10:55:06,498:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 10:55:06,499:INFO:   <<< num_thread_reader: 4
2025-08-15 10:55:06,499:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 10:55:06,499:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 10:55:06,499:INFO:   <<< rank: 0
2025-08-15 10:55:06,499:INFO:   <<< resume_model: None
2025-08-15 10:55:06,499:INFO:   <<< sampled_use_mil: False
2025-08-15 10:55:06,499:INFO:   <<< seed: 42
2025-08-15 10:55:06,499:INFO:   <<< sim_header: meanP
2025-08-15 10:55:06,499:INFO:   <<< slice_framepos: 0
2025-08-15 10:55:06,499:INFO:   <<< task_type: retrieval
2025-08-15 10:55:06,499:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 10:55:06,500:INFO:   <<< train_csv: data/.train.csv
2025-08-15 10:55:06,500:INFO:   <<< train_frame_order: 0
2025-08-15 10:55:06,500:INFO:   <<< use_mil: False
2025-08-15 10:55:06,500:INFO:   <<< val_csv: data/.val.csv
2025-08-15 10:55:06,500:INFO:   <<< video_dim: 1024
2025-08-15 10:55:06,500:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 10:55:06,500:INFO:   <<< warmup_proportion: 0.1
2025-08-15 10:55:06,500:INFO:   <<< world_size: 2
2025-08-15 10:55:06,500:INFO: device: cuda:0 n_gpu: 2
2025-08-15 10:55:07,391:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 10:55:07,392:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 10:55:07,392:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 10:55:07,392:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 10:55:07,392:WARNING: Test retrieval by loose type.
2025-08-15 10:55:07,392:WARNING: 	 embed_dim: 512
2025-08-15 10:55:07,392:WARNING: 	 image_resolution: 224
2025-08-15 10:55:07,392:WARNING: 	 vision_layers: 12
2025-08-15 10:55:07,392:WARNING: 	 vision_width: 768
2025-08-15 10:55:07,392:WARNING: 	 vision_patch_size: 32
2025-08-15 10:55:07,392:WARNING: 	 context_length: 77
2025-08-15 10:55:07,393:WARNING: 	 vocab_size: 49408
2025-08-15 10:55:07,393:WARNING: 	 transformer_width: 512
2025-08-15 10:55:07,393:WARNING: 	 transformer_heads: 8
2025-08-15 10:55:07,393:WARNING: 	 transformer_layers: 12
2025-08-15 10:55:07,393:WARNING: 		 linear_patch: 2d
2025-08-15 10:55:07,393:WARNING: 	 cut_top_layer: 0
2025-08-15 10:55:09,060:WARNING: 	 sim_header: meanP
2025-08-15 10:55:13,542:INFO: --------------------
2025-08-15 10:55:13,543:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_i.bias
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_m.bias
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.patch_embed_r.bias
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_i.bias
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_m.bias
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm1_r.bias
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm2_i.bias
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.bias
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.0.bias
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.0.ffn.2.bias
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_i.bias
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_m.bias
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm1_r.bias
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm2_i.bias
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.bias
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.0.bias
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.blocks.1.ffn.2.bias
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.norm.bias
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
   teacher.temporal_fusion.gating_module.0.weight
   teacher.temporal_fusion.gating_module.0.bias
   teacher.temporal_fusion.gating_module.2.weight
   teacher.temporal_fusion.gating_module.2.bias
2025-08-15 10:55:13,543:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 10:55:16,098:INFO: ***** Running test *****
2025-08-15 10:55:16,098:INFO:   Num examples = 27763
2025-08-15 10:55:16,098:INFO:   Batch size = 32
2025-08-15 10:55:16,098:INFO:   Num steps = 868
2025-08-15 10:55:16,098:INFO: ***** Running val *****
2025-08-15 10:55:16,098:INFO:   Num examples = 4290
2025-08-15 10:55:16,547:INFO: ***** Running training *****
2025-08-15 10:55:16,547:INFO:   Num examples = 48774
2025-08-15 10:55:16,547:INFO:   Batch size = 64
2025-08-15 10:55:16,547:INFO:   Num steps = 762
2025-08-15 10:56:04,350:INFO: Epoch: 1/1, Step: 5/762, Lr: , Loss: 2.713245, Time/step: 9.553239
2025-08-15 10:56:28,608:INFO: Epoch: 1/1, Step: 10/762, Lr: , Loss: 2.628470, Time/step: 4.851561
2025-08-15 10:57:29,432:INFO: Effective parameters:
2025-08-15 10:57:29,432:INFO:   <<< batch_size: 128
2025-08-15 10:57:29,432:INFO:   <<< batch_size_val: 32
2025-08-15 10:57:29,432:INFO:   <<< cache_dir: 
2025-08-15 10:57:29,432:INFO:   <<< coef_lr: 0.001
2025-08-15 10:57:29,432:INFO:   <<< cross_model: cross-base
2025-08-15 10:57:29,432:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 10:57:29,432:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 10:57:29,432:INFO:   <<< datatype: msvd
2025-08-15 10:57:29,432:INFO:   <<< do_eval: False
2025-08-15 10:57:29,432:INFO:   <<< do_lower_case: False
2025-08-15 10:57:29,432:INFO:   <<< do_pretrain: False
2025-08-15 10:57:29,433:INFO:   <<< do_train: True
2025-08-15 10:57:29,433:INFO:   <<< epochs: 1
2025-08-15 10:57:29,433:INFO:   <<< eval_frame_order: 0
2025-08-15 10:57:29,433:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 10:57:29,433:INFO:   <<< feature_framerate: 1
2025-08-15 10:57:29,433:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 10:57:29,433:INFO:   <<< fp16: False
2025-08-15 10:57:29,433:INFO:   <<< fp16_opt_level: O1
2025-08-15 10:57:29,433:INFO:   <<< freeze_layer_num: 9
2025-08-15 10:57:29,433:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 10:57:29,433:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 10:57:29,433:INFO:   <<< init_model: None
2025-08-15 10:57:29,433:INFO:   <<< linear_patch: 2d
2025-08-15 10:57:29,433:INFO:   <<< local_rank: 0
2025-08-15 10:57:29,433:INFO:   <<< loose_type: True
2025-08-15 10:57:29,433:INFO:   <<< lr: 0.0001
2025-08-15 10:57:29,433:INFO:   <<< lr_decay: 0.9
2025-08-15 10:57:29,433:INFO:   <<< margin: 0.1
2025-08-15 10:57:29,433:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 10:57:29,434:INFO:   <<< max_frames: 12
2025-08-15 10:57:29,434:INFO:   <<< max_words: 32
2025-08-15 10:57:29,434:INFO:   <<< n_display: 5
2025-08-15 10:57:29,434:INFO:   <<< n_gpu: 1
2025-08-15 10:57:29,434:INFO:   <<< n_pair: 1
2025-08-15 10:57:29,434:INFO:   <<< negative_weighting: 1
2025-08-15 10:57:29,434:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 10:57:29,434:INFO:   <<< num_thread_reader: 4
2025-08-15 10:57:29,434:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 10:57:29,434:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 10:57:29,434:INFO:   <<< rank: 0
2025-08-15 10:57:29,434:INFO:   <<< resume_model: None
2025-08-15 10:57:29,434:INFO:   <<< sampled_use_mil: False
2025-08-15 10:57:29,434:INFO:   <<< seed: 42
2025-08-15 10:57:29,434:INFO:   <<< sim_header: meanP
2025-08-15 10:57:29,434:INFO:   <<< slice_framepos: 0
2025-08-15 10:57:29,434:INFO:   <<< task_type: retrieval
2025-08-15 10:57:29,434:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 10:57:29,435:INFO:   <<< train_csv: data/.train.csv
2025-08-15 10:57:29,435:INFO:   <<< train_frame_order: 0
2025-08-15 10:57:29,435:INFO:   <<< use_mil: False
2025-08-15 10:57:29,435:INFO:   <<< val_csv: data/.val.csv
2025-08-15 10:57:29,435:INFO:   <<< video_dim: 1024
2025-08-15 10:57:29,435:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 10:57:29,435:INFO:   <<< warmup_proportion: 0.1
2025-08-15 10:57:29,435:INFO:   <<< world_size: 2
2025-08-15 10:57:29,435:INFO: device: cuda:0 n_gpu: 2
2025-08-15 10:57:29,445:INFO: device: cuda:1 n_gpu: 2
2025-08-15 10:57:30,318:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 10:57:30,318:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 10:57:30,318:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 10:57:30,318:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 10:57:30,319:WARNING: Test retrieval by loose type.
2025-08-15 10:57:30,319:WARNING: 	 embed_dim: 512
2025-08-15 10:57:30,319:WARNING: 	 image_resolution: 224
2025-08-15 10:57:30,319:WARNING: 	 vision_layers: 12
2025-08-15 10:57:30,319:WARNING: 	 vision_width: 768
2025-08-15 10:57:30,319:WARNING: 	 vision_patch_size: 32
2025-08-15 10:57:30,319:WARNING: 	 context_length: 77
2025-08-15 10:57:30,319:WARNING: 	 vocab_size: 49408
2025-08-15 10:57:30,319:WARNING: 	 transformer_width: 512
2025-08-15 10:57:30,319:WARNING: 	 transformer_heads: 8
2025-08-15 10:57:30,319:WARNING: 	 transformer_layers: 12
2025-08-15 10:57:30,319:WARNING: 		 linear_patch: 2d
2025-08-15 10:57:30,319:WARNING: 	 cut_top_layer: 0
2025-08-15 10:57:31,971:WARNING: 	 sim_header: meanP
2025-08-15 10:57:36,423:INFO: --------------------
2025-08-15 10:57:36,423:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_i.bias
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_m.bias
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.patch_embed_r.bias
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_i.bias
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_m.bias
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm1_r.bias
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm2_i.bias
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.bias
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.0.bias
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.0.ffn.2.bias
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_i.bias
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_m.bias
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm1_r.bias
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm2_i.bias
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.bias
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.0.bias
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.blocks.1.ffn.2.bias
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.norm.bias
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
   teacher.temporal_fusion.gating_module.0.weight
   teacher.temporal_fusion.gating_module.0.bias
   teacher.temporal_fusion.gating_module.2.weight
   teacher.temporal_fusion.gating_module.2.bias
2025-08-15 10:57:36,423:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 10:57:38,618:INFO: ***** Running test *****
2025-08-15 10:57:38,618:INFO:   Num examples = 27763
2025-08-15 10:57:38,619:INFO:   Batch size = 32
2025-08-15 10:57:38,619:INFO:   Num steps = 868
2025-08-15 10:57:38,619:INFO: ***** Running val *****
2025-08-15 10:57:38,619:INFO:   Num examples = 4290
2025-08-15 10:57:39,241:INFO: ***** Running training *****
2025-08-15 10:57:39,241:INFO:   Num examples = 48774
2025-08-15 10:57:39,241:INFO:   Batch size = 128
2025-08-15 10:57:39,241:INFO:   Num steps = 381
2025-08-15 11:00:03,708:INFO: Effective parameters:
2025-08-15 11:00:03,708:INFO:   <<< batch_size: 96
2025-08-15 11:00:03,708:INFO:   <<< batch_size_val: 32
2025-08-15 11:00:03,708:INFO:   <<< cache_dir: 
2025-08-15 11:00:03,708:INFO:   <<< coef_lr: 0.001
2025-08-15 11:00:03,708:INFO:   <<< cross_model: cross-base
2025-08-15 11:00:03,708:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 11:00:03,708:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 11:00:03,709:INFO:   <<< datatype: msvd
2025-08-15 11:00:03,709:INFO:   <<< do_eval: False
2025-08-15 11:00:03,709:INFO:   <<< do_lower_case: False
2025-08-15 11:00:03,709:INFO:   <<< do_pretrain: False
2025-08-15 11:00:03,709:INFO:   <<< do_train: True
2025-08-15 11:00:03,709:INFO:   <<< epochs: 1
2025-08-15 11:00:03,709:INFO:   <<< eval_frame_order: 0
2025-08-15 11:00:03,709:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 11:00:03,709:INFO:   <<< feature_framerate: 1
2025-08-15 11:00:03,709:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 11:00:03,709:INFO:   <<< fp16: False
2025-08-15 11:00:03,709:INFO:   <<< fp16_opt_level: O1
2025-08-15 11:00:03,709:INFO:   <<< freeze_layer_num: 9
2025-08-15 11:00:03,709:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 11:00:03,709:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 11:00:03,709:INFO:   <<< init_model: None
2025-08-15 11:00:03,709:INFO:   <<< linear_patch: 2d
2025-08-15 11:00:03,709:INFO:   <<< local_rank: 0
2025-08-15 11:00:03,709:INFO:   <<< loose_type: True
2025-08-15 11:00:03,709:INFO:   <<< lr: 0.0001
2025-08-15 11:00:03,710:INFO:   <<< lr_decay: 0.9
2025-08-15 11:00:03,710:INFO:   <<< margin: 0.1
2025-08-15 11:00:03,710:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 11:00:03,710:INFO:   <<< max_frames: 12
2025-08-15 11:00:03,710:INFO:   <<< max_words: 32
2025-08-15 11:00:03,710:INFO:   <<< n_display: 5
2025-08-15 11:00:03,710:INFO:   <<< n_gpu: 1
2025-08-15 11:00:03,710:INFO:   <<< n_pair: 1
2025-08-15 11:00:03,710:INFO:   <<< negative_weighting: 1
2025-08-15 11:00:03,710:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 11:00:03,710:INFO:   <<< num_thread_reader: 4
2025-08-15 11:00:03,710:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 11:00:03,710:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 11:00:03,710:INFO:   <<< rank: 0
2025-08-15 11:00:03,710:INFO:   <<< resume_model: None
2025-08-15 11:00:03,710:INFO:   <<< sampled_use_mil: False
2025-08-15 11:00:03,710:INFO:   <<< seed: 42
2025-08-15 11:00:03,710:INFO:   <<< sim_header: meanP
2025-08-15 11:00:03,711:INFO:   <<< slice_framepos: 0
2025-08-15 11:00:03,711:INFO:   <<< task_type: retrieval
2025-08-15 11:00:03,711:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 11:00:03,711:INFO:   <<< train_csv: data/.train.csv
2025-08-15 11:00:03,711:INFO:   <<< train_frame_order: 0
2025-08-15 11:00:03,711:INFO:   <<< use_mil: False
2025-08-15 11:00:03,711:INFO:   <<< val_csv: data/.val.csv
2025-08-15 11:00:03,711:INFO:   <<< video_dim: 1024
2025-08-15 11:00:03,711:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 11:00:03,711:INFO:   <<< warmup_proportion: 0.1
2025-08-15 11:00:03,711:INFO:   <<< world_size: 2
2025-08-15 11:00:03,711:INFO: device: cuda:0 n_gpu: 2
2025-08-15 11:00:03,721:INFO: device: cuda:1 n_gpu: 2
2025-08-15 11:00:04,594:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 11:00:04,595:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 11:00:04,595:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 11:00:04,595:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 11:00:04,595:WARNING: Test retrieval by loose type.
2025-08-15 11:00:04,595:WARNING: 	 embed_dim: 512
2025-08-15 11:00:04,595:WARNING: 	 image_resolution: 224
2025-08-15 11:00:04,595:WARNING: 	 vision_layers: 12
2025-08-15 11:00:04,595:WARNING: 	 vision_width: 768
2025-08-15 11:00:04,595:WARNING: 	 vision_patch_size: 32
2025-08-15 11:00:04,595:WARNING: 	 context_length: 77
2025-08-15 11:00:04,595:WARNING: 	 vocab_size: 49408
2025-08-15 11:00:04,595:WARNING: 	 transformer_width: 512
2025-08-15 11:00:04,595:WARNING: 	 transformer_heads: 8
2025-08-15 11:00:04,595:WARNING: 	 transformer_layers: 12
2025-08-15 11:00:04,595:WARNING: 		 linear_patch: 2d
2025-08-15 11:00:04,595:WARNING: 	 cut_top_layer: 0
2025-08-15 11:00:06,239:WARNING: 	 sim_header: meanP
2025-08-15 11:00:10,647:INFO: --------------------
2025-08-15 11:00:10,647:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_i.bias
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_m.bias
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.patch_embed_r.bias
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_i.bias
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_m.bias
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm1_r.bias
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm2_i.bias
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.bias
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.0.bias
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.0.ffn.2.bias
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_i.bias
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_m.bias
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm1_r.bias
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm2_i.bias
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.bias
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.0.bias
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.blocks.1.ffn.2.bias
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.norm.bias
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
   teacher.temporal_fusion.gating_module.0.weight
   teacher.temporal_fusion.gating_module.0.bias
   teacher.temporal_fusion.gating_module.2.weight
   teacher.temporal_fusion.gating_module.2.bias
2025-08-15 11:00:10,647:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 11:00:12,789:INFO: ***** Running test *****
2025-08-15 11:00:12,789:INFO:   Num examples = 27763
2025-08-15 11:00:12,789:INFO:   Batch size = 32
2025-08-15 11:00:12,789:INFO:   Num steps = 868
2025-08-15 11:00:12,789:INFO: ***** Running val *****
2025-08-15 11:00:12,789:INFO:   Num examples = 4290
2025-08-15 11:00:13,740:INFO: ***** Running training *****
2025-08-15 11:00:13,740:INFO:   Num examples = 48774
2025-08-15 11:00:13,740:INFO:   Batch size = 96
2025-08-15 11:00:13,740:INFO:   Num steps = 508
2025-08-15 11:01:19,908:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.144600, Time/step: 13.233073
2025-08-15 11:01:55,097:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 2.947613, Time/step: 7.037792
2025-08-15 11:02:45,055:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.653375, Time/step: 9.991360
2025-08-15 11:03:18,457:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.219499, Time/step: 6.680341
2025-08-15 11:04:24,386:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.204059, Time/step: 13.185640
2025-08-15 11:05:03,735:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 1.940255, Time/step: 7.869722
2025-08-15 11:05:41,292:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 1.399467, Time/step: 7.511346
2025-08-15 11:06:13,669:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.438618, Time/step: 6.475270
2025-08-15 11:07:15,632:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.023447, Time/step: 12.392453
2025-08-15 11:07:44,848:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.271341, Time/step: 5.842974
2025-08-15 11:08:23,710:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.007617, Time/step: 7.772380
2025-08-15 11:09:10,848:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.007138, Time/step: 9.427424
2025-08-15 11:09:51,199:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 0.798590, Time/step: 8.070121
2025-08-15 11:10:26,240:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.713797, Time/step: 7.008017
2025-08-15 11:11:13,730:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.792678, Time/step: 9.497987
2025-08-15 11:11:48,475:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.566667, Time/step: 6.948927
2025-08-15 11:12:38,778:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.750109, Time/step: 10.060499
2025-08-15 11:13:16,537:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.729466, Time/step: 7.551609
2025-08-15 11:13:49,450:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.847836, Time/step: 6.582426
2025-08-15 11:14:24,087:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.821206, Time/step: 6.927240
2025-08-15 11:15:23,581:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.575221, Time/step: 11.898776
2025-08-15 11:15:57,605:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.674519, Time/step: 6.804633
2025-08-15 11:16:29,490:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.704022, Time/step: 6.376847
2025-08-15 11:17:10,390:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.627259, Time/step: 8.179944
2025-08-15 11:18:05,453:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.717470, Time/step: 11.012527
2025-08-15 11:18:36,770:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.813665, Time/step: 6.263276
2025-08-15 11:19:13,142:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.327717, Time/step: 7.274201
2025-08-15 11:19:46,513:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 0.594699, Time/step: 6.674018
2025-08-15 11:20:42,457:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 0.724849, Time/step: 11.188766
2025-08-15 11:21:16,529:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 0.563198, Time/step: 6.814282
2025-08-15 11:21:49,726:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 0.450963, Time/step: 6.639261
2025-08-15 11:22:20,572:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 0.632757, Time/step: 6.169039
2025-08-15 11:23:15,920:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 0.496845, Time/step: 11.069429
2025-08-15 11:23:54,067:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 0.585371, Time/step: 7.629451
2025-08-15 11:24:29,262:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 0.510925, Time/step: 7.038807
2025-08-15 11:25:02,168:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 0.586445, Time/step: 6.580996
2025-08-15 11:25:55,620:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 0.586093, Time/step: 10.690264
2025-08-15 11:26:28,066:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 0.413171, Time/step: 6.489181
2025-08-15 11:27:11,050:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 0.566985, Time/step: 8.596564
2025-08-15 11:27:43,067:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.478042, Time/step: 6.403300
2025-08-15 11:28:36,619:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 0.514452, Time/step: 10.710339
2025-08-15 11:29:05,330:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 0.798790, Time/step: 5.742048
2025-08-15 11:29:37,440:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.325731, Time/step: 6.421794
2025-08-15 11:30:07,363:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.451901, Time/step: 5.984444
2025-08-15 11:30:58,593:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.452738, Time/step: 10.246003
2025-08-15 11:31:39,010:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 0.827565, Time/step: 8.083091
2025-08-15 11:32:09,842:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.409193, Time/step: 6.166452
2025-08-15 11:32:40,176:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.399596, Time/step: 6.066561
2025-08-15 11:33:18,939:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 0.737703, Time/step: 7.752457
2025-08-15 11:34:09,240:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.332556, Time/step: 10.060182
2025-08-15 11:34:44,438:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.670994, Time/step: 7.039298
2025-08-15 11:35:19,534:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.413894, Time/step: 7.019156
2025-08-15 11:35:52,945:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.413348, Time/step: 6.682130
2025-08-15 11:36:47,552:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.480756, Time/step: 10.921118
2025-08-15 11:37:20,111:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 0.511270, Time/step: 6.511737
2025-08-15 11:37:58,159:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.375103, Time/step: 7.609449
2025-08-15 11:38:27,903:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.536785, Time/step: 5.948808
2025-08-15 11:39:26,583:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.427806, Time/step: 11.735778
2025-08-15 11:39:56,723:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.446270, Time/step: 6.027806
2025-08-15 11:40:32,416:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.307004, Time/step: 7.138460
2025-08-15 11:41:04,722:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.319321, Time/step: 6.461168
2025-08-15 11:42:00,287:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.246983, Time/step: 11.112851
2025-08-15 11:42:31,456:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.341292, Time/step: 6.233702
2025-08-15 11:43:03,799:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 0.724725, Time/step: 6.468495
2025-08-15 11:43:40,808:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.425708, Time/step: 7.401699
2025-08-15 11:44:37,855:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 0.586816, Time/step: 11.409276
2025-08-15 11:45:06,715:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.616172, Time/step: 5.771748
2025-08-15 11:45:41,012:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.450081, Time/step: 6.859296
2025-08-15 11:46:13,954:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.469671, Time/step: 6.588283
2025-08-15 11:47:04,184:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.490285, Time/step: 10.045875
2025-08-15 11:47:33,696:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.388321, Time/step: 5.902323
2025-08-15 11:48:12,206:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.312498, Time/step: 7.701801
2025-08-15 11:48:48,934:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.453964, Time/step: 7.345509
2025-08-15 11:49:35,753:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.569697, Time/step: 9.363569
2025-08-15 11:50:06,614:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.439224, Time/step: 6.172072
2025-08-15 11:50:47,859:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 0.634955, Time/step: 8.248885
2025-08-15 11:51:19,759:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.573955, Time/step: 6.379948
2025-08-15 11:52:08,996:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 0.685862, Time/step: 9.847304
2025-08-15 11:52:37,403:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.319326, Time/step: 5.681244
2025-08-15 11:53:17,034:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.369879, Time/step: 7.926154
2025-08-15 11:53:49,863:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.469194, Time/step: 6.565577
2025-08-15 11:54:37,155:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.397327, Time/step: 9.458246
2025-08-15 11:55:10,106:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.476391, Time/step: 6.590072
2025-08-15 11:55:49,430:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 0.747665, Time/step: 7.864769
2025-08-15 11:56:24,909:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.445487, Time/step: 7.095583
2025-08-15 11:57:16,343:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 0.722742, Time/step: 10.286676
2025-08-15 11:57:52,116:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.441121, Time/step: 7.154447
2025-08-15 11:58:27,343:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 0.704016, Time/step: 7.045330
2025-08-15 11:58:59,772:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.553963, Time/step: 6.485650
2025-08-15 11:59:50,996:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.389847, Time/step: 10.244694
2025-08-15 12:00:29,268:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.391684, Time/step: 7.654205
2025-08-15 12:01:00,259:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.392348, Time/step: 6.198143
2025-08-15 12:01:30,925:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.472986, Time/step: 6.133042
2025-08-15 12:02:28,827:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.523409, Time/step: 11.580195
2025-08-15 12:03:03,461:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 0.796951, Time/step: 6.926704
2025-08-15 12:03:36,383:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.582888, Time/step: 6.584312
2025-08-15 12:04:11,897:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.333816, Time/step: 7.102561
2025-08-15 12:05:03,260:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.417194, Time/step: 10.272505
2025-08-15 12:05:39,878:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.411941, Time/step: 7.323360
2025-08-15 12:06:13,039:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.456386, Time/step: 6.632204
2025-08-15 12:06:48,599:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.438414, Time/step: 7.111818
2025-08-15 12:07:17,795:INFO: Epoch 1/1 Finished, Train Loss: 0.710754
2025-08-15 12:07:18,763:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 12:07:18,763:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_opt.bin.0
2025-08-15 12:07:18,763:INFO: Eval on val dataset
2025-08-15 12:07:18,767:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 12:07:18,767:WARNING: sentence num: 4290, video num: 100
2025-08-15 12:20:49,314:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-15 12:20:49,319:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-15 12:20:49,814:INFO: Text-to-Video:
2025-08-15 12:20:49,814:INFO: 	>>>  R@1: 66.1 - R@5: 91.2 - R@10: 95.7 - Median R: 1.0 - Mean R: 2.7
2025-08-15 12:20:49,814:INFO: Video-to-Text:
2025-08-15 12:20:49,814:INFO: 	>>>  V2T$R@1: 77.2 - V2T$R@5: 97.0 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.6
2025-08-15 12:20:49,818:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0, the R1 is: 66.1072
2025-08-15 12:20:50,171:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 12:20:50,720:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 12:20:50,721:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 12:20:50,721:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 12:20:50,721:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 12:20:50,721:WARNING: Test retrieval by loose type.
2025-08-15 12:20:50,721:WARNING: 	 embed_dim: 512
2025-08-15 12:20:50,721:WARNING: 	 image_resolution: 224
2025-08-15 12:20:50,721:WARNING: 	 vision_layers: 12
2025-08-15 12:20:50,721:WARNING: 	 vision_width: 768
2025-08-15 12:20:50,721:WARNING: 	 vision_patch_size: 32
2025-08-15 12:20:50,721:WARNING: 	 context_length: 77
2025-08-15 12:20:50,721:WARNING: 	 vocab_size: 49408
2025-08-15 12:20:50,721:WARNING: 	 transformer_width: 512
2025-08-15 12:20:50,721:WARNING: 	 transformer_heads: 8
2025-08-15 12:20:50,721:WARNING: 	 transformer_layers: 12
2025-08-15 12:20:50,722:WARNING: 		 linear_patch: 2d
2025-08-15 12:20:50,722:WARNING: 	 cut_top_layer: 0
2025-08-15 12:20:52,286:WARNING: 	 sim_header: meanP
2025-08-15 12:20:56,913:INFO: --------------------
2025-08-15 12:20:56,913:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 12:20:57,050:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 12:20:57,050:WARNING: sentence num: 27763, video num: 670
2025-08-15 13:49:22,059:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-15 13:49:22,402:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-15 13:49:26,089:INFO: Text-to-Video:
2025-08-15 13:49:26,090:INFO: 	>>>  R@1: 39.7 - R@5: 71.0 - R@10: 81.1 - Median R: 2.0 - Mean R: 12.4
2025-08-15 13:49:26,090:INFO: Video-to-Text:
2025-08-15 13:49:26,090:INFO: 	>>>  V2T$R@1: 43.9 - V2T$R@5: 76.9 - V2T$R@10: 86.6 - V2T$Median R: 2.0 - V2T$Mean R: 6.9
2025-08-15 14:22:42,467:INFO: Effective parameters:
2025-08-15 14:22:42,467:INFO:   <<< batch_size: 96
2025-08-15 14:22:42,467:INFO:   <<< batch_size_val: 32
2025-08-15 14:22:42,467:INFO:   <<< cache_dir: 
2025-08-15 14:22:42,467:INFO:   <<< coef_lr: 0.001
2025-08-15 14:22:42,467:INFO:   <<< cross_model: cross-base
2025-08-15 14:22:42,468:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 14:22:42,468:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 14:22:42,468:INFO:   <<< datatype: msvd
2025-08-15 14:22:42,468:INFO:   <<< do_eval: False
2025-08-15 14:22:42,468:INFO:   <<< do_lower_case: False
2025-08-15 14:22:42,468:INFO:   <<< do_pretrain: False
2025-08-15 14:22:42,468:INFO:   <<< do_train: True
2025-08-15 14:22:42,468:INFO:   <<< epochs: 1
2025-08-15 14:22:42,468:INFO:   <<< eval_frame_order: 0
2025-08-15 14:22:42,468:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 14:22:42,468:INFO:   <<< feature_framerate: 1
2025-08-15 14:22:42,468:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 14:22:42,468:INFO:   <<< fp16: False
2025-08-15 14:22:42,468:INFO:   <<< fp16_opt_level: O1
2025-08-15 14:22:42,468:INFO:   <<< freeze_layer_num: 9
2025-08-15 14:22:42,468:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 14:22:42,468:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 14:22:42,468:INFO:   <<< init_model: None
2025-08-15 14:22:42,468:INFO:   <<< linear_patch: 2d
2025-08-15 14:22:42,468:INFO:   <<< local_rank: 0
2025-08-15 14:22:42,468:INFO:   <<< loose_type: True
2025-08-15 14:22:42,468:INFO:   <<< lr: 0.0001
2025-08-15 14:22:42,469:INFO:   <<< lr_decay: 0.9
2025-08-15 14:22:42,469:INFO:   <<< margin: 0.1
2025-08-15 14:22:42,469:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 14:22:42,469:INFO:   <<< max_frames: 12
2025-08-15 14:22:42,469:INFO:   <<< max_words: 32
2025-08-15 14:22:42,469:INFO:   <<< n_display: 5
2025-08-15 14:22:42,469:INFO:   <<< n_gpu: 1
2025-08-15 14:22:42,469:INFO:   <<< n_pair: 1
2025-08-15 14:22:42,469:INFO:   <<< negative_weighting: 1
2025-08-15 14:22:42,469:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 14:22:42,469:INFO:   <<< num_thread_reader: 4
2025-08-15 14:22:42,469:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 14:22:42,469:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 14:22:42,469:INFO:   <<< rank: 0
2025-08-15 14:22:42,469:INFO:   <<< resume_model: None
2025-08-15 14:22:42,469:INFO:   <<< sampled_use_mil: False
2025-08-15 14:22:42,469:INFO:   <<< seed: 42
2025-08-15 14:22:42,469:INFO:   <<< sim_header: meanP
2025-08-15 14:22:42,469:INFO:   <<< slice_framepos: 0
2025-08-15 14:22:42,470:INFO:   <<< task_type: retrieval
2025-08-15 14:22:42,470:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 14:22:42,470:INFO:   <<< train_csv: data/.train.csv
2025-08-15 14:22:42,470:INFO:   <<< train_frame_order: 0
2025-08-15 14:22:42,470:INFO:   <<< use_mil: False
2025-08-15 14:22:42,470:INFO:   <<< val_csv: data/.val.csv
2025-08-15 14:22:42,470:INFO:   <<< video_dim: 1024
2025-08-15 14:22:42,470:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 14:22:42,470:INFO:   <<< warmup_proportion: 0.1
2025-08-15 14:22:42,470:INFO:   <<< world_size: 2
2025-08-15 14:22:42,470:INFO: device: cuda:0 n_gpu: 2
2025-08-15 14:22:42,476:INFO: device: cuda:1 n_gpu: 2
2025-08-15 14:22:43,357:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 14:22:43,357:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 14:22:43,357:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 14:22:43,357:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 14:22:43,357:WARNING: Test retrieval by loose type.
2025-08-15 14:22:43,358:WARNING: 	 embed_dim: 512
2025-08-15 14:22:43,358:WARNING: 	 image_resolution: 224
2025-08-15 14:22:43,358:WARNING: 	 vision_layers: 12
2025-08-15 14:22:43,358:WARNING: 	 vision_width: 768
2025-08-15 14:22:43,358:WARNING: 	 vision_patch_size: 32
2025-08-15 14:22:43,358:WARNING: 	 context_length: 77
2025-08-15 14:22:43,358:WARNING: 	 vocab_size: 49408
2025-08-15 14:22:43,358:WARNING: 	 transformer_width: 512
2025-08-15 14:22:43,358:WARNING: 	 transformer_heads: 8
2025-08-15 14:22:43,358:WARNING: 	 transformer_layers: 12
2025-08-15 14:22:43,358:WARNING: 		 linear_patch: 2d
2025-08-15 14:22:43,358:WARNING: 	 cut_top_layer: 0
2025-08-15 14:22:45,026:WARNING: 	 sim_header: meanP
2025-08-15 14:22:49,444:INFO: --------------------
2025-08-15 14:22:49,445:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_i.bias
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_m.bias
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.patch_embed_r.bias
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_i.bias
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_m.bias
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm1_r.bias
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm2_i.bias
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.bias
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.0.bias
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.0.ffn.2.bias
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.bias
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_bias
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.bias
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.bias
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_i.bias
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_m.bias
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm1_r.bias
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm2_i.bias
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.bias
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.0.bias
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.blocks.1.ffn.2.bias
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.norm.bias
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.in_proj_bias
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.bias
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_i.bias
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.norm_m.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
2025-08-15 14:22:49,445:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 14:22:51,976:INFO: ***** Running test *****
2025-08-15 14:22:51,976:INFO:   Num examples = 27763
2025-08-15 14:22:51,976:INFO:   Batch size = 32
2025-08-15 14:22:51,976:INFO:   Num steps = 868
2025-08-15 14:22:51,976:INFO: ***** Running val *****
2025-08-15 14:22:51,976:INFO:   Num examples = 4290
2025-08-15 14:22:52,513:INFO: ***** Running training *****
2025-08-15 14:22:52,513:INFO:   Num examples = 48774
2025-08-15 14:22:52,513:INFO:   Batch size = 96
2025-08-15 14:22:52,513:INFO:   Num steps = 508
2025-08-15 14:24:00,157:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.354748, Time/step: 13.521346
2025-08-15 14:24:31,348:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.178149, Time/step: 6.238111
2025-08-15 14:25:09,117:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.924240, Time/step: 7.553645
2025-08-15 14:25:38,947:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.567771, Time/step: 5.965944
2025-08-15 14:26:34,360:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.524460, Time/step: 11.082490
2025-08-15 14:27:07,227:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.216597, Time/step: 6.573199
2025-08-15 14:27:37,991:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 1.875903, Time/step: 6.152645
2025-08-15 14:28:07,415:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.797025, Time/step: 5.884769
2025-08-15 14:29:00,827:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.472310, Time/step: 10.682289
2025-08-15 14:29:27,520:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.402888, Time/step: 5.338359
2025-08-15 14:30:09,331:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.175199, Time/step: 8.362059
2025-08-15 14:30:51,630:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.178779, Time/step: 8.459711
2025-08-15 14:31:26,004:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.011446, Time/step: 6.874637
2025-08-15 14:31:58,523:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.912067, Time/step: 6.503793
2025-08-15 14:32:51,776:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.917930, Time/step: 10.650483
2025-08-15 14:33:25,561:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.713018, Time/step: 6.756728
2025-08-15 14:33:56,630:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.942567, Time/step: 6.213702
2025-08-15 14:34:27,203:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.817080, Time/step: 6.114539
2025-08-15 14:35:20,842:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.957304, Time/step: 10.727764
2025-08-15 14:35:52,165:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.918324, Time/step: 6.264327
2025-08-15 14:36:34,200:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.673054, Time/step: 8.407011
2025-08-15 14:37:03,091:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.779817, Time/step: 5.778076
2025-08-15 14:37:54,540:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.715763, Time/step: 10.289595
2025-08-15 14:38:23,691:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.672333, Time/step: 5.830212
2025-08-15 14:38:52,649:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.770263, Time/step: 5.791421
2025-08-15 14:39:24,415:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.811431, Time/step: 6.353018
2025-08-15 14:40:20,837:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.374923, Time/step: 11.284288
2025-08-15 14:40:52,367:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 0.651503, Time/step: 6.305885
2025-08-15 14:41:27,533:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 0.764439, Time/step: 7.033117
2025-08-15 14:42:01,777:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 0.635797, Time/step: 6.848671
2025-08-15 14:42:46,336:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 0.453317, Time/step: 8.911554
2025-08-15 14:43:14,575:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 0.710804, Time/step: 5.647683
2025-08-15 14:43:59,099:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 0.540152, Time/step: 8.904707
2025-08-15 14:44:38,612:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 0.603551, Time/step: 7.902352
2025-08-15 14:45:11,490:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 0.553471, Time/step: 6.575493
2025-08-15 14:45:44,602:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 0.648176, Time/step: 6.622413
2025-08-15 14:46:35,565:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 0.698342, Time/step: 10.192510
2025-08-15 14:47:07,642:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 0.459196, Time/step: 6.415187
2025-08-15 14:47:48,624:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 0.550966, Time/step: 8.196183
2025-08-15 14:48:21,671:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.502792, Time/step: 6.609437
2025-08-15 14:49:13,708:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 0.511726, Time/step: 10.407295
2025-08-15 14:49:41,265:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 0.817978, Time/step: 5.511168
2025-08-15 14:50:13,426:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.335881, Time/step: 6.432101
2025-08-15 14:50:43,232:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.470541, Time/step: 5.961035
2025-08-15 14:51:34,754:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.468219, Time/step: 10.304343
2025-08-15 14:52:01,835:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 0.910799, Time/step: 5.415926
2025-08-15 14:52:48,593:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.407396, Time/step: 9.351531
2025-08-15 14:53:16,647:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.444782, Time/step: 5.610760
2025-08-15 14:54:00,540:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 0.742574, Time/step: 8.778365
2025-08-15 14:54:33,400:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.388306, Time/step: 6.571904
2025-08-15 14:55:08,502:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.686502, Time/step: 7.020420
2025-08-15 14:55:43,835:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.459553, Time/step: 7.066265
2025-08-15 14:56:39,069:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.448245, Time/step: 11.046829
2025-08-15 14:57:12,882:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.545197, Time/step: 6.762393
2025-08-15 14:57:44,279:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 0.538404, Time/step: 6.279267
2025-08-15 14:58:25,225:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.400275, Time/step: 8.189163
2025-08-15 14:59:08,139:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.565951, Time/step: 8.582559
2025-08-15 14:59:55,048:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.471806, Time/step: 9.381672
2025-08-15 15:00:26,394:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.471274, Time/step: 6.269140
2025-08-15 15:01:06,734:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.334408, Time/step: 8.067939
2025-08-15 15:01:48,720:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.354122, Time/step: 8.397116
2025-08-15 15:02:33,482:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.275161, Time/step: 8.952115
2025-08-15 15:03:05,563:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.344231, Time/step: 6.416138
2025-08-15 15:03:51,543:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 0.725324, Time/step: 9.195978
2025-08-15 15:04:44,116:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.428234, Time/step: 10.514406
2025-08-15 15:05:16,752:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 0.592245, Time/step: 6.526968
2025-08-15 15:05:49,552:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.625035, Time/step: 6.559879
2025-08-15 15:06:31,822:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.486119, Time/step: 8.454017
2025-08-15 15:07:16,183:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.468397, Time/step: 8.871977
2025-08-15 15:07:52,331:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.493975, Time/step: 7.229415
2025-08-15 15:08:23,961:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.440138, Time/step: 6.325942
2025-08-15 15:09:14,418:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.328134, Time/step: 10.091363
2025-08-15 15:09:58,765:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.453321, Time/step: 8.869131
2025-08-15 15:10:32,770:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.569881, Time/step: 6.801002
2025-08-15 15:11:06,224:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.457581, Time/step: 6.690660
2025-08-15 15:12:00,747:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 0.706104, Time/step: 10.904421
2025-08-15 15:12:36,182:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.594407, Time/step: 7.086909
2025-08-15 15:13:09,162:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 0.720851, Time/step: 6.595844
2025-08-15 15:13:40,739:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.324560, Time/step: 6.315253
2025-08-15 15:14:34,688:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.360001, Time/step: 10.789722
2025-08-15 15:15:09,509:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.496037, Time/step: 6.964071
2025-08-15 15:15:42,122:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.406384, Time/step: 6.522465
2025-08-15 15:16:14,436:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.499288, Time/step: 6.462794
2025-08-15 15:17:12,446:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 0.781253, Time/step: 11.601792
2025-08-15 15:17:41,752:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.484227, Time/step: 5.861010
2025-08-15 15:18:16,782:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 0.731775, Time/step: 7.006032
2025-08-15 15:18:51,702:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.450220, Time/step: 6.983748
2025-08-15 15:19:40,057:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 0.719684, Time/step: 9.670967
2025-08-15 15:20:14,339:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.536148, Time/step: 6.856121
2025-08-15 15:20:50,570:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.391253, Time/step: 7.246167
2025-08-15 15:21:30,387:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.369749, Time/step: 7.963261
2025-08-15 15:22:03,959:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.400706, Time/step: 6.714312
2025-08-15 15:22:43,147:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.469978, Time/step: 7.837385
2025-08-15 15:23:29,819:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.549648, Time/step: 9.334301
2025-08-15 15:24:04,346:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 0.783677, Time/step: 6.905257
2025-08-15 15:25:02,929:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.570773, Time/step: 11.716560
2025-08-15 15:25:35,934:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.377165, Time/step: 6.600932
2025-08-15 15:26:14,109:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.463831, Time/step: 7.634728
2025-08-15 15:26:51,862:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.398701, Time/step: 7.550631
2025-08-15 15:27:49,447:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.445598, Time/step: 11.516724
2025-08-15 15:28:23,571:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.429137, Time/step: 6.824809
2025-08-15 15:28:59,831:INFO: Epoch 1/1 Finished, Train Loss: 0.776536
2025-08-15 15:29:01,052:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 15:29:01,052:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_opt.bin.0
2025-08-15 15:29:01,052:INFO: Eval on val dataset
2025-08-15 15:29:01,076:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 15:29:01,076:WARNING: sentence num: 4290, video num: 100
2025-08-15 15:42:36,968:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-15 15:42:36,976:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-15 15:42:39,139:INFO: Text-to-Video:
2025-08-15 15:42:39,143:INFO: 	>>>  R@1: 64.6 - R@5: 90.5 - R@10: 96.1 - Median R: 1.0 - Mean R: 2.7
2025-08-15 15:42:39,143:INFO: Video-to-Text:
2025-08-15 15:42:39,143:INFO: 	>>>  V2T$R@1: 78.2 - V2T$R@5: 97.0 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.5
2025-08-15 15:42:39,153:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0, the R1 is: 64.5688
2025-08-15 15:42:39,528:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 15:43:00,123:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 15:43:00,123:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 15:43:00,123:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 15:43:00,123:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 15:43:00,124:WARNING: Test retrieval by loose type.
2025-08-15 15:43:00,124:WARNING: 	 embed_dim: 512
2025-08-15 15:43:00,124:WARNING: 	 image_resolution: 224
2025-08-15 15:43:00,124:WARNING: 	 vision_layers: 12
2025-08-15 15:43:00,124:WARNING: 	 vision_width: 768
2025-08-15 15:43:00,124:WARNING: 	 vision_patch_size: 32
2025-08-15 15:43:00,124:WARNING: 	 context_length: 77
2025-08-15 15:43:00,124:WARNING: 	 vocab_size: 49408
2025-08-15 15:43:00,124:WARNING: 	 transformer_width: 512
2025-08-15 15:43:00,124:WARNING: 	 transformer_heads: 8
2025-08-15 15:43:00,124:WARNING: 	 transformer_layers: 12
2025-08-15 15:43:00,124:WARNING: 		 linear_patch: 2d
2025-08-15 15:43:00,124:WARNING: 	 cut_top_layer: 0
2025-08-15 15:43:01,803:WARNING: 	 sim_header: meanP
2025-08-15 15:43:06,395:INFO: --------------------
2025-08-15 15:43:06,395:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 15:43:06,493:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 15:43:06,493:WARNING: sentence num: 27763, video num: 670
2025-08-15 17:11:01,431:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-15 17:11:01,773:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-15 17:11:05,551:INFO: Text-to-Video:
2025-08-15 17:11:05,551:INFO: 	>>>  R@1: 37.5 - R@5: 68.9 - R@10: 79.8 - Median R: 2.0 - Mean R: 13.3
2025-08-15 17:11:05,551:INFO: Video-to-Text:
2025-08-15 17:11:05,551:INFO: 	>>>  V2T$R@1: 40.6 - V2T$R@5: 71.5 - V2T$R@10: 80.6 - V2T$Median R: 2.0 - V2T$Mean R: 8.5
2025-08-15 17:23:06,896:INFO: device: cuda:1 n_gpu: 2
2025-08-15 17:23:06,926:INFO: Effective parameters:
2025-08-15 17:23:06,926:INFO:   <<< batch_size: 96
2025-08-15 17:23:06,926:INFO:   <<< batch_size_val: 32
2025-08-15 17:23:06,926:INFO:   <<< cache_dir: 
2025-08-15 17:23:06,926:INFO:   <<< coef_lr: 0.001
2025-08-15 17:23:06,927:INFO:   <<< cross_model: cross-base
2025-08-15 17:23:06,927:INFO:   <<< cross_num_hidden_layers: 4
2025-08-15 17:23:06,927:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-15 17:23:06,927:INFO:   <<< datatype: msvd
2025-08-15 17:23:06,927:INFO:   <<< do_eval: False
2025-08-15 17:23:06,927:INFO:   <<< do_lower_case: False
2025-08-15 17:23:06,927:INFO:   <<< do_pretrain: False
2025-08-15 17:23:06,927:INFO:   <<< do_train: True
2025-08-15 17:23:06,927:INFO:   <<< epochs: 1
2025-08-15 17:23:06,927:INFO:   <<< eval_frame_order: 0
2025-08-15 17:23:06,928:INFO:   <<< expand_msrvtt_sentences: False
2025-08-15 17:23:06,928:INFO:   <<< feature_framerate: 1
2025-08-15 17:23:06,928:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-15 17:23:06,928:INFO:   <<< fp16: False
2025-08-15 17:23:06,928:INFO:   <<< fp16_opt_level: O1
2025-08-15 17:23:06,928:INFO:   <<< freeze_layer_num: 9
2025-08-15 17:23:06,928:INFO:   <<< gradient_accumulation_steps: 1
2025-08-15 17:23:06,928:INFO:   <<< hard_negative_rate: 0.5
2025-08-15 17:23:06,928:INFO:   <<< init_model: None
2025-08-15 17:23:06,928:INFO:   <<< linear_patch: 2d
2025-08-15 17:23:06,928:INFO:   <<< local_rank: 0
2025-08-15 17:23:06,929:INFO:   <<< loose_type: True
2025-08-15 17:23:06,929:INFO:   <<< lr: 0.0001
2025-08-15 17:23:06,929:INFO:   <<< lr_decay: 0.9
2025-08-15 17:23:06,929:INFO:   <<< margin: 0.1
2025-08-15 17:23:06,929:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-15 17:23:06,929:INFO:   <<< max_frames: 12
2025-08-15 17:23:06,929:INFO:   <<< max_words: 32
2025-08-15 17:23:06,929:INFO:   <<< n_display: 5
2025-08-15 17:23:06,929:INFO:   <<< n_gpu: 1
2025-08-15 17:23:06,929:INFO:   <<< n_pair: 1
2025-08-15 17:23:06,929:INFO:   <<< negative_weighting: 1
2025-08-15 17:23:06,930:INFO:   <<< new_added_modules: ['Adapter']
2025-08-15 17:23:06,930:INFO:   <<< num_thread_reader: 4
2025-08-15 17:23:06,930:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1
2025-08-15 17:23:06,930:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-15 17:23:06,930:INFO:   <<< rank: 0
2025-08-15 17:23:06,930:INFO:   <<< resume_model: None
2025-08-15 17:23:06,930:INFO:   <<< sampled_use_mil: False
2025-08-15 17:23:06,930:INFO:   <<< seed: 42
2025-08-15 17:23:06,930:INFO:   <<< sim_header: meanP
2025-08-15 17:23:06,930:INFO:   <<< slice_framepos: 0
2025-08-15 17:23:06,930:INFO:   <<< task_type: retrieval
2025-08-15 17:23:06,930:INFO:   <<< text_num_hidden_layers: 12
2025-08-15 17:23:06,931:INFO:   <<< train_csv: data/.train.csv
2025-08-15 17:23:06,931:INFO:   <<< train_frame_order: 0
2025-08-15 17:23:06,931:INFO:   <<< use_mil: False
2025-08-15 17:23:06,931:INFO:   <<< val_csv: data/.val.csv
2025-08-15 17:23:06,931:INFO:   <<< video_dim: 1024
2025-08-15 17:23:06,931:INFO:   <<< visual_num_hidden_layers: 12
2025-08-15 17:23:06,931:INFO:   <<< warmup_proportion: 0.1
2025-08-15 17:23:06,931:INFO:   <<< world_size: 2
2025-08-15 17:23:06,932:INFO: device: cuda:0 n_gpu: 2
2025-08-15 17:23:10,175:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 17:23:10,175:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 17:23:10,175:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 17:23:10,176:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 17:23:10,176:WARNING: Test retrieval by loose type.
2025-08-15 17:23:10,176:WARNING: 	 embed_dim: 512
2025-08-15 17:23:10,176:WARNING: 	 image_resolution: 224
2025-08-15 17:23:10,176:WARNING: 	 vision_layers: 12
2025-08-15 17:23:10,176:WARNING: 	 vision_width: 768
2025-08-15 17:23:10,176:WARNING: 	 vision_patch_size: 32
2025-08-15 17:23:10,176:WARNING: 	 context_length: 77
2025-08-15 17:23:10,176:WARNING: 	 vocab_size: 49408
2025-08-15 17:23:10,176:WARNING: 	 transformer_width: 512
2025-08-15 17:23:10,176:WARNING: 	 transformer_heads: 8
2025-08-15 17:23:10,176:WARNING: 	 transformer_layers: 12
2025-08-15 17:23:10,176:WARNING: 		 linear_patch: 2d
2025-08-15 17:23:10,176:WARNING: 	 cut_top_layer: 0
2025-08-15 17:23:11,892:WARNING: 	 sim_header: meanP
2025-08-15 17:23:16,668:INFO: --------------------
2025-08-15 17:23:16,669:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.0.weight
   teacher.motion_encoder.blocks.0.ffn.2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.0.weight
   teacher.motion_encoder.blocks.1.ffn.2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
2025-08-15 17:23:16,669:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 17:23:19,626:INFO: ***** Running test *****
2025-08-15 17:23:19,627:INFO:   Num examples = 27763
2025-08-15 17:23:19,627:INFO:   Batch size = 32
2025-08-15 17:23:19,627:INFO:   Num steps = 868
2025-08-15 17:23:19,627:INFO: ***** Running val *****
2025-08-15 17:23:19,627:INFO:   Num examples = 4290
2025-08-15 17:23:20,451:INFO: ***** Running training *****
2025-08-15 17:23:20,451:INFO:   Num examples = 48774
2025-08-15 17:23:20,451:INFO:   Batch size = 96
2025-08-15 17:23:20,451:INFO:   Num steps = 508
2025-08-15 17:25:06,340:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.342048, Time/step: 21.170251
2025-08-15 17:26:02,287:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.205794, Time/step: 11.189241
2025-08-15 17:27:07,550:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 3.016239, Time/step: 13.052344
2025-08-15 17:27:45,763:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.703212, Time/step: 7.642579
2025-08-15 17:28:48,849:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.770261, Time/step: 12.617113
2025-08-15 17:29:37,568:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.530387, Time/step: 9.743697
2025-08-15 17:30:25,176:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.250849, Time/step: 9.521407
2025-08-15 17:31:23,650:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.158846, Time/step: 11.694673
2025-08-15 17:33:14,407:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.825572, Time/step: 22.151280
2025-08-15 17:33:44,684:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.701602, Time/step: 6.055234
2025-08-15 17:34:20,720:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.491490, Time/step: 7.207099
2025-08-15 17:34:58,302:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.370176, Time/step: 7.516307
2025-08-15 17:35:51,472:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.114946, Time/step: 10.633936
2025-08-15 17:36:24,755:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 1.011401, Time/step: 6.656323
2025-08-15 17:37:09,837:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 1.013736, Time/step: 9.016382
2025-08-15 17:37:45,675:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.779392, Time/step: 7.167534
2025-08-15 17:38:35,479:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.915407, Time/step: 9.960666
2025-08-15 17:39:15,039:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.881268, Time/step: 7.911831
2025-08-15 17:39:58,325:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.986044, Time/step: 8.656971
2025-08-15 17:40:33,126:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.970539, Time/step: 6.960231
2025-08-15 17:41:31,135:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.649155, Time/step: 11.601698
2025-08-15 17:42:05,231:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.848439, Time/step: 6.819041
2025-08-15 17:42:53,675:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.766893, Time/step: 9.688644
2025-08-15 17:43:28,227:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.699449, Time/step: 6.910186
2025-08-15 17:44:26,384:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.784938, Time/step: 11.631271
2025-08-15 17:44:58,503:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.915191, Time/step: 6.423732
2025-08-15 17:45:37,803:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.392246, Time/step: 7.859962
2025-08-15 17:46:13,848:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 0.644866, Time/step: 7.208717
2025-08-15 17:47:21,699:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 0.747891, Time/step: 13.570092
2025-08-15 17:48:00,660:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 0.617226, Time/step: 7.792205
2025-08-15 17:48:35,722:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 0.509320, Time/step: 7.012166
2025-08-15 17:49:08,402:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 0.707734, Time/step: 6.535992
2025-08-15 17:50:08,936:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 0.592531, Time/step: 12.106670
2025-08-15 17:50:50,293:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 0.634536, Time/step: 8.271219
2025-08-15 17:51:35,490:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 0.571841, Time/step: 9.039242
2025-08-15 17:52:14,443:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 0.637475, Time/step: 7.790433
2025-08-15 17:53:13,532:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 0.699488, Time/step: 11.815176
2025-08-15 17:53:48,878:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 0.505340, Time/step: 7.069205
2025-08-15 17:54:34,778:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 0.603687, Time/step: 9.179796
2025-08-15 17:55:17,902:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.529060, Time/step: 8.624589
2025-08-15 17:57:44,153:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 0.554795, Time/step: 29.250078
2025-08-15 17:59:05,706:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 0.839111, Time/step: 16.310570
2025-08-15 18:00:15,995:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.336707, Time/step: 14.057654
2025-08-15 18:01:14,467:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.508345, Time/step: 11.694290
2025-08-15 18:02:32,219:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.487528, Time/step: 15.550402
2025-08-15 18:03:11,249:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 0.860254, Time/step: 7.805860
2025-08-15 18:03:50,710:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.435237, Time/step: 7.891979
2025-08-15 18:04:28,980:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.455523, Time/step: 7.653807
2025-08-15 18:05:29,469:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 0.706703, Time/step: 12.097735
2025-08-15 18:06:02,952:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.418984, Time/step: 6.696466
2025-08-15 18:06:40,044:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.704740, Time/step: 7.418271
2025-08-15 18:07:22,445:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.466441, Time/step: 8.480114
2025-08-15 18:08:26,571:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.446394, Time/step: 12.824998
2025-08-15 18:09:13,212:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.544965, Time/step: 9.328041
2025-08-15 18:09:47,859:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 0.545629, Time/step: 6.929427
2025-08-15 18:10:46,518:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.431356, Time/step: 11.731534
2025-08-15 18:11:46,083:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.546099, Time/step: 11.912928
2025-08-15 18:12:40,204:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.474516, Time/step: 10.824014
2025-08-15 18:13:27,133:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.465808, Time/step: 9.385686
2025-08-15 18:14:36,007:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.330969, Time/step: 13.774695
2025-08-15 18:15:28,374:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.337794, Time/step: 10.473291
2025-08-15 18:16:04,109:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.288924, Time/step: 7.146865
2025-08-15 18:16:39,593:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.327464, Time/step: 7.096716
2025-08-15 18:17:39,729:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 0.715209, Time/step: 12.026987
2025-08-15 18:18:36,761:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.413695, Time/step: 11.406320
2025-08-15 18:19:15,924:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 0.590612, Time/step: 7.832618
2025-08-15 18:19:52,820:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.643376, Time/step: 7.378986
2025-08-15 18:20:43,430:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.504326, Time/step: 10.121934
2025-08-15 18:21:28,055:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.493770, Time/step: 8.924907
2025-08-15 18:22:03,449:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.500777, Time/step: 7.078689
2025-08-15 18:22:37,047:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.419849, Time/step: 6.719421
2025-08-15 18:23:31,225:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.335913, Time/step: 10.835425
2025-08-15 18:24:19,216:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.479488, Time/step: 9.597970
2025-08-15 18:24:54,729:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.580355, Time/step: 7.102643
2025-08-15 18:25:29,463:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.450881, Time/step: 6.946665
2025-08-15 18:26:17,089:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 0.663811, Time/step: 9.525011
2025-08-15 18:27:00,832:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.612047, Time/step: 8.748531
2025-08-15 18:27:33,301:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 0.732836, Time/step: 6.490067
2025-08-15 18:28:04,253:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.333974, Time/step: 6.190284
2025-08-15 18:28:52,928:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.346309, Time/step: 9.734869
2025-08-15 18:29:32,676:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.510681, Time/step: 7.949354
2025-08-15 18:30:09,343:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.407412, Time/step: 7.333347
2025-08-15 18:30:41,797:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.511856, Time/step: 6.490601
2025-08-15 18:31:34,801:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 0.747660, Time/step: 10.600704
2025-08-15 18:32:11,099:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.483843, Time/step: 7.259451
2025-08-15 18:32:49,007:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 0.752717, Time/step: 7.581578
2025-08-15 18:33:27,332:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.444828, Time/step: 7.664739
2025-08-15 18:34:08,484:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 0.725104, Time/step: 8.230384
2025-08-15 18:34:53,888:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.534104, Time/step: 9.080723
2025-08-15 18:35:41,208:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.373530, Time/step: 9.463877
2025-08-15 18:36:30,167:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.400008, Time/step: 9.791607
2025-08-15 18:37:03,296:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.409395, Time/step: 6.625581
2025-08-15 18:37:37,215:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.494511, Time/step: 6.783742
2025-08-15 18:38:37,054:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.551022, Time/step: 11.967574
2025-08-15 18:39:15,387:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 0.844758, Time/step: 7.666554
2025-08-15 18:39:58,961:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.557986, Time/step: 8.714718
2025-08-15 18:40:38,647:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.401465, Time/step: 7.936964
2025-08-15 18:41:39,767:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.463885, Time/step: 12.223986
2025-08-15 18:42:19,572:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.414076, Time/step: 7.960864
2025-08-15 18:43:02,310:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.480086, Time/step: 8.547506
2025-08-15 18:43:34,328:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.445924, Time/step: 6.403440
2025-08-15 18:44:10,357:INFO: Epoch 1/1 Finished, Train Loss: 0.817958
2025-08-15 18:44:11,730:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 18:44:11,731:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_opt.bin.0
2025-08-15 18:44:11,731:INFO: Eval on val dataset
2025-08-15 18:44:11,794:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 18:44:11,794:WARNING: sentence num: 4290, video num: 100
2025-08-15 18:58:11,679:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-15 18:58:11,685:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-15 18:58:12,227:INFO: Text-to-Video:
2025-08-15 18:58:12,228:INFO: 	>>>  R@1: 65.2 - R@5: 90.3 - R@10: 95.5 - Median R: 1.0 - Mean R: 2.8
2025-08-15 18:58:12,228:INFO: Video-to-Text:
2025-08-15 18:58:12,228:INFO: 	>>>  V2T$R@1: 78.2 - V2T$R@5: 96.0 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.6
2025-08-15 18:58:12,231:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0, the R1 is: 65.1748
2025-08-15 18:58:12,567:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0815_teacher_1/pytorch_model.bin.0
2025-08-15 18:58:17,347:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-15 18:58:17,347:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-15 18:58:17,347:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-15 18:58:17,360:WARNING: Stage-One:True, Stage-Two:False
2025-08-15 18:58:17,360:WARNING: Test retrieval by loose type.
2025-08-15 18:58:17,376:WARNING: 	 embed_dim: 512
2025-08-15 18:58:17,376:WARNING: 	 image_resolution: 224
2025-08-15 18:58:17,376:WARNING: 	 vision_layers: 12
2025-08-15 18:58:17,376:WARNING: 	 vision_width: 768
2025-08-15 18:58:17,376:WARNING: 	 vision_patch_size: 32
2025-08-15 18:58:17,376:WARNING: 	 context_length: 77
2025-08-15 18:58:17,376:WARNING: 	 vocab_size: 49408
2025-08-15 18:58:17,376:WARNING: 	 transformer_width: 512
2025-08-15 18:58:17,376:WARNING: 	 transformer_heads: 8
2025-08-15 18:58:17,376:WARNING: 	 transformer_layers: 12
2025-08-15 18:58:17,376:WARNING: 		 linear_patch: 2d
2025-08-15 18:58:17,376:WARNING: 	 cut_top_layer: 0
2025-08-15 18:58:19,174:WARNING: 	 sim_header: meanP
2025-08-15 18:58:24,029:INFO: --------------------
2025-08-15 18:58:24,029:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-15 18:58:24,230:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-15 18:58:24,230:WARNING: sentence num: 27763, video num: 670
2025-08-15 20:31:55,762:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-15 20:31:56,161:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-15 20:32:00,344:INFO: Text-to-Video:
2025-08-15 20:32:00,344:INFO: 	>>>  R@1: 36.9 - R@5: 69.0 - R@10: 79.9 - Median R: 2.0 - Mean R: 13.2
2025-08-15 20:32:00,345:INFO: Video-to-Text:
2025-08-15 20:32:00,345:INFO: 	>>>  V2T$R@1: 39.9 - V2T$R@5: 71.2 - V2T$R@10: 80.2 - V2T$Median R: 2.0 - V2T$Mean R: 8.8
