2025-08-16 09:57:11,951:INFO: Effective parameters:
2025-08-16 09:57:11,952:INFO:   <<< batch_size: 96
2025-08-16 09:57:11,952:INFO:   <<< batch_size_val: 32
2025-08-16 09:57:11,952:INFO: device: cuda:1 n_gpu: 2
2025-08-16 09:57:11,952:INFO:   <<< cache_dir: 
2025-08-16 09:57:11,952:INFO:   <<< coef_lr: 0.001
2025-08-16 09:57:11,952:INFO:   <<< cross_model: cross-base
2025-08-16 09:57:11,952:INFO:   <<< cross_num_hidden_layers: 4
2025-08-16 09:57:11,952:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-16 09:57:11,952:INFO:   <<< datatype: msvd
2025-08-16 09:57:11,953:INFO:   <<< do_eval: False
2025-08-16 09:57:11,953:INFO:   <<< do_lower_case: False
2025-08-16 09:57:11,953:INFO:   <<< do_pretrain: False
2025-08-16 09:57:11,953:INFO:   <<< do_train: True
2025-08-16 09:57:11,953:INFO:   <<< epochs: 1
2025-08-16 09:57:11,953:INFO:   <<< eval_frame_order: 0
2025-08-16 09:57:11,953:INFO:   <<< expand_msrvtt_sentences: False
2025-08-16 09:57:11,953:INFO:   <<< feature_framerate: 1
2025-08-16 09:57:11,953:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-16 09:57:11,953:INFO:   <<< fp16: False
2025-08-16 09:57:11,954:INFO:   <<< fp16_opt_level: O1
2025-08-16 09:57:11,954:INFO:   <<< freeze_layer_num: 9
2025-08-16 09:57:11,954:INFO:   <<< gradient_accumulation_steps: 1
2025-08-16 09:57:11,954:INFO:   <<< hard_negative_rate: 0.5
2025-08-16 09:57:11,954:INFO:   <<< init_model: None
2025-08-16 09:57:11,954:INFO:   <<< linear_patch: 2d
2025-08-16 09:57:11,954:INFO:   <<< local_rank: 0
2025-08-16 09:57:11,954:INFO:   <<< loose_type: True
2025-08-16 09:57:11,954:INFO:   <<< lr: 0.0001
2025-08-16 09:57:11,954:INFO:   <<< lr_decay: 0.9
2025-08-16 09:57:11,954:INFO:   <<< margin: 0.1
2025-08-16 09:57:11,955:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-16 09:57:11,955:INFO:   <<< max_frames: 12
2025-08-16 09:57:11,955:INFO:   <<< max_words: 32
2025-08-16 09:57:11,955:INFO:   <<< n_display: 5
2025-08-16 09:57:11,955:INFO:   <<< n_gpu: 1
2025-08-16 09:57:11,955:INFO:   <<< n_pair: 1
2025-08-16 09:57:11,955:INFO:   <<< negative_weighting: 1
2025-08-16 09:57:11,955:INFO:   <<< new_added_modules: ['Adapter']
2025-08-16 09:57:11,955:INFO:   <<< num_thread_reader: 4
2025-08-16 09:57:11,955:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1
2025-08-16 09:57:11,956:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-16 09:57:11,956:INFO:   <<< rank: 0
2025-08-16 09:57:11,956:INFO:   <<< resume_model: None
2025-08-16 09:57:11,956:INFO:   <<< sampled_use_mil: False
2025-08-16 09:57:11,956:INFO:   <<< seed: 42
2025-08-16 09:57:11,956:INFO:   <<< sim_header: meanP
2025-08-16 09:57:11,956:INFO:   <<< slice_framepos: 0
2025-08-16 09:57:11,956:INFO:   <<< task_type: retrieval
2025-08-16 09:57:11,956:INFO:   <<< text_num_hidden_layers: 12
2025-08-16 09:57:11,956:INFO:   <<< train_csv: data/.train.csv
2025-08-16 09:57:11,956:INFO:   <<< train_frame_order: 0
2025-08-16 09:57:11,957:INFO:   <<< use_mil: False
2025-08-16 09:57:11,957:INFO:   <<< val_csv: data/.val.csv
2025-08-16 09:57:11,957:INFO:   <<< video_dim: 1024
2025-08-16 09:57:11,957:INFO:   <<< visual_num_hidden_layers: 12
2025-08-16 09:57:11,957:INFO:   <<< warmup_proportion: 0.1
2025-08-16 09:57:11,957:INFO:   <<< world_size: 2
2025-08-16 09:57:11,958:INFO: device: cuda:0 n_gpu: 2
2025-08-16 09:57:12,901:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-16 09:57:12,901:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-16 09:57:12,901:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-16 09:57:12,901:WARNING: Stage-One:True, Stage-Two:False
2025-08-16 09:57:12,901:WARNING: Test retrieval by loose type.
2025-08-16 09:57:12,902:WARNING: 	 embed_dim: 512
2025-08-16 09:57:12,902:WARNING: 	 image_resolution: 224
2025-08-16 09:57:12,902:WARNING: 	 vision_layers: 12
2025-08-16 09:57:12,902:WARNING: 	 vision_width: 768
2025-08-16 09:57:12,902:WARNING: 	 vision_patch_size: 32
2025-08-16 09:57:12,902:WARNING: 	 context_length: 77
2025-08-16 09:57:12,902:WARNING: 	 vocab_size: 49408
2025-08-16 09:57:12,902:WARNING: 	 transformer_width: 512
2025-08-16 09:57:12,902:WARNING: 	 transformer_heads: 8
2025-08-16 09:57:12,902:WARNING: 	 transformer_layers: 12
2025-08-16 09:57:12,902:WARNING: 		 linear_patch: 2d
2025-08-16 09:57:12,902:WARNING: 	 cut_top_layer: 0
2025-08-16 09:57:14,595:WARNING: 	 sim_header: meanP
2025-08-16 09:57:19,151:INFO: --------------------
2025-08-16 09:57:19,151:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.pos_embedding_scale
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.0.norm2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.in_proj_bias
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.self_attn.out_proj.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.linear2.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm1.bias
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder.layers.1.norm2.bias
   teacher.temporal_fusion.positional_embedding.weight
2025-08-16 09:57:19,151:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-16 09:57:21,765:INFO: ***** Running test *****
2025-08-16 09:57:21,765:INFO:   Num examples = 27763
2025-08-16 09:57:21,765:INFO:   Batch size = 32
2025-08-16 09:57:21,765:INFO:   Num steps = 868
2025-08-16 09:57:21,765:INFO: ***** Running val *****
2025-08-16 09:57:21,765:INFO:   Num examples = 4290
2025-08-16 09:57:22,380:INFO: ***** Running training *****
2025-08-16 09:57:22,380:INFO:   Num examples = 48774
2025-08-16 09:57:22,380:INFO:   Batch size = 96
2025-08-16 09:57:22,380:INFO:   Num steps = 508
2025-08-16 09:58:45,043:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.382546, Time/step: 16.525104
2025-08-16 09:59:21,181:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.284946, Time/step: 7.224643
2025-08-16 10:00:10,638:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 3.009844, Time/step: 9.891345
2025-08-16 10:00:41,734:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.781805, Time/step: 6.218986
2025-08-16 10:01:39,752:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.740818, Time/step: 11.603428
2025-08-16 10:02:16,223:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.567168, Time/step: 7.294147
2025-08-16 10:02:45,196:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.303833, Time/step: 5.794534
2025-08-16 10:03:29,996:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.158302, Time/step: 8.959767
2025-08-16 10:04:35,039:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.800144, Time/step: 13.008413
2025-08-16 10:05:06,619:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.696270, Time/step: 6.316031
2025-08-16 10:05:42,626:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.473526, Time/step: 7.201100
2025-08-16 10:06:23,243:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.370212, Time/step: 8.123368
2025-08-16 10:07:19,232:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.051865, Time/step: 11.197629
2025-08-16 10:07:54,282:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 1.016152, Time/step: 7.009977
2025-08-16 10:08:32,028:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.946035, Time/step: 7.548955
2025-08-16 10:09:07,620:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.762546, Time/step: 7.118287
2025-08-16 10:10:00,992:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.949605, Time/step: 10.674258
2025-08-16 10:10:34,176:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.867702, Time/step: 6.636798
2025-08-16 10:11:09,552:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.974808, Time/step: 7.075056
2025-08-16 10:11:45,305:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.985685, Time/step: 7.150413
2025-08-16 10:12:41,854:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.708708, Time/step: 11.309771
2025-08-16 10:13:17,398:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.826333, Time/step: 7.108732
2025-08-16 10:13:50,555:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.778830, Time/step: 6.631142
2025-08-16 10:14:24,811:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.717991, Time/step: 6.851150
2025-08-16 10:15:23,732:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.799274, Time/step: 11.784093
2025-08-16 10:15:58,366:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.873406, Time/step: 6.926647
2025-08-16 10:16:32,655:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.440440, Time/step: 6.857613
2025-08-16 10:17:07,705:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 0.663300, Time/step: 7.009898
2025-08-16 10:18:10,313:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 0.840080, Time/step: 12.521489
2025-08-16 10:18:45,768:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 0.629789, Time/step: 7.090782
2025-08-16 10:19:24,029:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 0.503911, Time/step: 7.652103
2025-08-16 10:19:59,269:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 0.716545, Time/step: 7.047792
2025-08-16 10:21:11,738:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 0.597733, Time/step: 14.493770
2025-08-16 10:21:54,810:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 0.675331, Time/step: 8.595878
2025-08-16 10:22:32,139:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 0.567064, Time/step: 7.464661
2025-08-16 10:23:08,611:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 0.614704, Time/step: 7.294091
2025-08-16 10:24:05,876:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 0.723015, Time/step: 11.452955
2025-08-16 10:24:41,092:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 0.493496, Time/step: 7.043106
2025-08-16 10:25:36,673:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 0.568706, Time/step: 11.115943
2025-08-16 10:26:10,545:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.519940, Time/step: 6.772250
2025-08-16 10:27:27,803:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 0.550439, Time/step: 15.448796
2025-08-16 10:27:59,159:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 0.839690, Time/step: 6.271149
2025-08-16 10:28:36,473:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.351713, Time/step: 7.462564
2025-08-16 10:29:11,622:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.504720, Time/step: 7.027022
2025-08-16 10:30:14,559:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.501593, Time/step: 12.587230
2025-08-16 10:30:47,803:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 0.876824, Time/step: 6.648599
2025-08-16 10:31:25,605:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.420781, Time/step: 7.560134
2025-08-16 10:31:59,156:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.463989, Time/step: 6.710116
2025-08-16 10:33:01,077:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 0.759387, Time/step: 12.383971
2025-08-16 10:33:35,331:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.397619, Time/step: 6.849173
2025-08-16 10:34:11,790:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.649444, Time/step: 7.291700
2025-08-16 10:34:49,282:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.483976, Time/step: 7.498109
2025-08-16 10:35:51,357:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.435856, Time/step: 12.414778
2025-08-16 10:36:29,934:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.530973, Time/step: 7.712973
2025-08-16 10:37:03,739:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 0.501227, Time/step: 6.760847
2025-08-16 10:37:39,988:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.416137, Time/step: 7.249722
2025-08-16 10:38:41,571:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.562378, Time/step: 12.316381
2025-08-16 10:39:24,773:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.486341, Time/step: 8.640269
2025-08-16 10:40:01,425:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.500612, Time/step: 7.330261
2025-08-16 10:40:40,832:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.328919, Time/step: 7.881432
2025-08-16 10:41:57,165:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.333086, Time/step: 15.266464
2025-08-16 10:42:31,928:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.278246, Time/step: 6.952370
2025-08-16 10:43:10,406:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.339172, Time/step: 7.695393
2025-08-16 10:43:52,668:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 0.740601, Time/step: 8.452276
2025-08-16 10:45:12,446:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.419638, Time/step: 15.955572
2025-08-16 10:46:20,405:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 0.585044, Time/step: 13.482586
2025-08-16 10:47:14,591:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.636277, Time/step: 10.837113
2025-08-16 10:47:50,988:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.453399, Time/step: 7.194591
2025-08-16 10:49:11,077:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.484451, Time/step: 16.017527
2025-08-16 10:49:46,276:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.546597, Time/step: 6.998935
2025-08-16 10:50:25,369:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.436422, Time/step: 7.818507
2025-08-16 10:51:05,891:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.342455, Time/step: 8.104300
2025-08-16 10:52:08,230:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.460637, Time/step: 12.467630
2025-08-16 10:52:43,497:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.563792, Time/step: 7.053253
2025-08-16 10:53:18,949:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.466659, Time/step: 7.090414
2025-08-16 10:53:57,536:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 0.701858, Time/step: 7.717206
2025-08-16 10:55:05,217:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.599752, Time/step: 13.536053
2025-08-16 10:55:40,854:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 0.769946, Time/step: 7.127172
2025-08-16 10:56:18,871:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.342301, Time/step: 7.603245
2025-08-16 10:56:58,739:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.357406, Time/step: 7.973597
2025-08-16 10:58:06,453:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.535090, Time/step: 13.542677
2025-08-16 10:58:41,677:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.439896, Time/step: 6.743770
2025-08-16 10:59:16,676:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.500115, Time/step: 6.999578
2025-08-16 10:59:54,392:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 0.755758, Time/step: 7.543120
2025-08-16 11:00:59,151:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.502931, Time/step: 12.951625
2025-08-16 11:01:35,781:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 0.738545, Time/step: 7.325784
2025-08-16 11:02:14,897:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.481721, Time/step: 7.823164
2025-08-16 11:02:51,735:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 0.712743, Time/step: 7.367437
2025-08-16 11:04:09,226:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.546413, Time/step: 15.497957
2025-08-16 11:04:55,350:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.398336, Time/step: 9.224796
2025-08-16 11:05:31,578:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.421002, Time/step: 7.245177
2025-08-16 11:06:10,220:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.409003, Time/step: 7.728276
2025-08-16 11:07:16,330:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.477998, Time/step: 13.222015
2025-08-16 11:07:56,564:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.535465, Time/step: 8.022690
2025-08-16 11:08:50,850:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 0.835605, Time/step: 10.857062
2025-08-16 11:10:14,422:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.570114, Time/step: 16.714340
2025-08-16 11:12:04,893:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.367954, Time/step: 22.094062
2025-08-16 11:13:12,059:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.439431, Time/step: 13.433153
2025-08-16 11:14:04,840:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.418613, Time/step: 10.556059
2025-08-16 11:15:07,987:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.458056, Time/step: 12.629333
2025-08-16 11:16:36,893:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.425086, Time/step: 17.781041
2025-08-16 11:16:46,105:INFO: Epoch 1/1 Finished, Train Loss: 0.822615
2025-08-16 11:16:51,346:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0
2025-08-16 11:16:51,561:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_opt.bin.0
2025-08-16 11:16:51,561:INFO: Eval on val dataset
2025-08-16 11:16:52,086:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-16 11:16:52,086:WARNING: sentence num: 4290, video num: 100
2025-08-16 11:31:09,559:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-16 11:31:09,681:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-16 11:31:10,567:INFO: Text-to-Video:
2025-08-16 11:31:10,567:INFO: 	>>>  R@1: 65.0 - R@5: 90.3 - R@10: 95.4 - Median R: 1.0 - Mean R: 2.8
2025-08-16 11:31:10,567:INFO: Video-to-Text:
2025-08-16 11:31:10,568:INFO: 	>>>  V2T$R@1: 77.2 - V2T$R@5: 97.0 - V2T$R@10: 100.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.6
2025-08-16 11:31:10,571:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0, the R1 is: 64.9883
2025-08-16 11:31:14,262:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0
2025-08-16 11:31:19,314:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-16 11:31:19,373:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-16 11:31:19,374:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-16 11:31:19,390:WARNING: Stage-One:True, Stage-Two:False
2025-08-16 11:31:19,392:WARNING: Test retrieval by loose type.
2025-08-16 11:31:19,435:WARNING: 	 embed_dim: 512
2025-08-16 11:31:19,435:WARNING: 	 image_resolution: 224
2025-08-16 11:31:19,435:WARNING: 	 vision_layers: 12
2025-08-16 11:31:19,435:WARNING: 	 vision_width: 768
2025-08-16 11:31:19,435:WARNING: 	 vision_patch_size: 32
2025-08-16 11:31:19,435:WARNING: 	 context_length: 77
2025-08-16 11:31:19,436:WARNING: 	 vocab_size: 49408
2025-08-16 11:31:19,436:WARNING: 	 transformer_width: 512
2025-08-16 11:31:19,436:WARNING: 	 transformer_heads: 8
2025-08-16 11:31:19,436:WARNING: 	 transformer_layers: 12
2025-08-16 11:31:19,436:WARNING: 		 linear_patch: 2d
2025-08-16 11:31:19,436:WARNING: 	 cut_top_layer: 0
2025-08-16 11:31:21,546:WARNING: 	 sim_header: meanP
2025-08-16 11:31:26,337:INFO: --------------------
2025-08-16 11:31:26,338:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-16 11:31:26,648:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-16 11:31:26,648:WARNING: sentence num: 27763, video num: 670
2025-08-16 13:01:39,690:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-16 13:01:40,148:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-16 13:01:44,581:INFO: Text-to-Video:
2025-08-16 13:01:44,582:INFO: 	>>>  R@1: 37.4 - R@5: 68.9 - R@10: 80.0 - Median R: 2.0 - Mean R: 13.4
2025-08-16 13:01:44,582:INFO: Video-to-Text:
2025-08-16 13:01:44,582:INFO: 	>>>  V2T$R@1: 40.3 - V2T$R@5: 74.5 - V2T$R@10: 83.7 - V2T$Median R: 2.0 - V2T$Mean R: 7.9
2025-08-16 15:39:35,821:INFO: Effective parameters:
2025-08-16 15:39:35,821:INFO:   <<< batch_size: 96
2025-08-16 15:39:35,821:INFO:   <<< batch_size_val: 32
2025-08-16 15:39:35,821:INFO:   <<< cache_dir: 
2025-08-16 15:39:35,821:INFO: device: cuda:1 n_gpu: 2
2025-08-16 15:39:35,822:INFO:   <<< coef_lr: 0.001
2025-08-16 15:39:35,822:INFO:   <<< cross_model: cross-base
2025-08-16 15:39:35,822:INFO:   <<< cross_num_hidden_layers: 4
2025-08-16 15:39:35,822:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-16 15:39:35,822:INFO:   <<< datatype: msvd
2025-08-16 15:39:35,822:INFO:   <<< do_eval: False
2025-08-16 15:39:35,822:INFO:   <<< do_lower_case: False
2025-08-16 15:39:35,822:INFO:   <<< do_pretrain: False
2025-08-16 15:39:35,822:INFO:   <<< do_train: True
2025-08-16 15:39:35,822:INFO:   <<< epochs: 1
2025-08-16 15:39:35,823:INFO:   <<< eval_frame_order: 0
2025-08-16 15:39:35,823:INFO:   <<< expand_msrvtt_sentences: False
2025-08-16 15:39:35,823:INFO:   <<< feature_framerate: 1
2025-08-16 15:39:35,823:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-16 15:39:35,823:INFO:   <<< fp16: False
2025-08-16 15:39:35,823:INFO:   <<< fp16_opt_level: O1
2025-08-16 15:39:35,823:INFO:   <<< freeze_layer_num: 9
2025-08-16 15:39:35,823:INFO:   <<< gradient_accumulation_steps: 1
2025-08-16 15:39:35,823:INFO:   <<< hard_negative_rate: 0.5
2025-08-16 15:39:35,823:INFO:   <<< init_model: None
2025-08-16 15:39:35,823:INFO:   <<< linear_patch: 2d
2025-08-16 15:39:35,824:INFO:   <<< local_rank: 0
2025-08-16 15:39:35,824:INFO:   <<< loose_type: True
2025-08-16 15:39:35,824:INFO:   <<< lr: 0.0001
2025-08-16 15:39:35,824:INFO:   <<< lr_decay: 0.9
2025-08-16 15:39:35,824:INFO:   <<< margin: 0.1
2025-08-16 15:39:35,824:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-16 15:39:35,824:INFO:   <<< max_frames: 12
2025-08-16 15:39:35,824:INFO:   <<< max_words: 32
2025-08-16 15:39:35,824:INFO:   <<< n_display: 5
2025-08-16 15:39:35,824:INFO:   <<< n_gpu: 1
2025-08-16 15:39:35,824:INFO:   <<< n_pair: 1
2025-08-16 15:39:35,825:INFO:   <<< negative_weighting: 1
2025-08-16 15:39:35,825:INFO:   <<< new_added_modules: ['Adapter']
2025-08-16 15:39:35,825:INFO:   <<< num_thread_reader: 4
2025-08-16 15:39:35,825:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1
2025-08-16 15:39:35,825:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-16 15:39:35,825:INFO:   <<< rank: 0
2025-08-16 15:39:35,825:INFO:   <<< resume_model: None
2025-08-16 15:39:35,825:INFO:   <<< sampled_use_mil: False
2025-08-16 15:39:35,825:INFO:   <<< seed: 42
2025-08-16 15:39:35,825:INFO:   <<< sim_header: meanP
2025-08-16 15:39:35,825:INFO:   <<< slice_framepos: 0
2025-08-16 15:39:35,826:INFO:   <<< task_type: retrieval
2025-08-16 15:39:35,826:INFO:   <<< text_num_hidden_layers: 12
2025-08-16 15:39:35,826:INFO:   <<< train_csv: data/.train.csv
2025-08-16 15:39:35,826:INFO:   <<< train_frame_order: 0
2025-08-16 15:39:35,826:INFO:   <<< use_mil: False
2025-08-16 15:39:35,826:INFO:   <<< val_csv: data/.val.csv
2025-08-16 15:39:35,826:INFO:   <<< video_dim: 1024
2025-08-16 15:39:35,826:INFO:   <<< visual_num_hidden_layers: 12
2025-08-16 15:39:35,826:INFO:   <<< warmup_proportion: 0.1
2025-08-16 15:39:35,826:INFO:   <<< world_size: 2
2025-08-16 15:39:35,827:INFO: device: cuda:0 n_gpu: 2
2025-08-16 15:39:36,739:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-16 15:39:36,740:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-16 15:39:36,740:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-16 15:39:36,740:WARNING: Stage-One:True, Stage-Two:False
2025-08-16 15:39:36,740:WARNING: Test retrieval by loose type.
2025-08-16 15:39:36,740:WARNING: 	 embed_dim: 512
2025-08-16 15:39:36,740:WARNING: 	 image_resolution: 224
2025-08-16 15:39:36,740:WARNING: 	 vision_layers: 12
2025-08-16 15:39:36,740:WARNING: 	 vision_width: 768
2025-08-16 15:39:36,740:WARNING: 	 vision_patch_size: 32
2025-08-16 15:39:36,740:WARNING: 	 context_length: 77
2025-08-16 15:39:36,741:WARNING: 	 vocab_size: 49408
2025-08-16 15:39:36,741:WARNING: 	 transformer_width: 512
2025-08-16 15:39:36,741:WARNING: 	 transformer_heads: 8
2025-08-16 15:39:36,741:WARNING: 	 transformer_layers: 12
2025-08-16 15:39:36,741:WARNING: 		 linear_patch: 2d
2025-08-16 15:39:36,741:WARNING: 	 cut_top_layer: 0
2025-08-16 15:39:38,545:WARNING: 	 sim_header: meanP
2025-08-16 15:39:43,488:INFO: --------------------
2025-08-16 15:39:43,488:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-16 15:39:43,488:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-16 15:39:46,089:INFO: ***** Running test *****
2025-08-16 15:39:46,090:INFO:   Num examples = 27763
2025-08-16 15:39:46,090:INFO:   Batch size = 32
2025-08-16 15:39:46,090:INFO:   Num steps = 868
2025-08-16 15:39:46,090:INFO: ***** Running val *****
2025-08-16 15:39:46,090:INFO:   Num examples = 4290
2025-08-16 15:39:46,910:INFO: ***** Running training *****
2025-08-16 15:39:46,911:INFO:   Num examples = 48774
2025-08-16 15:39:46,911:INFO:   Batch size = 96
2025-08-16 15:39:46,911:INFO:   Num steps = 508
2025-08-16 15:41:23,296:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.148196, Time/step: 19.271606
2025-08-16 15:41:59,364:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.029150, Time/step: 7.213442
2025-08-16 15:42:49,867:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.846721, Time/step: 10.100429
2025-08-16 15:43:22,032:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.532522, Time/step: 6.432940
2025-08-16 15:44:26,370:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.522690, Time/step: 12.867340
2025-08-16 15:45:11,724:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.342144, Time/step: 9.070747
2025-08-16 15:45:50,248:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.031947, Time/step: 7.704682
2025-08-16 15:46:25,090:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.934312, Time/step: 6.968301
2025-08-16 15:47:28,204:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.567915, Time/step: 12.622577
2025-08-16 15:47:57,285:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.603093, Time/step: 5.816083
2025-08-16 15:48:32,083:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.312273, Time/step: 6.959385
2025-08-16 15:49:17,915:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.226677, Time/step: 9.166351
2025-08-16 15:50:15,103:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 0.921959, Time/step: 11.437499
2025-08-16 15:50:55,843:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.859378, Time/step: 8.147770
2025-08-16 15:51:47,469:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.921617, Time/step: 10.325021
2025-08-16 15:52:28,511:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.668513, Time/step: 8.208314
2025-08-16 15:53:19,201:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.862821, Time/step: 10.138018
2025-08-16 15:54:00,350:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.786323, Time/step: 8.229516
2025-08-16 15:54:50,448:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.938177, Time/step: 10.019578
2025-08-16 15:55:30,432:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.885353, Time/step: 7.992989
2025-08-16 15:56:42,286:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.573392, Time/step: 14.370643
2025-08-16 15:57:20,817:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.731305, Time/step: 7.703237
2025-08-16 15:58:04,917:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.760481, Time/step: 8.819833
2025-08-16 15:58:44,176:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.655350, Time/step: 7.851785
2025-08-16 15:59:48,711:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.733902, Time/step: 12.906830
2025-08-16 16:00:22,538:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.825370, Time/step: 6.765215
2025-08-16 16:01:03,318:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.348839, Time/step: 8.155948
2025-08-16 16:01:43,436:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 0.609698, Time/step: 8.023415
2025-08-16 16:03:11,736:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 0.721231, Time/step: 17.659878
2025-08-16 16:04:01,438:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 0.605915, Time/step: 9.905485
2025-08-16 16:04:39,513:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 0.455898, Time/step: 7.614919
2025-08-16 16:05:15,896:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 0.660195, Time/step: 7.276557
2025-08-16 16:06:20,011:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 0.528068, Time/step: 12.822786
2025-08-16 16:07:06,118:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 0.608431, Time/step: 9.221342
2025-08-16 16:07:44,788:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 0.523806, Time/step: 7.730597
2025-08-16 16:08:20,877:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 0.548489, Time/step: 7.213912
2025-08-16 16:09:17,469:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 0.637017, Time/step: 11.318255
2025-08-16 16:09:52,415:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 0.415331, Time/step: 6.989071
2025-08-16 16:10:35,204:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 0.573660, Time/step: 8.557863
2025-08-16 16:11:10,694:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.464245, Time/step: 7.097762
2025-08-16 16:12:10,712:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 0.507206, Time/step: 12.003580
2025-08-16 16:12:40,886:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 0.804236, Time/step: 6.034535
2025-08-16 16:13:15,697:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.299494, Time/step: 6.962077
2025-08-16 16:13:50,808:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.431916, Time/step: 7.022197
2025-08-16 16:14:44,518:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.458385, Time/step: 10.741821
2025-08-16 16:15:29,615:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 0.860310, Time/step: 9.019224
2025-08-16 16:16:01,390:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.381711, Time/step: 6.351621
2025-08-16 16:16:33,289:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.384830, Time/step: 6.379651
2025-08-16 16:17:20,533:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 0.672540, Time/step: 9.448732
2025-08-16 16:18:10,069:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.347095, Time/step: 9.906992
2025-08-16 16:18:48,729:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.649365, Time/step: 7.731911
2025-08-16 16:19:25,564:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.414047, Time/step: 7.366796
2025-08-16 16:20:08,578:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.410828, Time/step: 8.602777
2025-08-16 16:21:00,779:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.499623, Time/step: 10.440083
2025-08-16 16:21:37,043:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 0.521373, Time/step: 7.252501
2025-08-16 16:22:17,699:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.378240, Time/step: 8.131185
2025-08-16 16:23:03,250:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.559832, Time/step: 9.110064
2025-08-16 16:24:04,003:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.425397, Time/step: 12.150402
2025-08-16 16:24:36,397:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.442697, Time/step: 6.478722
2025-08-16 16:25:18,050:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.270792, Time/step: 8.330547
2025-08-16 16:25:51,210:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.318674, Time/step: 6.631832
2025-08-16 16:26:52,313:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.247607, Time/step: 12.220496
2025-08-16 16:27:25,374:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.321398, Time/step: 6.612044
2025-08-16 16:27:59,364:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 0.698588, Time/step: 6.797841
2025-08-16 16:28:42,907:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.397557, Time/step: 8.708380
2025-08-16 16:29:41,467:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 0.542392, Time/step: 11.711962
2025-08-16 16:30:14,324:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.606309, Time/step: 6.571228
2025-08-16 16:30:52,220:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.435213, Time/step: 7.579128
2025-08-16 16:31:28,312:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.468214, Time/step: 7.218209
2025-08-16 16:32:25,304:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.475645, Time/step: 11.398320
2025-08-16 16:32:57,966:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.407423, Time/step: 6.532414
2025-08-16 16:33:30,440:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.324505, Time/step: 6.494642
2025-08-16 16:34:12,557:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.450999, Time/step: 8.423171
2025-08-16 16:35:10,248:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.586890, Time/step: 11.538104
2025-08-16 16:35:43,403:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.412385, Time/step: 6.630908
2025-08-16 16:36:18,887:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 0.646746, Time/step: 7.096654
2025-08-16 16:37:01,658:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.566945, Time/step: 8.550338
2025-08-16 16:38:06,027:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 0.661983, Time/step: 12.873614
2025-08-16 16:38:28,886:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.309125, Time/step: 4.568377
2025-08-16 16:39:06,451:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.353364, Time/step: 7.512962
2025-08-16 16:40:02,279:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.430729, Time/step: 11.163874
2025-08-16 16:40:43,351:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.374018, Time/step: 8.214171
2025-08-16 16:41:19,024:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.499002, Time/step: 7.134563
2025-08-16 16:42:01,596:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 0.726005, Time/step: 8.514230
2025-08-16 16:42:45,318:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.430755, Time/step: 8.744240
2025-08-16 16:43:39,481:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 0.691638, Time/step: 10.832478
2025-08-16 16:44:18,256:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.398073, Time/step: 7.752288
2025-08-16 16:44:58,706:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 0.691327, Time/step: 8.089839
2025-08-16 16:45:36,014:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.501175, Time/step: 7.457706
2025-08-16 16:46:34,430:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.392273, Time/step: 11.683209
2025-08-16 16:47:18,012:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.369788, Time/step: 8.716175
2025-08-16 16:47:50,399:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.381867, Time/step: 6.477179
2025-08-16 16:48:23,721:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.439101, Time/step: 6.664360
2025-08-16 16:49:28,020:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.510374, Time/step: 12.859629
2025-08-16 16:50:06,004:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 0.796446, Time/step: 7.595330
2025-08-16 16:50:44,806:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.540999, Time/step: 7.760360
2025-08-16 16:51:34,370:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.328858, Time/step: 9.912561
2025-08-16 16:52:25,255:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.387807, Time/step: 10.176994
2025-08-16 16:53:06,072:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.383216, Time/step: 8.163221
2025-08-16 16:53:41,448:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.431592, Time/step: 7.075058
2025-08-16 16:54:40,104:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.416490, Time/step: 11.721069
2025-08-16 16:54:52,114:INFO: Epoch 1/1 Finished, Train Loss: 0.755069
2025-08-16 16:54:58,086:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0
2025-08-16 16:54:58,486:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_opt.bin.0
2025-08-16 16:54:58,486:INFO: Eval on val dataset
2025-08-16 16:54:59,384:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-16 16:54:59,385:WARNING: sentence num: 4290, video num: 100
2025-08-16 17:09:11,164:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-16 17:09:11,175:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-16 17:09:11,853:INFO: Text-to-Video:
2025-08-16 17:09:11,853:INFO: 	>>>  R@1: 65.2 - R@5: 90.7 - R@10: 95.9 - Median R: 1.0 - Mean R: 2.6
2025-08-16 17:09:11,853:INFO: Video-to-Text:
2025-08-16 17:09:11,865:INFO: 	>>>  V2T$R@1: 80.2 - V2T$R@5: 96.0 - V2T$R@10: 98.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.7
2025-08-16 17:09:11,869:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0, the R1 is: 65.1515
2025-08-16 17:09:12,261:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0816_teacher_1/pytorch_model.bin.0
2025-08-16 17:09:16,090:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-16 17:09:16,248:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-16 17:09:16,248:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-16 17:09:16,275:WARNING: Stage-One:True, Stage-Two:False
2025-08-16 17:09:16,275:WARNING: Test retrieval by loose type.
2025-08-16 17:09:16,295:WARNING: 	 embed_dim: 512
2025-08-16 17:09:16,295:WARNING: 	 image_resolution: 224
2025-08-16 17:09:16,296:WARNING: 	 vision_layers: 12
2025-08-16 17:09:16,296:WARNING: 	 vision_width: 768
2025-08-16 17:09:16,296:WARNING: 	 vision_patch_size: 32
2025-08-16 17:09:16,296:WARNING: 	 context_length: 77
2025-08-16 17:09:16,296:WARNING: 	 vocab_size: 49408
2025-08-16 17:09:16,296:WARNING: 	 transformer_width: 512
2025-08-16 17:09:16,296:WARNING: 	 transformer_heads: 8
2025-08-16 17:09:16,296:WARNING: 	 transformer_layers: 12
2025-08-16 17:09:16,296:WARNING: 		 linear_patch: 2d
2025-08-16 17:09:16,296:WARNING: 	 cut_top_layer: 0
2025-08-16 17:09:18,108:WARNING: 	 sim_header: meanP
2025-08-16 17:09:22,995:INFO: --------------------
2025-08-16 17:09:22,996:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-16 17:09:23,386:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-16 17:09:23,386:WARNING: sentence num: 27763, video num: 670
2025-08-16 18:39:47,655:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-16 18:39:47,994:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-16 18:39:52,214:INFO: Text-to-Video:
2025-08-16 18:39:52,214:INFO: 	>>>  R@1: 38.9 - R@5: 70.2 - R@10: 80.9 - Median R: 2.0 - Mean R: 12.4
2025-08-16 18:39:52,215:INFO: Video-to-Text:
2025-08-16 18:39:52,215:INFO: 	>>>  V2T$R@1: 41.5 - V2T$R@5: 71.2 - V2T$R@10: 81.7 - V2T$Median R: 2.0 - V2T$Mean R: 8.3
