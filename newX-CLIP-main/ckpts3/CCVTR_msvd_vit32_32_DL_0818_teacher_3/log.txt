2025-08-18 17:29:09,003:INFO: Effective parameters:
2025-08-18 17:29:09,003:INFO:   <<< amp: True
2025-08-18 17:29:09,003:INFO:   <<< batch_size: 96
2025-08-18 17:29:09,003:INFO:   <<< batch_size_val: 32
2025-08-18 17:29:09,004:INFO:   <<< cache_dir: 
2025-08-18 17:29:09,004:INFO:   <<< coef_lr: 0.001
2025-08-18 17:29:09,004:INFO:   <<< cross_model: cross-base
2025-08-18 17:29:09,004:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 17:29:09,004:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 17:29:09,004:INFO:   <<< datatype: msvd
2025-08-18 17:29:09,004:INFO:   <<< do_eval: False
2025-08-18 17:29:09,004:INFO:   <<< do_lower_case: False
2025-08-18 17:29:09,004:INFO:   <<< do_pretrain: False
2025-08-18 17:29:09,004:INFO:   <<< do_train: True
2025-08-18 17:29:09,005:INFO:   <<< epochs: 1
2025-08-18 17:29:09,005:INFO:   <<< eval_frame_order: 0
2025-08-18 17:29:09,004:INFO: device: cuda:1 n_gpu: 2
2025-08-18 17:29:09,005:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 17:29:09,005:INFO:   <<< feature_framerate: 1
2025-08-18 17:29:09,005:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 17:29:09,005:INFO:   <<< freeze_layer_num: 9
2025-08-18 17:29:09,005:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 17:29:09,005:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 17:29:09,005:INFO:   <<< init_model: None
2025-08-18 17:29:09,005:INFO:   <<< linear_patch: 2d
2025-08-18 17:29:09,005:INFO:   <<< local_rank: 0
2025-08-18 17:29:09,006:INFO:   <<< loose_type: True
2025-08-18 17:29:09,006:INFO:   <<< lr: 0.0001
2025-08-18 17:29:09,006:INFO:   <<< lr_decay: 0.9
2025-08-18 17:29:09,006:INFO:   <<< margin: 0.1
2025-08-18 17:29:09,006:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 17:29:09,006:INFO:   <<< max_frames: 12
2025-08-18 17:29:09,006:INFO:   <<< max_words: 32
2025-08-18 17:29:09,006:INFO:   <<< n_display: 5
2025-08-18 17:29:09,006:INFO:   <<< n_gpu: 1
2025-08-18 17:29:09,006:INFO:   <<< n_pair: 1
2025-08-18 17:29:09,007:INFO:   <<< negative_weighting: 1
2025-08-18 17:29:09,007:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 17:29:09,007:INFO:   <<< num_thread_reader: 4
2025-08-18 17:29:09,007:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 17:29:09,007:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 17:29:09,007:INFO:   <<< rank: 0
2025-08-18 17:29:09,007:INFO:   <<< resume_model: None
2025-08-18 17:29:09,007:INFO:   <<< sampled_use_mil: False
2025-08-18 17:29:09,007:INFO:   <<< seed: 42
2025-08-18 17:29:09,007:INFO:   <<< sim_header: meanP
2025-08-18 17:29:09,007:INFO:   <<< slice_framepos: 0
2025-08-18 17:29:09,008:INFO:   <<< task_type: retrieval
2025-08-18 17:29:09,008:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 17:29:09,008:INFO:   <<< train_csv: data/.train.csv
2025-08-18 17:29:09,008:INFO:   <<< train_frame_order: 0
2025-08-18 17:29:09,008:INFO:   <<< use_mil: False
2025-08-18 17:29:09,008:INFO:   <<< val_csv: data/.val.csv
2025-08-18 17:29:09,008:INFO:   <<< video_dim: 1024
2025-08-18 17:29:09,008:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 17:29:09,008:INFO:   <<< warmup_proportion: 0.1
2025-08-18 17:29:09,008:INFO:   <<< world_size: 2
2025-08-18 17:29:09,009:INFO: device: cuda:0 n_gpu: 2
2025-08-18 17:29:10,053:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 17:29:10,054:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 17:29:10,054:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 17:29:10,054:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 17:29:10,054:WARNING: Test retrieval by loose type.
2025-08-18 17:29:10,054:WARNING: 	 embed_dim: 512
2025-08-18 17:29:10,054:WARNING: 	 image_resolution: 224
2025-08-18 17:29:10,054:WARNING: 	 vision_layers: 12
2025-08-18 17:29:10,054:WARNING: 	 vision_width: 768
2025-08-18 17:29:10,054:WARNING: 	 vision_patch_size: 32
2025-08-18 17:29:10,054:WARNING: 	 context_length: 77
2025-08-18 17:29:10,054:WARNING: 	 vocab_size: 49408
2025-08-18 17:29:10,054:WARNING: 	 transformer_width: 512
2025-08-18 17:29:10,054:WARNING: 	 transformer_heads: 8
2025-08-18 17:29:10,054:WARNING: 	 transformer_layers: 12
2025-08-18 17:29:10,054:WARNING: 		 linear_patch: 2d
2025-08-18 17:29:10,054:WARNING: 	 cut_top_layer: 0
2025-08-18 17:29:11,782:WARNING: 	 sim_header: meanP
2025-08-18 17:29:16,550:INFO: --------------------
2025-08-18 17:29:16,550:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 17:29:16,550:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 17:29:17,131:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 17:29:17,730:INFO: ***** Running test *****
2025-08-18 17:29:17,730:INFO:   Num examples = 27763
2025-08-18 17:29:17,730:INFO:   Batch size = 32
2025-08-18 17:29:17,730:INFO:   Num steps = 868
2025-08-18 17:29:17,730:INFO: ***** Running val *****
2025-08-18 17:29:17,730:INFO:   Num examples = 4290
2025-08-18 17:29:18,421:INFO: ***** Running training *****
2025-08-18 17:29:18,421:INFO:   Num examples = 48774
2025-08-18 17:29:18,421:INFO:   Batch size = 96
2025-08-18 17:29:18,421:INFO:   Num steps = 508
2025-08-18 17:32:32,085:INFO: Effective parameters:
2025-08-18 17:32:32,086:INFO:   <<< amp: True
2025-08-18 17:32:32,086:INFO:   <<< batch_size: 96
2025-08-18 17:32:32,086:INFO:   <<< batch_size_val: 32
2025-08-18 17:32:32,086:INFO:   <<< cache_dir: 
2025-08-18 17:32:32,086:INFO:   <<< coef_lr: 0.001
2025-08-18 17:32:32,086:INFO:   <<< cross_model: cross-base
2025-08-18 17:32:32,086:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 17:32:32,086:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 17:32:32,086:INFO:   <<< datatype: msvd
2025-08-18 17:32:32,086:INFO:   <<< do_eval: False
2025-08-18 17:32:32,086:INFO:   <<< do_lower_case: False
2025-08-18 17:32:32,086:INFO:   <<< do_pretrain: False
2025-08-18 17:32:32,086:INFO:   <<< do_train: True
2025-08-18 17:32:32,086:INFO:   <<< epochs: 1
2025-08-18 17:32:32,086:INFO:   <<< eval_frame_order: 0
2025-08-18 17:32:32,086:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 17:32:32,087:INFO:   <<< feature_framerate: 1
2025-08-18 17:32:32,087:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 17:32:32,087:INFO:   <<< freeze_layer_num: 9
2025-08-18 17:32:32,087:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 17:32:32,087:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 17:32:32,087:INFO:   <<< init_model: None
2025-08-18 17:32:32,087:INFO:   <<< linear_patch: 2d
2025-08-18 17:32:32,087:INFO:   <<< local_rank: 0
2025-08-18 17:32:32,087:INFO:   <<< loose_type: True
2025-08-18 17:32:32,087:INFO:   <<< lr: 0.0001
2025-08-18 17:32:32,087:INFO:   <<< lr_decay: 0.9
2025-08-18 17:32:32,087:INFO:   <<< margin: 0.1
2025-08-18 17:32:32,087:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 17:32:32,087:INFO:   <<< max_frames: 12
2025-08-18 17:32:32,087:INFO:   <<< max_words: 32
2025-08-18 17:32:32,087:INFO:   <<< n_display: 5
2025-08-18 17:32:32,087:INFO:   <<< n_gpu: 1
2025-08-18 17:32:32,088:INFO:   <<< n_pair: 1
2025-08-18 17:32:32,088:INFO:   <<< negative_weighting: 1
2025-08-18 17:32:32,088:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 17:32:32,088:INFO:   <<< num_thread_reader: 4
2025-08-18 17:32:32,088:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 17:32:32,088:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 17:32:32,088:INFO:   <<< rank: 0
2025-08-18 17:32:32,088:INFO:   <<< resume_model: None
2025-08-18 17:32:32,088:INFO:   <<< sampled_use_mil: False
2025-08-18 17:32:32,088:INFO:   <<< seed: 42
2025-08-18 17:32:32,088:INFO:   <<< sim_header: meanP
2025-08-18 17:32:32,088:INFO:   <<< slice_framepos: 0
2025-08-18 17:32:32,088:INFO:   <<< task_type: retrieval
2025-08-18 17:32:32,088:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 17:32:32,088:INFO:   <<< train_csv: data/.train.csv
2025-08-18 17:32:32,088:INFO:   <<< train_frame_order: 0
2025-08-18 17:32:32,088:INFO:   <<< use_mil: False
2025-08-18 17:32:32,088:INFO:   <<< val_csv: data/.val.csv
2025-08-18 17:32:32,089:INFO:   <<< video_dim: 1024
2025-08-18 17:32:32,089:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 17:32:32,089:INFO:   <<< warmup_proportion: 0.1
2025-08-18 17:32:32,089:INFO:   <<< world_size: 2
2025-08-18 17:32:32,089:INFO: device: cuda:0 n_gpu: 2
2025-08-18 17:32:32,105:INFO: device: cuda:1 n_gpu: 2
2025-08-18 17:32:32,994:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 17:32:32,995:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 17:32:32,995:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 17:32:32,995:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 17:32:32,995:WARNING: Test retrieval by loose type.
2025-08-18 17:32:32,995:WARNING: 	 embed_dim: 512
2025-08-18 17:32:32,995:WARNING: 	 image_resolution: 224
2025-08-18 17:32:32,995:WARNING: 	 vision_layers: 12
2025-08-18 17:32:32,995:WARNING: 	 vision_width: 768
2025-08-18 17:32:32,995:WARNING: 	 vision_patch_size: 32
2025-08-18 17:32:32,995:WARNING: 	 context_length: 77
2025-08-18 17:32:32,995:WARNING: 	 vocab_size: 49408
2025-08-18 17:32:32,995:WARNING: 	 transformer_width: 512
2025-08-18 17:32:32,995:WARNING: 	 transformer_heads: 8
2025-08-18 17:32:32,995:WARNING: 	 transformer_layers: 12
2025-08-18 17:32:32,996:WARNING: 		 linear_patch: 2d
2025-08-18 17:32:32,996:WARNING: 	 cut_top_layer: 0
2025-08-18 17:32:34,744:WARNING: 	 sim_header: meanP
2025-08-18 17:32:39,203:INFO: --------------------
2025-08-18 17:32:39,204:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 17:32:39,204:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 17:32:39,642:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 17:32:40,230:INFO: ***** Running test *****
2025-08-18 17:32:40,230:INFO:   Num examples = 27763
2025-08-18 17:32:40,230:INFO:   Batch size = 32
2025-08-18 17:32:40,230:INFO:   Num steps = 868
2025-08-18 17:32:40,230:INFO: ***** Running val *****
2025-08-18 17:32:40,230:INFO:   Num examples = 4290
2025-08-18 17:32:41,392:INFO: ***** Running training *****
2025-08-18 17:32:41,392:INFO:   Num examples = 48774
2025-08-18 17:32:41,392:INFO:   Batch size = 96
2025-08-18 17:32:41,392:INFO:   Num steps = 508
2025-08-18 17:36:32,496:INFO: Effective parameters:
2025-08-18 17:36:32,496:INFO:   <<< amp: True
2025-08-18 17:36:32,496:INFO:   <<< batch_size: 96
2025-08-18 17:36:32,496:INFO:   <<< batch_size_val: 32
2025-08-18 17:36:32,496:INFO:   <<< cache_dir: 
2025-08-18 17:36:32,496:INFO:   <<< coef_lr: 0.001
2025-08-18 17:36:32,496:INFO:   <<< cross_model: cross-base
2025-08-18 17:36:32,496:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 17:36:32,496:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 17:36:32,496:INFO:   <<< datatype: msvd
2025-08-18 17:36:32,496:INFO:   <<< do_eval: False
2025-08-18 17:36:32,496:INFO:   <<< do_lower_case: False
2025-08-18 17:36:32,497:INFO:   <<< do_pretrain: False
2025-08-18 17:36:32,497:INFO:   <<< do_train: True
2025-08-18 17:36:32,497:INFO:   <<< epochs: 1
2025-08-18 17:36:32,497:INFO:   <<< eval_frame_order: 0
2025-08-18 17:36:32,497:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 17:36:32,497:INFO:   <<< feature_framerate: 1
2025-08-18 17:36:32,497:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 17:36:32,497:INFO:   <<< freeze_layer_num: 9
2025-08-18 17:36:32,497:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 17:36:32,497:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 17:36:32,497:INFO:   <<< init_model: None
2025-08-18 17:36:32,497:INFO:   <<< linear_patch: 2d
2025-08-18 17:36:32,497:INFO:   <<< local_rank: 0
2025-08-18 17:36:32,497:INFO:   <<< loose_type: True
2025-08-18 17:36:32,497:INFO:   <<< lr: 0.0001
2025-08-18 17:36:32,497:INFO:   <<< lr_decay: 0.9
2025-08-18 17:36:32,497:INFO:   <<< margin: 0.1
2025-08-18 17:36:32,497:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 17:36:32,498:INFO:   <<< max_frames: 12
2025-08-18 17:36:32,498:INFO:   <<< max_words: 32
2025-08-18 17:36:32,498:INFO:   <<< n_display: 5
2025-08-18 17:36:32,498:INFO:   <<< n_gpu: 1
2025-08-18 17:36:32,498:INFO:   <<< n_pair: 1
2025-08-18 17:36:32,498:INFO:   <<< negative_weighting: 1
2025-08-18 17:36:32,498:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 17:36:32,498:INFO:   <<< num_thread_reader: 4
2025-08-18 17:36:32,498:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 17:36:32,498:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 17:36:32,498:INFO:   <<< rank: 0
2025-08-18 17:36:32,498:INFO:   <<< resume_model: None
2025-08-18 17:36:32,498:INFO:   <<< sampled_use_mil: False
2025-08-18 17:36:32,498:INFO:   <<< seed: 42
2025-08-18 17:36:32,498:INFO:   <<< sim_header: meanP
2025-08-18 17:36:32,498:INFO:   <<< slice_framepos: 0
2025-08-18 17:36:32,498:INFO:   <<< task_type: retrieval
2025-08-18 17:36:32,498:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 17:36:32,498:INFO:   <<< train_csv: data/.train.csv
2025-08-18 17:36:32,499:INFO:   <<< train_frame_order: 0
2025-08-18 17:36:32,499:INFO:   <<< use_mil: False
2025-08-18 17:36:32,499:INFO:   <<< val_csv: data/.val.csv
2025-08-18 17:36:32,499:INFO:   <<< video_dim: 1024
2025-08-18 17:36:32,499:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 17:36:32,499:INFO:   <<< warmup_proportion: 0.1
2025-08-18 17:36:32,499:INFO:   <<< world_size: 2
2025-08-18 17:36:32,499:INFO: device: cuda:0 n_gpu: 2
2025-08-18 17:36:32,515:INFO: device: cuda:1 n_gpu: 2
2025-08-18 17:36:33,365:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 17:36:33,366:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 17:36:33,366:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 17:36:33,366:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 17:36:33,366:WARNING: Test retrieval by loose type.
2025-08-18 17:36:33,366:WARNING: 	 embed_dim: 512
2025-08-18 17:36:33,366:WARNING: 	 image_resolution: 224
2025-08-18 17:36:33,366:WARNING: 	 vision_layers: 12
2025-08-18 17:36:33,366:WARNING: 	 vision_width: 768
2025-08-18 17:36:33,366:WARNING: 	 vision_patch_size: 32
2025-08-18 17:36:33,366:WARNING: 	 context_length: 77
2025-08-18 17:36:33,366:WARNING: 	 vocab_size: 49408
2025-08-18 17:36:33,366:WARNING: 	 transformer_width: 512
2025-08-18 17:36:33,366:WARNING: 	 transformer_heads: 8
2025-08-18 17:36:33,366:WARNING: 	 transformer_layers: 12
2025-08-18 17:36:33,366:WARNING: 		 linear_patch: 2d
2025-08-18 17:36:33,366:WARNING: 	 cut_top_layer: 0
2025-08-18 17:36:35,082:WARNING: 	 sim_header: meanP
2025-08-18 17:36:39,530:INFO: --------------------
2025-08-18 17:36:39,530:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 17:36:39,530:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 17:36:39,997:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 17:36:40,497:INFO: ***** Running test *****
2025-08-18 17:36:40,497:INFO:   Num examples = 27763
2025-08-18 17:36:40,497:INFO:   Batch size = 32
2025-08-18 17:36:40,497:INFO:   Num steps = 868
2025-08-18 17:36:40,497:INFO: ***** Running val *****
2025-08-18 17:36:40,497:INFO:   Num examples = 4290
2025-08-18 17:36:41,169:INFO: ***** Running training *****
2025-08-18 17:36:41,169:INFO:   Num examples = 48774
2025-08-18 17:36:41,169:INFO:   Batch size = 96
2025-08-18 17:36:41,169:INFO:   Num steps = 508
2025-08-18 17:37:47,471:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.161426, Time/step: 13.246460
2025-08-18 17:38:26,698:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.085009, Time/step: 7.845195
2025-08-18 17:39:06,452:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.932509, Time/step: 7.950730
2025-08-18 17:39:37,004:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.599844, Time/step: 6.110392
2025-08-18 17:40:36,851:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.630015, Time/step: 11.969175
2025-08-18 17:41:13,973:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.486357, Time/step: 7.424211
2025-08-18 17:41:47,717:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.203114, Time/step: 6.748666
2025-08-18 17:42:17,989:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.184721, Time/step: 6.054357
2025-08-18 17:43:20,290:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.928461, Time/step: 12.459951
2025-08-18 17:43:44,799:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.986707, Time/step: 4.901686
2025-08-18 17:44:21,963:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.883633, Time/step: 7.432576
2025-08-18 17:45:04,901:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.834529, Time/step: 8.587614
2025-08-18 17:45:52,046:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.569236, Time/step: 9.428891
2025-08-18 17:46:32,501:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 1.562603, Time/step: 8.090905
2025-08-18 17:47:16,493:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 1.606098, Time/step: 8.798230
2025-08-18 17:47:52,374:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 1.403099, Time/step: 7.176072
2025-08-18 17:48:36,172:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 1.476207, Time/step: 8.759447
2025-08-18 17:49:13,656:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 1.498378, Time/step: 7.496689
2025-08-18 17:49:54,519:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 1.553866, Time/step: 8.172349
2025-08-18 17:50:27,805:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 1.544838, Time/step: 6.657181
2025-08-18 17:51:21,986:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 1.175345, Time/step: 10.836113
2025-08-18 17:51:55,604:INFO: Epoch: 1/1, Step: 110/508, Lr: , Loss: 1.316082, Time/step: 6.723412
2025-08-18 17:52:40,125:INFO: Epoch: 1/1, Step: 115/508, Lr: , Loss: 1.293036, Time/step: 8.904053
2025-08-18 17:53:12,312:INFO: Epoch: 1/1, Step: 120/508, Lr: , Loss: 1.202477, Time/step: 6.437348
2025-08-18 17:54:06,719:INFO: Epoch: 1/1, Step: 125/508, Lr: , Loss: 1.369235, Time/step: 10.881191
2025-08-18 17:54:38,994:INFO: Epoch: 1/1, Step: 130/508, Lr: , Loss: 1.429738, Time/step: 6.454845
2025-08-18 17:55:20,539:INFO: Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.933321, Time/step: 8.308952
2025-08-18 17:55:54,149:INFO: Epoch: 1/1, Step: 140/508, Lr: , Loss: 1.181649, Time/step: 6.721774
2025-08-18 17:56:52,431:INFO: Epoch: 1/1, Step: 145/508, Lr: , Loss: 1.283373, Time/step: 11.656304
2025-08-18 17:57:29,224:INFO: Epoch: 1/1, Step: 150/508, Lr: , Loss: 1.123219, Time/step: 7.358436
2025-08-18 17:58:04,448:INFO: Epoch: 1/1, Step: 155/508, Lr: , Loss: 1.127915, Time/step: 7.044674
2025-08-18 17:58:37,605:INFO: Epoch: 1/1, Step: 160/508, Lr: , Loss: 1.161927, Time/step: 6.631194
2025-08-18 17:59:36,634:INFO: Epoch: 1/1, Step: 165/508, Lr: , Loss: 1.138118, Time/step: 11.805762
2025-08-18 18:00:17,453:INFO: Epoch: 1/1, Step: 170/508, Lr: , Loss: 1.096594, Time/step: 8.163590
2025-08-18 18:00:56,412:INFO: Epoch: 1/1, Step: 175/508, Lr: , Loss: 1.107583, Time/step: 7.791730
2025-08-18 18:01:30,925:INFO: Epoch: 1/1, Step: 180/508, Lr: , Loss: 1.156983, Time/step: 6.902431
2025-08-18 18:02:27,194:INFO: Epoch: 1/1, Step: 185/508, Lr: , Loss: 1.179742, Time/step: 11.253794
2025-08-18 18:03:01,490:INFO: Epoch: 1/1, Step: 190/508, Lr: , Loss: 1.012215, Time/step: 6.859085
2025-08-18 18:03:48,743:INFO: Epoch: 1/1, Step: 195/508, Lr: , Loss: 1.041937, Time/step: 9.450497
2025-08-18 18:04:20,500:INFO: Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.957414, Time/step: 6.351155
2025-08-18 18:05:21,872:INFO: Epoch: 1/1, Step: 205/508, Lr: , Loss: 1.000951, Time/step: 12.274240
2025-08-18 18:05:56,082:INFO: Epoch: 1/1, Step: 210/508, Lr: , Loss: 1.244059, Time/step: 6.841869
2025-08-18 18:06:34,402:INFO: Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.843953, Time/step: 7.663966
2025-08-18 18:07:21,135:INFO: Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.859549, Time/step: 9.346547
2025-08-18 18:08:24,505:INFO: Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.884872, Time/step: 12.673782
2025-08-18 18:09:05,310:INFO: Epoch: 1/1, Step: 230/508, Lr: , Loss: 1.259686, Time/step: 8.160812
2025-08-18 18:09:37,174:INFO: Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.892167, Time/step: 6.372672
2025-08-18 18:10:08,217:INFO: Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.937097, Time/step: 6.208399
2025-08-18 18:11:05,473:INFO: Epoch: 1/1, Step: 245/508, Lr: , Loss: 1.172001, Time/step: 11.451114
2025-08-18 18:11:45,453:INFO: Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.973590, Time/step: 7.995824
2025-08-18 18:12:22,880:INFO: Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.992582, Time/step: 7.485335
2025-08-18 18:12:57,364:INFO: Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.904333, Time/step: 6.896545
2025-08-18 18:13:49,841:INFO: Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.883007, Time/step: 10.495320
2025-08-18 18:14:28,766:INFO: Epoch: 1/1, Step: 270/508, Lr: , Loss: 1.000435, Time/step: 7.785007
2025-08-18 18:15:02,317:INFO: Epoch: 1/1, Step: 275/508, Lr: , Loss: 1.078750, Time/step: 6.709914
2025-08-18 18:15:41,793:INFO: Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.797094, Time/step: 7.895198
2025-08-18 18:16:24,490:INFO: Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.967564, Time/step: 8.539256
2025-08-18 18:17:16,345:INFO: Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.904698, Time/step: 10.370857
2025-08-18 18:17:47,434:INFO: Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.883788, Time/step: 6.217591
2025-08-18 18:18:24,599:INFO: Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.739349, Time/step: 7.432969
2025-08-18 18:19:02,840:INFO: Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.767068, Time/step: 7.647994
2025-08-18 18:19:56,633:INFO: Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.714174, Time/step: 10.758537
2025-08-18 18:20:28,725:INFO: Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.804595, Time/step: 6.418276
2025-08-18 18:21:01,214:INFO: Epoch: 1/1, Step: 320/508, Lr: , Loss: 1.106791, Time/step: 6.497769
2025-08-18 18:21:51,267:INFO: Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.806202, Time/step: 10.006044
2025-08-18 18:22:40,600:INFO: Epoch: 1/1, Step: 330/508, Lr: , Loss: 1.012513, Time/step: 9.866504
2025-08-18 18:23:12,355:INFO: Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.987066, Time/step: 6.346520
2025-08-18 18:23:49,205:INFO: Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.899571, Time/step: 7.370012
2025-08-18 18:24:29,034:INFO: Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.924871, Time/step: 7.965521
2025-08-18 18:25:19,333:INFO: Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.916904, Time/step: 10.050457
2025-08-18 18:25:51,598:INFO: Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.727529, Time/step: 6.452748
2025-08-18 18:26:23,121:INFO: Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.621280, Time/step: 6.304520
2025-08-18 18:27:14,260:INFO: Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.889139, Time/step: 10.227668
2025-08-18 18:27:57,347:INFO: Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.947095, Time/step: 8.617139
2025-08-18 18:28:29,165:INFO: Epoch: 1/1, Step: 375/508, Lr: , Loss: 0.999187, Time/step: 6.363590
2025-08-18 18:29:03,220:INFO: Epoch: 1/1, Step: 380/508, Lr: , Loss: 1.038437, Time/step: 6.810739
2025-08-18 18:29:55,457:INFO: Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.972125, Time/step: 10.447436
2025-08-18 18:30:35,630:INFO: Epoch: 1/1, Step: 390/508, Lr: , Loss: 1.008993, Time/step: 8.034315
2025-08-18 18:31:04,008:INFO: Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.698121, Time/step: 5.675450
2025-08-18 18:31:38,220:INFO: Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.812434, Time/step: 6.842278
2025-08-18 18:32:25,804:INFO: Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.906049, Time/step: 9.516600
2025-08-18 18:33:08,833:INFO: Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.872121, Time/step: 8.605650
2025-08-18 18:33:44,081:INFO: Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.854274, Time/step: 7.049638
2025-08-18 18:34:25,239:INFO: Epoch: 1/1, Step: 420/508, Lr: , Loss: 1.103003, Time/step: 8.231358
2025-08-18 18:35:07,125:INFO: Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.899185, Time/step: 8.376967
2025-08-18 18:35:57,147:INFO: Epoch: 1/1, Step: 430/508, Lr: , Loss: 1.146560, Time/step: 10.004262
2025-08-18 18:36:34,626:INFO: Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.913592, Time/step: 7.495834
2025-08-18 18:37:11,280:INFO: Epoch: 1/1, Step: 440/508, Lr: , Loss: 1.190752, Time/step: 7.330528
2025-08-18 18:37:48,191:INFO: Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.916774, Time/step: 7.382086
2025-08-18 18:38:37,479:INFO: Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.846891, Time/step: 9.857545
2025-08-18 18:39:19,754:INFO: Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.805264, Time/step: 8.452499
2025-08-18 18:39:49,291:INFO: Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.736234, Time/step: 5.907170
2025-08-18 18:40:21,117:INFO: Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.860758, Time/step: 6.365082
2025-08-18 18:41:21,134:INFO: Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.952434, Time/step: 12.003271
2025-08-18 18:41:55,836:INFO: Epoch: 1/1, Step: 475/508, Lr: , Loss: 1.144332, Time/step: 6.940374
2025-08-18 18:42:31,765:INFO: Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.886798, Time/step: 7.185644
2025-08-18 18:43:07,525:INFO: Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.848738, Time/step: 7.151832
2025-08-18 18:44:02,316:INFO: Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.799007, Time/step: 10.958149
2025-08-18 18:44:40,551:INFO: Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.908516, Time/step: 7.646784
2025-08-18 18:45:14,595:INFO: Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.887245, Time/step: 6.808803
2025-08-18 18:45:46,521:INFO: Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.866812, Time/step: 6.385056
2025-08-18 18:46:21,689:INFO: Epoch 1/1 Finished, Train Loss: 1.214681
2025-08-18 18:46:25,236:INFO: Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3/pytorch_model.bin.0
2025-08-18 18:46:25,238:INFO: Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3/pytorch_opt.bin.0
2025-08-18 18:46:25,238:INFO: Eval on val dataset
2025-08-18 18:46:25,532:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-18 18:46:25,533:WARNING: sentence num: 4290, video num: 100
2025-08-18 19:00:19,975:INFO: before reshape, sim matrix size: 4290 x 100
2025-08-18 19:00:19,982:INFO: after reshape, sim matrix size: 100 x 62 x 100
2025-08-18 19:00:20,663:INFO: Text-to-Video:
2025-08-18 19:00:20,663:INFO: 	>>>  R@1: 59.0 - R@5: 86.8 - R@10: 92.6 - Median R: 1.0 - Mean R: 3.7
2025-08-18 19:00:20,663:INFO: Video-to-Text:
2025-08-18 19:00:20,670:INFO: 	>>>  V2T$R@1: 77.7 - V2T$R@5: 97.1 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.7
2025-08-18 19:00:20,677:INFO: The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3/pytorch_model.bin.0, the R1 is: 58.9977
2025-08-18 19:00:21,249:INFO: Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3/pytorch_model.bin.0
2025-08-18 19:00:26,516:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 19:00:26,519:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 19:00:26,519:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 19:00:26,520:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 19:00:26,520:WARNING: Test retrieval by loose type.
2025-08-18 19:00:26,536:WARNING: 	 embed_dim: 512
2025-08-18 19:00:26,536:WARNING: 	 image_resolution: 224
2025-08-18 19:00:26,536:WARNING: 	 vision_layers: 12
2025-08-18 19:00:26,536:WARNING: 	 vision_width: 768
2025-08-18 19:00:26,536:WARNING: 	 vision_patch_size: 32
2025-08-18 19:00:26,536:WARNING: 	 context_length: 77
2025-08-18 19:00:26,536:WARNING: 	 vocab_size: 49408
2025-08-18 19:00:26,536:WARNING: 	 transformer_width: 512
2025-08-18 19:00:26,536:WARNING: 	 transformer_heads: 8
2025-08-18 19:00:26,536:WARNING: 	 transformer_layers: 12
2025-08-18 19:00:26,536:WARNING: 		 linear_patch: 2d
2025-08-18 19:00:26,537:WARNING: 	 cut_top_layer: 0
2025-08-18 19:00:28,452:WARNING: 	 sim_header: meanP
2025-08-18 19:00:33,585:INFO: --------------------
2025-08-18 19:00:33,585:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 19:00:33,767:WARNING: Eval under the multi-sentence per video clip setting.
2025-08-18 19:00:33,767:WARNING: sentence num: 27763, video num: 670
2025-08-18 20:30:29,664:INFO: before reshape, sim matrix size: 27763 x 670
2025-08-18 20:30:30,041:INFO: after reshape, sim matrix size: 670 x 81 x 670
2025-08-18 20:30:34,270:INFO: Text-to-Video:
2025-08-18 20:30:34,270:INFO: 	>>>  R@1: 29.2 - R@5: 59.3 - R@10: 71.6 - Median R: 4.0 - Mean R: 21.3
2025-08-18 20:30:34,270:INFO: Video-to-Text:
2025-08-18 20:30:34,270:INFO: 	>>>  V2T$R@1: 41.5 - V2T$R@5: 73.1 - V2T$R@10: 84.9 - V2T$Median R: 2.0 - V2T$Mean R: 9.0
2025-08-18 20:31:44,720:INFO: Effective parameters:
2025-08-18 20:31:44,721:INFO:   <<< amp: True
2025-08-18 20:31:44,721:INFO:   <<< batch_size: 96
2025-08-18 20:31:44,721:INFO:   <<< batch_size_val: 32
2025-08-18 20:31:44,721:INFO:   <<< cache_dir: 
2025-08-18 20:31:44,721:INFO:   <<< coef_lr: 0.001
2025-08-18 20:31:44,721:INFO: device: cuda:1 n_gpu: 2
2025-08-18 20:31:44,721:INFO:   <<< cross_model: cross-base
2025-08-18 20:31:44,721:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 20:31:44,721:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 20:31:44,721:INFO:   <<< datatype: msvd
2025-08-18 20:31:44,722:INFO:   <<< do_eval: False
2025-08-18 20:31:44,722:INFO:   <<< do_lower_case: False
2025-08-18 20:31:44,722:INFO:   <<< do_pretrain: False
2025-08-18 20:31:44,722:INFO:   <<< do_train: True
2025-08-18 20:31:44,722:INFO:   <<< epochs: 1
2025-08-18 20:31:44,722:INFO:   <<< eval_frame_order: 0
2025-08-18 20:31:44,722:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 20:31:44,722:INFO:   <<< feature_framerate: 1
2025-08-18 20:31:44,722:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 20:31:44,722:INFO:   <<< freeze_layer_num: 9
2025-08-18 20:31:44,723:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 20:31:44,723:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 20:31:44,723:INFO:   <<< init_model: None
2025-08-18 20:31:44,723:INFO:   <<< linear_patch: 2d
2025-08-18 20:31:44,723:INFO:   <<< local_rank: 0
2025-08-18 20:31:44,723:INFO:   <<< loose_type: True
2025-08-18 20:31:44,723:INFO:   <<< lr: 0.0001
2025-08-18 20:31:44,723:INFO:   <<< lr_decay: 0.9
2025-08-18 20:31:44,723:INFO:   <<< margin: 0.1
2025-08-18 20:31:44,723:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 20:31:44,724:INFO:   <<< max_frames: 12
2025-08-18 20:31:44,724:INFO:   <<< max_words: 32
2025-08-18 20:31:44,724:INFO:   <<< n_display: 5
2025-08-18 20:31:44,724:INFO:   <<< n_gpu: 1
2025-08-18 20:31:44,724:INFO:   <<< n_pair: 1
2025-08-18 20:31:44,724:INFO:   <<< negative_weighting: 1
2025-08-18 20:31:44,724:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 20:31:44,724:INFO:   <<< num_thread_reader: 4
2025-08-18 20:31:44,724:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 20:31:44,724:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 20:31:44,725:INFO:   <<< rank: 0
2025-08-18 20:31:44,725:INFO:   <<< resume_model: None
2025-08-18 20:31:44,725:INFO:   <<< sampled_use_mil: False
2025-08-18 20:31:44,725:INFO:   <<< seed: 42
2025-08-18 20:31:44,725:INFO:   <<< sim_header: meanP
2025-08-18 20:31:44,725:INFO:   <<< slice_framepos: 0
2025-08-18 20:31:44,725:INFO:   <<< task_type: retrieval
2025-08-18 20:31:44,725:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 20:31:44,725:INFO:   <<< train_csv: data/.train.csv
2025-08-18 20:31:44,725:INFO:   <<< train_frame_order: 0
2025-08-18 20:31:44,725:INFO:   <<< use_mil: False
2025-08-18 20:31:44,726:INFO:   <<< val_csv: data/.val.csv
2025-08-18 20:31:44,726:INFO:   <<< video_dim: 1024
2025-08-18 20:31:44,726:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 20:31:44,726:INFO:   <<< warmup_proportion: 0.1
2025-08-18 20:31:44,726:INFO:   <<< world_size: 2
2025-08-18 20:31:44,727:INFO: device: cuda:0 n_gpu: 2
2025-08-18 20:31:45,767:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 20:31:45,767:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 20:31:45,767:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 20:31:45,768:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 20:31:45,768:WARNING: Test retrieval by loose type.
2025-08-18 20:31:45,768:WARNING: 	 embed_dim: 512
2025-08-18 20:31:45,768:WARNING: 	 image_resolution: 224
2025-08-18 20:31:45,768:WARNING: 	 vision_layers: 12
2025-08-18 20:31:45,768:WARNING: 	 vision_width: 768
2025-08-18 20:31:45,768:WARNING: 	 vision_patch_size: 32
2025-08-18 20:31:45,768:WARNING: 	 context_length: 77
2025-08-18 20:31:45,768:WARNING: 	 vocab_size: 49408
2025-08-18 20:31:45,768:WARNING: 	 transformer_width: 512
2025-08-18 20:31:45,768:WARNING: 	 transformer_heads: 8
2025-08-18 20:31:45,768:WARNING: 	 transformer_layers: 12
2025-08-18 20:31:45,768:WARNING: 		 linear_patch: 2d
2025-08-18 20:31:45,768:WARNING: 	 cut_top_layer: 0
2025-08-18 20:31:47,475:WARNING: 	 sim_header: meanP
2025-08-18 20:31:52,130:INFO: --------------------
2025-08-18 20:31:52,131:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 20:31:52,131:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 20:31:52,768:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 20:31:53,425:INFO: ***** Running test *****
2025-08-18 20:31:53,425:INFO:   Num examples = 27763
2025-08-18 20:31:53,426:INFO:   Batch size = 32
2025-08-18 20:31:53,426:INFO:   Num steps = 868
2025-08-18 20:31:53,426:INFO: ***** Running val *****
2025-08-18 20:31:53,426:INFO:   Num examples = 4290
2025-08-18 20:31:54,146:INFO: ***** Running training *****
2025-08-18 20:31:54,146:INFO:   Num examples = 48774
2025-08-18 20:31:54,146:INFO:   Batch size = 96
2025-08-18 20:31:54,146:INFO:   Num steps = 508
2025-08-18 20:35:54,558:INFO: device: cuda:1 n_gpu: 2
2025-08-18 20:35:54,570:INFO: Effective parameters:
2025-08-18 20:35:54,571:INFO:   <<< amp: True
2025-08-18 20:35:54,571:INFO:   <<< batch_size: 96
2025-08-18 20:35:54,571:INFO:   <<< batch_size_val: 32
2025-08-18 20:35:54,571:INFO:   <<< cache_dir: 
2025-08-18 20:35:54,571:INFO:   <<< coef_lr: 0.001
2025-08-18 20:35:54,571:INFO:   <<< cross_model: cross-base
2025-08-18 20:35:54,572:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 20:35:54,572:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 20:35:54,572:INFO:   <<< datatype: msvd
2025-08-18 20:35:54,572:INFO:   <<< do_eval: False
2025-08-18 20:35:54,572:INFO:   <<< do_lower_case: False
2025-08-18 20:35:54,572:INFO:   <<< do_pretrain: False
2025-08-18 20:35:54,572:INFO:   <<< do_train: True
2025-08-18 20:35:54,572:INFO:   <<< epochs: 1
2025-08-18 20:35:54,573:INFO:   <<< eval_frame_order: 0
2025-08-18 20:35:54,573:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 20:35:54,573:INFO:   <<< feature_framerate: 1
2025-08-18 20:35:54,573:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 20:35:54,573:INFO:   <<< freeze_layer_num: 9
2025-08-18 20:35:54,573:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 20:35:54,573:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 20:35:54,573:INFO:   <<< init_model: None
2025-08-18 20:35:54,573:INFO:   <<< linear_patch: 2d
2025-08-18 20:35:54,574:INFO:   <<< local_rank: 0
2025-08-18 20:35:54,574:INFO:   <<< loose_type: True
2025-08-18 20:35:54,574:INFO:   <<< lr: 0.0001
2025-08-18 20:35:54,574:INFO:   <<< lr_decay: 0.9
2025-08-18 20:35:54,574:INFO:   <<< margin: 0.1
2025-08-18 20:35:54,574:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 20:35:54,574:INFO:   <<< max_frames: 12
2025-08-18 20:35:54,574:INFO:   <<< max_words: 32
2025-08-18 20:35:54,575:INFO:   <<< n_display: 5
2025-08-18 20:35:54,575:INFO:   <<< n_gpu: 1
2025-08-18 20:35:54,575:INFO:   <<< n_pair: 1
2025-08-18 20:35:54,575:INFO:   <<< negative_weighting: 1
2025-08-18 20:35:54,575:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 20:35:54,575:INFO:   <<< num_thread_reader: 4
2025-08-18 20:35:54,575:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 20:35:54,575:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 20:35:54,576:INFO:   <<< rank: 0
2025-08-18 20:35:54,576:INFO:   <<< resume_model: None
2025-08-18 20:35:54,576:INFO:   <<< sampled_use_mil: False
2025-08-18 20:35:54,576:INFO:   <<< seed: 42
2025-08-18 20:35:54,576:INFO:   <<< sim_header: meanP
2025-08-18 20:35:54,576:INFO:   <<< slice_framepos: 0
2025-08-18 20:35:54,576:INFO:   <<< task_type: retrieval
2025-08-18 20:35:54,576:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 20:35:54,576:INFO:   <<< train_csv: data/.train.csv
2025-08-18 20:35:54,577:INFO:   <<< train_frame_order: 0
2025-08-18 20:35:54,577:INFO:   <<< use_mil: False
2025-08-18 20:35:54,577:INFO:   <<< val_csv: data/.val.csv
2025-08-18 20:35:54,577:INFO:   <<< video_dim: 1024
2025-08-18 20:35:54,577:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 20:35:54,577:INFO:   <<< warmup_proportion: 0.1
2025-08-18 20:35:54,577:INFO:   <<< world_size: 2
2025-08-18 20:35:54,578:INFO: device: cuda:0 n_gpu: 2
2025-08-18 20:35:55,486:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 20:35:55,486:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 20:35:55,486:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 20:35:55,486:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 20:35:55,486:WARNING: Test retrieval by loose type.
2025-08-18 20:35:55,487:WARNING: 	 embed_dim: 512
2025-08-18 20:35:55,487:WARNING: 	 image_resolution: 224
2025-08-18 20:35:55,487:WARNING: 	 vision_layers: 12
2025-08-18 20:35:55,487:WARNING: 	 vision_width: 768
2025-08-18 20:35:55,487:WARNING: 	 vision_patch_size: 32
2025-08-18 20:35:55,487:WARNING: 	 context_length: 77
2025-08-18 20:35:55,487:WARNING: 	 vocab_size: 49408
2025-08-18 20:35:55,487:WARNING: 	 transformer_width: 512
2025-08-18 20:35:55,487:WARNING: 	 transformer_heads: 8
2025-08-18 20:35:55,487:WARNING: 	 transformer_layers: 12
2025-08-18 20:35:55,487:WARNING: 		 linear_patch: 2d
2025-08-18 20:35:55,487:WARNING: 	 cut_top_layer: 0
2025-08-18 20:35:57,303:WARNING: 	 sim_header: meanP
2025-08-18 20:36:02,046:INFO: --------------------
2025-08-18 20:36:02,047:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 20:36:02,047:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 20:36:02,488:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 20:36:03,151:INFO: ***** Running test *****
2025-08-18 20:36:03,151:INFO:   Num examples = 27763
2025-08-18 20:36:03,151:INFO:   Batch size = 32
2025-08-18 20:36:03,151:INFO:   Num steps = 868
2025-08-18 20:36:03,152:INFO: ***** Running val *****
2025-08-18 20:36:03,152:INFO:   Num examples = 4290
2025-08-18 20:36:03,660:INFO: ***** Running training *****
2025-08-18 20:36:03,661:INFO:   Num examples = 48774
2025-08-18 20:36:03,661:INFO:   Batch size = 96
2025-08-18 20:36:03,661:INFO:   Num steps = 508
2025-08-18 20:41:02,526:INFO: device: cuda:1 n_gpu: 2
2025-08-18 20:41:02,537:INFO: Effective parameters:
2025-08-18 20:41:02,537:INFO:   <<< amp: True
2025-08-18 20:41:02,537:INFO:   <<< batch_size: 96
2025-08-18 20:41:02,537:INFO:   <<< batch_size_val: 32
2025-08-18 20:41:02,537:INFO:   <<< cache_dir: 
2025-08-18 20:41:02,538:INFO:   <<< coef_lr: 0.001
2025-08-18 20:41:02,538:INFO:   <<< cross_model: cross-base
2025-08-18 20:41:02,538:INFO:   <<< cross_num_hidden_layers: 4
2025-08-18 20:41:02,538:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-18 20:41:02,538:INFO:   <<< datatype: msvd
2025-08-18 20:41:02,538:INFO:   <<< do_eval: False
2025-08-18 20:41:02,538:INFO:   <<< do_lower_case: False
2025-08-18 20:41:02,538:INFO:   <<< do_pretrain: False
2025-08-18 20:41:02,538:INFO:   <<< do_train: True
2025-08-18 20:41:02,538:INFO:   <<< epochs: 1
2025-08-18 20:41:02,539:INFO:   <<< eval_frame_order: 0
2025-08-18 20:41:02,539:INFO:   <<< expand_msrvtt_sentences: False
2025-08-18 20:41:02,539:INFO:   <<< feature_framerate: 1
2025-08-18 20:41:02,539:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-18 20:41:02,539:INFO:   <<< freeze_layer_num: 9
2025-08-18 20:41:02,539:INFO:   <<< gradient_accumulation_steps: 1
2025-08-18 20:41:02,539:INFO:   <<< hard_negative_rate: 0.5
2025-08-18 20:41:02,539:INFO:   <<< init_model: None
2025-08-18 20:41:02,539:INFO:   <<< linear_patch: 2d
2025-08-18 20:41:02,539:INFO:   <<< local_rank: 0
2025-08-18 20:41:02,540:INFO:   <<< loose_type: True
2025-08-18 20:41:02,540:INFO:   <<< lr: 0.0001
2025-08-18 20:41:02,540:INFO:   <<< lr_decay: 0.9
2025-08-18 20:41:02,540:INFO:   <<< margin: 0.1
2025-08-18 20:41:02,540:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-18 20:41:02,540:INFO:   <<< max_frames: 12
2025-08-18 20:41:02,540:INFO:   <<< max_words: 32
2025-08-18 20:41:02,540:INFO:   <<< n_display: 5
2025-08-18 20:41:02,540:INFO:   <<< n_gpu: 1
2025-08-18 20:41:02,540:INFO:   <<< n_pair: 1
2025-08-18 20:41:02,541:INFO:   <<< negative_weighting: 1
2025-08-18 20:41:02,541:INFO:   <<< new_added_modules: ['Adapter']
2025-08-18 20:41:02,541:INFO:   <<< num_thread_reader: 4
2025-08-18 20:41:02,541:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_3
2025-08-18 20:41:02,541:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-18 20:41:02,541:INFO:   <<< rank: 0
2025-08-18 20:41:02,541:INFO:   <<< resume_model: None
2025-08-18 20:41:02,541:INFO:   <<< sampled_use_mil: False
2025-08-18 20:41:02,541:INFO:   <<< seed: 42
2025-08-18 20:41:02,541:INFO:   <<< sim_header: meanP
2025-08-18 20:41:02,541:INFO:   <<< slice_framepos: 0
2025-08-18 20:41:02,542:INFO:   <<< task_type: retrieval
2025-08-18 20:41:02,542:INFO:   <<< text_num_hidden_layers: 12
2025-08-18 20:41:02,542:INFO:   <<< train_csv: data/.train.csv
2025-08-18 20:41:02,542:INFO:   <<< train_frame_order: 0
2025-08-18 20:41:02,542:INFO:   <<< use_mil: False
2025-08-18 20:41:02,542:INFO:   <<< val_csv: data/.val.csv
2025-08-18 20:41:02,542:INFO:   <<< video_dim: 1024
2025-08-18 20:41:02,542:INFO:   <<< visual_num_hidden_layers: 12
2025-08-18 20:41:02,542:INFO:   <<< warmup_proportion: 0.1
2025-08-18 20:41:02,542:INFO:   <<< world_size: 2
2025-08-18 20:41:02,543:INFO: device: cuda:0 n_gpu: 2
2025-08-18 20:41:03,469:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-18 20:41:03,469:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-18 20:41:03,469:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-18 20:41:03,469:WARNING: Stage-One:True, Stage-Two:False
2025-08-18 20:41:03,469:WARNING: Test retrieval by loose type.
2025-08-18 20:41:03,470:WARNING: 	 embed_dim: 512
2025-08-18 20:41:03,470:WARNING: 	 image_resolution: 224
2025-08-18 20:41:03,470:WARNING: 	 vision_layers: 12
2025-08-18 20:41:03,470:WARNING: 	 vision_width: 768
2025-08-18 20:41:03,470:WARNING: 	 vision_patch_size: 32
2025-08-18 20:41:03,470:WARNING: 	 context_length: 77
2025-08-18 20:41:03,470:WARNING: 	 vocab_size: 49408
2025-08-18 20:41:03,470:WARNING: 	 transformer_width: 512
2025-08-18 20:41:03,470:WARNING: 	 transformer_heads: 8
2025-08-18 20:41:03,470:WARNING: 	 transformer_layers: 12
2025-08-18 20:41:03,470:WARNING: 		 linear_patch: 2d
2025-08-18 20:41:03,470:WARNING: 	 cut_top_layer: 0
2025-08-18 20:41:05,183:WARNING: 	 sim_header: meanP
2025-08-18 20:41:09,907:INFO: --------------------
2025-08-18 20:41:09,907:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
2025-08-18 20:41:09,907:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-18 20:41:10,388:WARNING: GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
2025-08-18 20:41:10,963:INFO: ***** Running test *****
2025-08-18 20:41:10,963:INFO:   Num examples = 27763
2025-08-18 20:41:10,963:INFO:   Batch size = 32
2025-08-18 20:41:10,963:INFO:   Num steps = 868
2025-08-18 20:41:10,963:INFO: ***** Running val *****
2025-08-18 20:41:10,963:INFO:   Num examples = 4290
2025-08-18 20:41:11,452:INFO: ***** Running training *****
2025-08-18 20:41:11,452:INFO:   Num examples = 48774
2025-08-18 20:41:11,452:INFO:   Batch size = 96
2025-08-18 20:41:11,452:INFO:   Num steps = 508
2025-08-18 20:42:19,238:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.161527, Time/step: 13.542889
2025-08-18 20:42:56,484:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.070773, Time/step: 7.448864
2025-08-18 20:43:48,340:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.913087, Time/step: 10.371149
2025-08-18 20:44:18,855:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.602206, Time/step: 6.102859
2025-08-18 20:45:14,223:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.620955, Time/step: 11.073470
2025-08-18 20:45:48,524:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.450599, Time/step: 6.860027
2025-08-18 20:46:20,518:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.178195, Time/step: 6.398577
2025-08-18 20:46:54,477:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.102684, Time/step: 6.791702
2025-08-18 20:47:45,803:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.889710, Time/step: 10.265129
2025-08-18 20:48:14,679:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.966754, Time/step: 5.775017
2025-08-18 20:49:05,879:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.892515, Time/step: 10.239778
2025-08-18 20:49:55,600:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.831938, Time/step: 9.943996
2025-08-18 20:50:36,265:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.562101, Time/step: 8.132959
2025-08-18 20:51:12,748:INFO: Epoch: 1/1, Step: 70/508, Lr: , Loss: 1.568685, Time/step: 7.296376
2025-08-18 20:52:08,876:INFO: Epoch: 1/1, Step: 75/508, Lr: , Loss: 1.594561, Time/step: 11.225578
2025-08-18 20:52:43,880:INFO: Epoch: 1/1, Step: 80/508, Lr: , Loss: 1.405734, Time/step: 7.000673
2025-08-18 20:53:16,970:INFO: Epoch: 1/1, Step: 85/508, Lr: , Loss: 1.478908, Time/step: 6.617906
2025-08-18 20:53:48,750:INFO: Epoch: 1/1, Step: 90/508, Lr: , Loss: 1.510104, Time/step: 6.355633
2025-08-18 20:54:45,279:INFO: Epoch: 1/1, Step: 95/508, Lr: , Loss: 1.554358, Time/step: 11.305598
2025-08-18 20:55:17,431:INFO: Epoch: 1/1, Step: 100/508, Lr: , Loss: 1.539223, Time/step: 6.430216
2025-08-18 20:56:00,383:INFO: Epoch: 1/1, Step: 105/508, Lr: , Loss: 1.175135, Time/step: 8.590424
