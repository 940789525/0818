2025-08-20 08:36:29,782:INFO: Effective parameters:
2025-08-20 08:36:29,782:INFO:   <<< batch_size: 96
2025-08-20 08:36:29,782:INFO:   <<< batch_size_val: 32
2025-08-20 08:36:29,782:INFO:   <<< cache_dir: 
2025-08-20 08:36:29,782:INFO:   <<< coef_lr: 0.001
2025-08-20 08:36:29,782:INFO:   <<< cross_model: cross-base
2025-08-20 08:36:29,782:INFO:   <<< cross_num_hidden_layers: 4
2025-08-20 08:36:29,782:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-20 08:36:29,782:INFO:   <<< datatype: msvd
2025-08-20 08:36:29,782:INFO:   <<< do_eval: True
2025-08-20 08:36:29,782:INFO:   <<< do_lower_case: False
2025-08-20 08:36:29,783:INFO:   <<< do_pretrain: False
2025-08-20 08:36:29,783:INFO:   <<< do_train: False
2025-08-20 08:36:29,783:INFO:   <<< epochs: 1
2025-08-20 08:36:29,783:INFO:   <<< eval_frame_order: 0
2025-08-20 08:36:29,783:INFO:   <<< expand_msrvtt_sentences: False
2025-08-20 08:36:29,783:INFO:   <<< feature_framerate: 1
2025-08-20 08:36:29,783:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-20 08:36:29,783:INFO:   <<< fp16: False
2025-08-20 08:36:29,783:INFO:   <<< fp16_opt_level: O1
2025-08-20 08:36:29,783:INFO:   <<< freeze_layer_num: 9
2025-08-20 08:36:29,783:INFO:   <<< gradient_accumulation_steps: 1
2025-08-20 08:36:29,783:INFO:   <<< hard_negative_rate: 0.5
2025-08-20 08:36:29,783:INFO:   <<< init_model: None
2025-08-20 08:36:29,783:INFO:   <<< linear_patch: 2d
2025-08-20 08:36:29,783:INFO:   <<< local_rank: 0
2025-08-20 08:36:29,783:INFO:   <<< loose_type: True
2025-08-20 08:36:29,783:INFO:   <<< lr: 0.0001
2025-08-20 08:36:29,784:INFO:   <<< lr_decay: 0.9
2025-08-20 08:36:29,784:INFO:   <<< margin: 0.1
2025-08-20 08:36:29,784:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-20 08:36:29,784:INFO:   <<< max_frames: 12
2025-08-20 08:36:29,784:INFO:   <<< max_words: 32
2025-08-20 08:36:29,784:INFO:   <<< n_display: 5
2025-08-20 08:36:29,784:INFO:   <<< n_gpu: 1
2025-08-20 08:36:29,784:INFO:   <<< n_pair: 1
2025-08-20 08:36:29,784:INFO:   <<< negative_weighting: 1
2025-08-20 08:36:29,784:INFO:   <<< new_added_modules: ['Adapter']
2025-08-20 08:36:29,784:INFO:   <<< num_thread_reader: 4
2025-08-20 08:36:29,784:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0820_teacher_1
2025-08-20 08:36:29,784:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-20 08:36:29,784:INFO:   <<< rank: 1
2025-08-20 08:36:29,785:INFO:   <<< resume_model: None
2025-08-20 08:36:29,785:INFO:   <<< sampled_use_mil: False
2025-08-20 08:36:29,785:INFO:   <<< seed: 42
2025-08-20 08:36:29,785:INFO:   <<< sim_header: meanP
2025-08-20 08:36:29,785:INFO:   <<< slice_framepos: 0
2025-08-20 08:36:29,785:INFO:   <<< task_type: retrieval
2025-08-20 08:36:29,785:INFO:   <<< text_num_hidden_layers: 12
2025-08-20 08:36:29,785:INFO:   <<< train_csv: data/.train.csv
2025-08-20 08:36:29,785:INFO:   <<< train_frame_order: 0
2025-08-20 08:36:29,785:INFO:   <<< use_mil: False
2025-08-20 08:36:29,785:INFO:   <<< val_csv: data/.val.csv
2025-08-20 08:36:29,785:INFO:   <<< video_dim: 1024
2025-08-20 08:36:29,785:INFO:   <<< visual_num_hidden_layers: 12
2025-08-20 08:36:29,785:INFO:   <<< warmup_proportion: 0.1
2025-08-20 08:36:29,786:INFO:   <<< world_size: 2
2025-08-20 08:36:29,786:INFO: device: cuda:0 n_gpu: 2
2025-08-20 08:36:29,803:INFO: Effective parameters:
2025-08-20 08:36:29,803:INFO:   <<< batch_size: 96
2025-08-20 08:36:29,803:INFO:   <<< batch_size_val: 32
2025-08-20 08:36:29,803:INFO:   <<< cache_dir: 
2025-08-20 08:36:29,803:INFO:   <<< coef_lr: 0.001
2025-08-20 08:36:29,803:INFO:   <<< cross_model: cross-base
2025-08-20 08:36:29,803:INFO:   <<< cross_num_hidden_layers: 4
2025-08-20 08:36:29,803:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-20 08:36:29,803:INFO:   <<< datatype: msvd
2025-08-20 08:36:29,803:INFO:   <<< do_eval: True
2025-08-20 08:36:29,803:INFO:   <<< do_lower_case: False
2025-08-20 08:36:29,804:INFO:   <<< do_pretrain: False
2025-08-20 08:36:29,804:INFO:   <<< do_train: False
2025-08-20 08:36:29,804:INFO:   <<< epochs: 1
2025-08-20 08:36:29,804:INFO:   <<< eval_frame_order: 0
2025-08-20 08:36:29,804:INFO:   <<< expand_msrvtt_sentences: False
2025-08-20 08:36:29,804:INFO:   <<< feature_framerate: 1
2025-08-20 08:36:29,804:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-20 08:36:29,804:INFO:   <<< fp16: False
2025-08-20 08:36:29,804:INFO:   <<< fp16_opt_level: O1
2025-08-20 08:36:29,804:INFO:   <<< freeze_layer_num: 9
2025-08-20 08:36:29,804:INFO:   <<< gradient_accumulation_steps: 1
2025-08-20 08:36:29,804:INFO:   <<< hard_negative_rate: 0.5
2025-08-20 08:36:29,804:INFO:   <<< init_model: None
2025-08-20 08:36:29,804:INFO:   <<< linear_patch: 2d
2025-08-20 08:36:29,804:INFO:   <<< local_rank: 0
2025-08-20 08:36:29,804:INFO:   <<< loose_type: True
2025-08-20 08:36:29,804:INFO:   <<< lr: 0.0001
2025-08-20 08:36:29,805:INFO:   <<< lr_decay: 0.9
2025-08-20 08:36:29,805:INFO:   <<< margin: 0.1
2025-08-20 08:36:29,805:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-20 08:36:29,805:INFO:   <<< max_frames: 12
2025-08-20 08:36:29,805:INFO:   <<< max_words: 32
2025-08-20 08:36:29,805:INFO:   <<< n_display: 5
2025-08-20 08:36:29,805:INFO:   <<< n_gpu: 1
2025-08-20 08:36:29,805:INFO:   <<< n_pair: 1
2025-08-20 08:36:29,805:INFO:   <<< negative_weighting: 1
2025-08-20 08:36:29,805:INFO:   <<< new_added_modules: ['Adapter']
2025-08-20 08:36:29,805:INFO:   <<< num_thread_reader: 4
2025-08-20 08:36:29,805:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0820_teacher_1
2025-08-20 08:36:29,805:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-20 08:36:29,805:INFO:   <<< rank: 0
2025-08-20 08:36:29,805:INFO:   <<< resume_model: None
2025-08-20 08:36:29,805:INFO:   <<< sampled_use_mil: False
2025-08-20 08:36:29,805:INFO:   <<< seed: 42
2025-08-20 08:36:29,805:INFO:   <<< sim_header: meanP
2025-08-20 08:36:29,806:INFO:   <<< slice_framepos: 0
2025-08-20 08:36:29,806:INFO:   <<< task_type: retrieval
2025-08-20 08:36:29,806:INFO:   <<< text_num_hidden_layers: 12
2025-08-20 08:36:29,806:INFO:   <<< train_csv: data/.train.csv
2025-08-20 08:36:29,806:INFO:   <<< train_frame_order: 0
2025-08-20 08:36:29,806:INFO:   <<< use_mil: False
2025-08-20 08:36:29,806:INFO:   <<< val_csv: data/.val.csv
2025-08-20 08:36:29,806:INFO:   <<< video_dim: 1024
2025-08-20 08:36:29,806:INFO:   <<< visual_num_hidden_layers: 12
2025-08-20 08:36:29,806:INFO:   <<< warmup_proportion: 0.1
2025-08-20 08:36:29,806:INFO:   <<< world_size: 2
2025-08-20 08:36:29,806:INFO: device: cuda:0 n_gpu: 2
2025-08-20 08:36:30,642:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-20 08:36:30,643:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-20 08:36:30,643:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-20 08:36:30,643:WARNING: Stage-One:True, Stage-Two:False
2025-08-20 08:36:30,643:WARNING: Test retrieval by loose type.
2025-08-20 08:36:30,643:WARNING: 	 embed_dim: 512
2025-08-20 08:36:30,643:WARNING: 	 image_resolution: 224
2025-08-20 08:36:30,643:WARNING: 	 vision_layers: 12
2025-08-20 08:36:30,643:WARNING: 	 vision_width: 768
2025-08-20 08:36:30,643:WARNING: 	 vision_patch_size: 32
2025-08-20 08:36:30,643:WARNING: 	 context_length: 77
2025-08-20 08:36:30,643:WARNING: 	 vocab_size: 49408
2025-08-20 08:36:30,643:WARNING: 	 transformer_width: 512
2025-08-20 08:36:30,643:WARNING: 	 transformer_heads: 8
2025-08-20 08:36:30,643:WARNING: 	 transformer_layers: 12
2025-08-20 08:36:30,643:WARNING: 		 linear_patch: 2d
2025-08-20 08:36:30,643:WARNING: 	 cut_top_layer: 0
2025-08-20 08:36:30,725:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-20 08:36:30,725:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-20 08:36:30,725:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-20 08:36:30,725:WARNING: Stage-One:True, Stage-Two:False
2025-08-20 08:36:30,725:WARNING: Test retrieval by loose type.
2025-08-20 08:36:30,726:WARNING: 	 embed_dim: 512
2025-08-20 08:36:30,726:WARNING: 	 image_resolution: 224
2025-08-20 08:36:30,726:WARNING: 	 vision_layers: 12
2025-08-20 08:36:30,726:WARNING: 	 vision_width: 768
2025-08-20 08:36:30,726:WARNING: 	 vision_patch_size: 32
2025-08-20 08:36:30,726:WARNING: 	 context_length: 77
2025-08-20 08:36:30,726:WARNING: 	 vocab_size: 49408
2025-08-20 08:36:30,726:WARNING: 	 transformer_width: 512
2025-08-20 08:36:30,726:WARNING: 	 transformer_heads: 8
2025-08-20 08:36:30,726:WARNING: 	 transformer_layers: 12
2025-08-20 08:36:30,726:WARNING: 		 linear_patch: 2d
2025-08-20 08:36:30,726:WARNING: 	 cut_top_layer: 0
2025-08-20 08:36:32,354:WARNING: 	 sim_header: meanP
2025-08-20 08:36:32,456:WARNING: 	 sim_header: meanP
2025-08-20 08:36:36,835:INFO: --------------------
2025-08-20 08:36:36,835:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.conv.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.conv.weight
2025-08-20 08:36:36,835:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-20 08:36:37,552:INFO: --------------------
2025-08-20 08:36:37,552:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.conv.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.conv.weight
2025-08-20 08:36:37,552:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-20 08:36:39,413:INFO: ***** Running test *****
2025-08-20 08:36:39,413:INFO:   Num examples = 27763
2025-08-20 08:36:39,413:INFO:   Batch size = 32
2025-08-20 08:36:39,413:INFO:   Num steps = 868
2025-08-20 08:36:39,413:INFO: ***** Running val *****
2025-08-20 08:36:39,413:INFO:   Num examples = 4290
2025-08-20 08:36:40,135:INFO: ***** Running test *****
2025-08-20 08:36:40,135:INFO:   Num examples = 27763
2025-08-20 08:36:40,135:INFO:   Batch size = 32
2025-08-20 08:36:40,135:INFO:   Num steps = 868
2025-08-20 08:36:40,135:INFO: ***** Running val *****
2025-08-20 08:36:40,136:INFO:   Num examples = 4290
2025-08-20 08:41:15,302:INFO: device: cuda:1 n_gpu: 2
2025-08-20 08:41:15,337:INFO: Effective parameters:
2025-08-20 08:41:15,337:INFO:   <<< batch_size: 96
2025-08-20 08:41:15,337:INFO:   <<< batch_size_val: 32
2025-08-20 08:41:15,338:INFO:   <<< cache_dir: 
2025-08-20 08:41:15,338:INFO:   <<< coef_lr: 0.001
2025-08-20 08:41:15,338:INFO:   <<< cross_model: cross-base
2025-08-20 08:41:15,338:INFO:   <<< cross_num_hidden_layers: 4
2025-08-20 08:41:15,338:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-20 08:41:15,338:INFO:   <<< datatype: msvd
2025-08-20 08:41:15,338:INFO:   <<< do_eval: True
2025-08-20 08:41:15,338:INFO:   <<< do_lower_case: False
2025-08-20 08:41:15,338:INFO:   <<< do_pretrain: False
2025-08-20 08:41:15,338:INFO:   <<< do_train: False
2025-08-20 08:41:15,338:INFO:   <<< epochs: 1
2025-08-20 08:41:15,338:INFO:   <<< eval_frame_order: 0
2025-08-20 08:41:15,338:INFO:   <<< expand_msrvtt_sentences: False
2025-08-20 08:41:15,339:INFO:   <<< feature_framerate: 1
2025-08-20 08:41:15,339:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-20 08:41:15,339:INFO:   <<< fp16: False
2025-08-20 08:41:15,339:INFO:   <<< fp16_opt_level: O1
2025-08-20 08:41:15,339:INFO:   <<< freeze_layer_num: 9
2025-08-20 08:41:15,339:INFO:   <<< gradient_accumulation_steps: 1
2025-08-20 08:41:15,339:INFO:   <<< hard_negative_rate: 0.5
2025-08-20 08:41:15,339:INFO:   <<< init_model: None
2025-08-20 08:41:15,339:INFO:   <<< linear_patch: 2d
2025-08-20 08:41:15,339:INFO:   <<< local_rank: 0
2025-08-20 08:41:15,339:INFO:   <<< loose_type: True
2025-08-20 08:41:15,339:INFO:   <<< lr: 0.0001
2025-08-20 08:41:15,339:INFO:   <<< lr_decay: 0.9
2025-08-20 08:41:15,339:INFO:   <<< margin: 0.1
2025-08-20 08:41:15,340:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-20 08:41:15,340:INFO:   <<< max_frames: 12
2025-08-20 08:41:15,340:INFO:   <<< max_words: 32
2025-08-20 08:41:15,340:INFO:   <<< n_display: 5
2025-08-20 08:41:15,340:INFO:   <<< n_gpu: 1
2025-08-20 08:41:15,340:INFO:   <<< n_pair: 1
2025-08-20 08:41:15,340:INFO:   <<< negative_weighting: 1
2025-08-20 08:41:15,340:INFO:   <<< new_added_modules: ['Adapter']
2025-08-20 08:41:15,340:INFO:   <<< num_thread_reader: 4
2025-08-20 08:41:15,340:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0820_teacher_1
2025-08-20 08:41:15,340:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-20 08:41:15,340:INFO:   <<< rank: 0
2025-08-20 08:41:15,340:INFO:   <<< resume_model: None
2025-08-20 08:41:15,340:INFO:   <<< sampled_use_mil: False
2025-08-20 08:41:15,341:INFO:   <<< seed: 42
2025-08-20 08:41:15,341:INFO:   <<< sim_header: meanP
2025-08-20 08:41:15,341:INFO:   <<< slice_framepos: 0
2025-08-20 08:41:15,341:INFO:   <<< task_type: retrieval
2025-08-20 08:41:15,341:INFO:   <<< text_num_hidden_layers: 12
2025-08-20 08:41:15,341:INFO:   <<< train_csv: data/.train.csv
2025-08-20 08:41:15,341:INFO:   <<< train_frame_order: 0
2025-08-20 08:41:15,341:INFO:   <<< use_mil: False
2025-08-20 08:41:15,341:INFO:   <<< val_csv: data/.val.csv
2025-08-20 08:41:15,341:INFO:   <<< video_dim: 1024
2025-08-20 08:41:15,341:INFO:   <<< visual_num_hidden_layers: 12
2025-08-20 08:41:15,341:INFO:   <<< warmup_proportion: 0.1
2025-08-20 08:41:15,341:INFO:   <<< world_size: 2
2025-08-20 08:41:15,342:INFO: device: cuda:0 n_gpu: 2
2025-08-20 08:41:16,208:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-20 08:41:16,208:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-20 08:41:16,208:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-20 08:41:16,208:WARNING: Stage-One:True, Stage-Two:False
2025-08-20 08:41:16,209:WARNING: Test retrieval by loose type.
2025-08-20 08:41:16,209:WARNING: 	 embed_dim: 512
2025-08-20 08:41:16,209:WARNING: 	 image_resolution: 224
2025-08-20 08:41:16,209:WARNING: 	 vision_layers: 12
2025-08-20 08:41:16,209:WARNING: 	 vision_width: 768
2025-08-20 08:41:16,209:WARNING: 	 vision_patch_size: 32
2025-08-20 08:41:16,209:WARNING: 	 context_length: 77
2025-08-20 08:41:16,209:WARNING: 	 vocab_size: 49408
2025-08-20 08:41:16,209:WARNING: 	 transformer_width: 512
2025-08-20 08:41:16,209:WARNING: 	 transformer_heads: 8
2025-08-20 08:41:16,209:WARNING: 	 transformer_layers: 12
2025-08-20 08:41:16,209:WARNING: 		 linear_patch: 2d
2025-08-20 08:41:16,209:WARNING: 	 cut_top_layer: 0
2025-08-20 08:41:17,927:WARNING: 	 sim_header: meanP
2025-08-20 08:41:22,727:INFO: --------------------
2025-08-20 08:41:22,728:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.conv.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.conv.weight
2025-08-20 08:41:22,728:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-20 08:41:23,514:INFO: ***** Running test *****
2025-08-20 08:41:23,515:INFO:   Num examples = 27763
2025-08-20 08:41:23,515:INFO:   Batch size = 32
2025-08-20 08:41:23,515:INFO:   Num steps = 868
2025-08-20 08:41:23,515:INFO: ***** Running val *****
2025-08-20 08:41:23,515:INFO:   Num examples = 4290
2025-08-20 08:42:20,297:INFO: device: cuda:1 n_gpu: 2
2025-08-20 08:42:20,302:INFO: Effective parameters:
2025-08-20 08:42:20,302:INFO:   <<< batch_size: 96
2025-08-20 08:42:20,302:INFO:   <<< batch_size_val: 32
2025-08-20 08:42:20,303:INFO:   <<< cache_dir: 
2025-08-20 08:42:20,303:INFO:   <<< coef_lr: 0.001
2025-08-20 08:42:20,303:INFO:   <<< cross_model: cross-base
2025-08-20 08:42:20,303:INFO:   <<< cross_num_hidden_layers: 4
2025-08-20 08:42:20,303:INFO:   <<< data_path: /home/wa24301158/dataset/MSVD
2025-08-20 08:42:20,303:INFO:   <<< datatype: msvd
2025-08-20 08:42:20,303:INFO:   <<< do_eval: False
2025-08-20 08:42:20,303:INFO:   <<< do_lower_case: False
2025-08-20 08:42:20,303:INFO:   <<< do_pretrain: False
2025-08-20 08:42:20,303:INFO:   <<< do_train: True
2025-08-20 08:42:20,304:INFO:   <<< epochs: 1
2025-08-20 08:42:20,304:INFO:   <<< eval_frame_order: 0
2025-08-20 08:42:20,304:INFO:   <<< expand_msrvtt_sentences: False
2025-08-20 08:42:20,304:INFO:   <<< feature_framerate: 1
2025-08-20 08:42:20,304:INFO:   <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
2025-08-20 08:42:20,304:INFO:   <<< fp16: False
2025-08-20 08:42:20,304:INFO:   <<< fp16_opt_level: O1
2025-08-20 08:42:20,304:INFO:   <<< freeze_layer_num: 9
2025-08-20 08:42:20,304:INFO:   <<< gradient_accumulation_steps: 1
2025-08-20 08:42:20,304:INFO:   <<< hard_negative_rate: 0.5
2025-08-20 08:42:20,304:INFO:   <<< init_model: None
2025-08-20 08:42:20,305:INFO:   <<< linear_patch: 2d
2025-08-20 08:42:20,305:INFO:   <<< local_rank: 0
2025-08-20 08:42:20,305:INFO:   <<< loose_type: True
2025-08-20 08:42:20,305:INFO:   <<< lr: 0.0001
2025-08-20 08:42:20,305:INFO:   <<< lr_decay: 0.9
2025-08-20 08:42:20,305:INFO:   <<< margin: 0.1
2025-08-20 08:42:20,305:INFO:   <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
2025-08-20 08:42:20,305:INFO:   <<< max_frames: 12
2025-08-20 08:42:20,305:INFO:   <<< max_words: 32
2025-08-20 08:42:20,305:INFO:   <<< n_display: 5
2025-08-20 08:42:20,306:INFO:   <<< n_gpu: 1
2025-08-20 08:42:20,306:INFO:   <<< n_pair: 1
2025-08-20 08:42:20,306:INFO:   <<< negative_weighting: 1
2025-08-20 08:42:20,306:INFO:   <<< new_added_modules: ['Adapter']
2025-08-20 08:42:20,306:INFO:   <<< num_thread_reader: 4
2025-08-20 08:42:20,306:INFO:   <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0820_teacher_1
2025-08-20 08:42:20,306:INFO:   <<< pretrained_clip_name: ViT-B/32
2025-08-20 08:42:20,306:INFO:   <<< rank: 0
2025-08-20 08:42:20,306:INFO:   <<< resume_model: None
2025-08-20 08:42:20,306:INFO:   <<< sampled_use_mil: False
2025-08-20 08:42:20,306:INFO:   <<< seed: 42
2025-08-20 08:42:20,306:INFO:   <<< sim_header: meanP
2025-08-20 08:42:20,306:INFO:   <<< slice_framepos: 0
2025-08-20 08:42:20,307:INFO:   <<< task_type: retrieval
2025-08-20 08:42:20,307:INFO:   <<< text_num_hidden_layers: 12
2025-08-20 08:42:20,307:INFO:   <<< train_csv: data/.train.csv
2025-08-20 08:42:20,307:INFO:   <<< train_frame_order: 0
2025-08-20 08:42:20,307:INFO:   <<< use_mil: False
2025-08-20 08:42:20,307:INFO:   <<< val_csv: data/.val.csv
2025-08-20 08:42:20,307:INFO:   <<< video_dim: 1024
2025-08-20 08:42:20,307:INFO:   <<< visual_num_hidden_layers: 12
2025-08-20 08:42:20,307:INFO:   <<< warmup_proportion: 0.1
2025-08-20 08:42:20,307:INFO:   <<< world_size: 2
2025-08-20 08:42:20,308:INFO: device: cuda:0 n_gpu: 2
2025-08-20 08:42:21,242:INFO: loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
2025-08-20 08:42:21,243:INFO: Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

2025-08-20 08:42:21,243:INFO: Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
2025-08-20 08:42:21,243:WARNING: Stage-One:True, Stage-Two:False
2025-08-20 08:42:21,243:WARNING: Test retrieval by loose type.
2025-08-20 08:42:21,243:WARNING: 	 embed_dim: 512
2025-08-20 08:42:21,243:WARNING: 	 image_resolution: 224
2025-08-20 08:42:21,243:WARNING: 	 vision_layers: 12
2025-08-20 08:42:21,243:WARNING: 	 vision_width: 768
2025-08-20 08:42:21,243:WARNING: 	 vision_patch_size: 32
2025-08-20 08:42:21,243:WARNING: 	 context_length: 77
2025-08-20 08:42:21,243:WARNING: 	 vocab_size: 49408
2025-08-20 08:42:21,243:WARNING: 	 transformer_width: 512
2025-08-20 08:42:21,244:WARNING: 	 transformer_heads: 8
2025-08-20 08:42:21,244:WARNING: 	 transformer_layers: 12
2025-08-20 08:42:21,244:WARNING: 		 linear_patch: 2d
2025-08-20 08:42:21,244:WARNING: 	 cut_top_layer: 0
2025-08-20 08:42:22,989:WARNING: 	 sim_header: meanP
2025-08-20 08:42:27,820:INFO: --------------------
2025-08-20 08:42:27,820:INFO: Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.conv.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.conv.weight
2025-08-20 08:42:27,820:INFO: Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
2025-08-20 08:42:28,685:INFO: ***** Running test *****
2025-08-20 08:42:28,685:INFO:   Num examples = 27763
2025-08-20 08:42:28,685:INFO:   Batch size = 32
2025-08-20 08:42:28,685:INFO:   Num steps = 868
2025-08-20 08:42:28,685:INFO: ***** Running val *****
2025-08-20 08:42:28,685:INFO:   Num examples = 4290
2025-08-20 08:42:29,230:INFO: ***** Running training *****
2025-08-20 08:42:29,230:INFO:   Num examples = 48774
2025-08-20 08:42:29,230:INFO:   Batch size = 96
2025-08-20 08:42:29,230:INFO:   Num steps = 508
2025-08-20 08:43:37,872:INFO: Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.314272, Time/step: 13.720214
2025-08-20 08:44:43,702:INFO: Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.152809, Time/step: 13.165843
2025-08-20 08:45:25,184:INFO: Epoch: 1/1, Step: 15/508, Lr: , Loss: 3.017333, Time/step: 8.296418
2025-08-20 08:46:08,903:INFO: Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.764051, Time/step: 8.743610
2025-08-20 08:47:11,457:INFO: Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.775309, Time/step: 12.510556
2025-08-20 08:47:52,636:INFO: Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.557904, Time/step: 8.235847
2025-08-20 08:48:24,613:INFO: Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.210453, Time/step: 6.395233
2025-08-20 08:48:55,473:INFO: Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.107491, Time/step: 6.171815
2025-08-20 08:49:56,469:INFO: Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.703379, Time/step: 12.199022
2025-08-20 08:50:23,735:INFO: Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.652812, Time/step: 5.453117
2025-08-20 08:50:58,693:INFO: Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.300541, Time/step: 6.991485
2025-08-20 08:51:39,794:INFO: Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.294949, Time/step: 8.220103
2025-08-20 08:52:23,646:INFO: Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.008494, Time/step: 8.770217
