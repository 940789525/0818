W0818 10:14:02.701489 140688683480896 torch/distributed/run.py:779] 
W0818 10:14:02.701489 140688683480896 torch/distributed/run.py:779] *****************************************
W0818 10:14:02.701489 140688683480896 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:14:02.701489 140688683480896 torch/distributed/run.py:779] *****************************************
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 677, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 558, in main
[rank0]:     args = set_seed_logger(args)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 145, in set_seed_logger
[rank0]:     torch.distributed.init_process_group(backend="nccl")
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
[rank0]:     return func(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
[rank0]:     func_return = func(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1302, in init_process_group
[rank0]:     raise ValueError("trying to initialize the default process group twice!")
[rank0]: ValueError: trying to initialize the default process group twice!
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 677, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 558, in main
[rank1]:     args = set_seed_logger(args)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 145, in set_seed_logger
[rank1]:     torch.distributed.init_process_group(backend="nccl")
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/c10d_logger.py", line 79, in wrapper
[rank1]:     return func(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/c10d_logger.py", line 93, in wrapper
[rank1]:     func_return = func(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/distributed_c10d.py", line 1302, in init_process_group
[rank1]:     raise ValueError("trying to initialize the default process group twice!")
[rank1]: ValueError: trying to initialize the default process group twice!
E0818 10:14:13.714375 140688683480896 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 24540) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
[1]:
  time      : 2025-08-18_10:14:13
  host      : localhost.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 24541)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:14:13
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 24540)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 10:16:49.053399 140312260163392 torch/distributed/run.py:779] 
W0818 10:16:49.053399 140312260163392 torch/distributed/run.py:779] *****************************************
W0818 10:16:49.053399 140312260163392 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:16:49.053399 140312260163392 torch/distributed/run.py:779] *****************************************
08/18/2025 10:16:51 - INFO -   Effective parameters:
08/18/2025 10:16:51 - INFO -     <<< batch_size: 96
08/18/2025 10:16:51 - INFO -     <<< batch_size_val: 32
08/18/2025 10:16:51 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:16:51 - INFO -     <<< cache_dir: 
08/18/2025 10:16:51 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:16:51 - INFO -     <<< cross_model: cross-base
08/18/2025 10:16:51 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:16:51 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:16:51 - INFO -     <<< datatype: msvd
08/18/2025 10:16:51 - INFO -     <<< do_eval: False
08/18/2025 10:16:51 - INFO -     <<< do_lower_case: False
08/18/2025 10:16:51 - INFO -     <<< do_pretrain: False
08/18/2025 10:16:51 - INFO -     <<< do_train: True
08/18/2025 10:16:51 - INFO -     <<< epochs: 1
08/18/2025 10:16:51 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:16:51 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:16:51 - INFO -     <<< feature_framerate: 1
08/18/2025 10:16:51 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:16:51 - INFO -     <<< fp16: False
08/18/2025 10:16:51 - INFO -     <<< fp16_opt_level: O1
08/18/2025 10:16:51 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:16:51 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:16:51 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:16:51 - INFO -     <<< init_model: None
08/18/2025 10:16:51 - INFO -     <<< linear_patch: 2d
08/18/2025 10:16:51 - INFO -     <<< local_rank: 0
08/18/2025 10:16:51 - INFO -     <<< loose_type: True
08/18/2025 10:16:51 - INFO -     <<< lr: 0.0001
08/18/2025 10:16:51 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:16:51 - INFO -     <<< margin: 0.1
08/18/2025 10:16:51 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:16:51 - INFO -     <<< max_frames: 12
08/18/2025 10:16:51 - INFO -     <<< max_words: 32
08/18/2025 10:16:51 - INFO -     <<< n_display: 5
08/18/2025 10:16:51 - INFO -     <<< n_gpu: 1
08/18/2025 10:16:51 - INFO -     <<< n_pair: 1
08/18/2025 10:16:51 - INFO -     <<< negative_weighting: 1
08/18/2025 10:16:51 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:16:51 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:16:51 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:16:51 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:16:51 - INFO -     <<< rank: 0
08/18/2025 10:16:51 - INFO -     <<< resume_model: None
08/18/2025 10:16:51 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:16:51 - INFO -     <<< seed: 42
08/18/2025 10:16:51 - INFO -     <<< sim_header: meanP
08/18/2025 10:16:51 - INFO -     <<< slice_framepos: 0
08/18/2025 10:16:51 - INFO -     <<< task_type: retrieval
08/18/2025 10:16:51 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:16:51 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:16:51 - INFO -     <<< train_frame_order: 0
08/18/2025 10:16:51 - INFO -     <<< use_mil: False
08/18/2025 10:16:51 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:16:51 - INFO -     <<< video_dim: 1024
08/18/2025 10:16:51 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:16:51 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:16:51 - INFO -     <<< world_size: 2
08/18/2025 10:16:51 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:16:52 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:16:52 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:16:52 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:16:52 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:16:52 - WARNING -   Test retrieval by loose type.
08/18/2025 10:16:52 - WARNING -   	 embed_dim: 512
08/18/2025 10:16:52 - WARNING -   	 image_resolution: 224
08/18/2025 10:16:52 - WARNING -   	 vision_layers: 12
08/18/2025 10:16:52 - WARNING -   	 vision_width: 768
08/18/2025 10:16:52 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:16:52 - WARNING -   	 context_length: 77
08/18/2025 10:16:52 - WARNING -   	 vocab_size: 49408
08/18/2025 10:16:52 - WARNING -   	 transformer_width: 512
08/18/2025 10:16:52 - WARNING -   	 transformer_heads: 8
08/18/2025 10:16:52 - WARNING -   	 transformer_layers: 12
08/18/2025 10:16:52 - WARNING -   		 linear_patch: 2d
08/18/2025 10:16:52 - WARNING -   	 cut_top_layer: 0
08/18/2025 10:16:54 - WARNING -   	 sim_header: meanP
08/18/2025 10:16:59 - INFO -   --------------------
08/18/2025 10:16:59 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 10:16:59 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 10:17:00 - INFO -   ***** Running test *****
08/18/2025 10:17:00 - INFO -     Num examples = 27763
08/18/2025 10:17:00 - INFO -     Batch size = 32
08/18/2025 10:17:00 - INFO -     Num steps = 868
08/18/2025 10:17:00 - INFO -   ***** Running val *****
08/18/2025 10:17:00 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/18/2025 10:17:00 - INFO -   ***** Running training *****
08/18/2025 10:17:00 - INFO -     Num examples = 48774
08/18/2025 10:17:00 - INFO -     Batch size = 96
08/18/2025 10:17:00 - INFO -     Num steps = 508
08/18/2025 10:19:17 - INFO -   Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.148196, Time/step: 27.375520
08/18/2025 10:20:16 - INFO -   Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.029150, Time/step: 11.719399
08/18/2025 10:21:13 - INFO -   Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.846721, Time/step: 11.405780
08/18/2025 10:22:01 - INFO -   Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.532522, Time/step: 9.573916
08/18/2025 10:22:56 - INFO -   Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.522690, Time/step: 11.101846
08/18/2025 10:23:33 - INFO -   Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.342144, Time/step: 7.415767
08/18/2025 10:24:13 - INFO -   Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.031947, Time/step: 7.853507
08/18/2025 10:24:45 - INFO -   Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.934312, Time/step: 6.557067
08/18/2025 10:25:35 - INFO -   Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.567915, Time/step: 9.982471
08/18/2025 10:26:09 - INFO -   Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.603093, Time/step: 6.635142
08/18/2025 10:26:52 - INFO -   Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.312273, Time/step: 8.772007
08/18/2025 10:27:24 - INFO -   Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.226677, Time/step: 6.389405
08/18/2025 10:28:05 - INFO -   Epoch: 1/1, Step: 65/508, Lr: , Loss: 0.921959, Time/step: 8.031839
08/18/2025 10:28:53 - INFO -   Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.859378, Time/step: 9.682492
08/18/2025 10:29:21 - INFO -   Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.921617, Time/step: 5.715254
08/18/2025 10:29:56 - INFO -   Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.668513, Time/step: 6.811773
08/18/2025 10:30:44 - INFO -   Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.862821, Time/step: 9.727447
08/18/2025 10:31:23 - INFO -   Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.786323, Time/step: 7.704763
08/18/2025 10:31:57 - INFO -   Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.938177, Time/step: 6.848244
08/18/2025 10:32:31 - INFO -   Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.885353, Time/step: 6.753670
08/18/2025 10:33:22 - INFO -   Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.573392, Time/step: 10.319683
08/18/2025 10:34:00 - INFO -   Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.731305, Time/step: 7.586804
08/18/2025 10:34:30 - INFO -   Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.760481, Time/step: 5.973219
08/18/2025 10:35:04 - INFO -   Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.655350, Time/step: 6.798212
08/18/2025 10:35:59 - INFO -   Epoch: 1/1, Step: 125/508, Lr: , Loss: 0.733902, Time/step: 11.021339
08/18/2025 10:36:36 - INFO -   Epoch: 1/1, Step: 130/508, Lr: , Loss: 0.825370, Time/step: 7.257968
W0818 10:37:30.986387 140244406921024 torch/distributed/run.py:779] 
W0818 10:37:30.986387 140244406921024 torch/distributed/run.py:779] *****************************************
W0818 10:37:30.986387 140244406921024 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:37:30.986387 140244406921024 torch/distributed/run.py:779] *****************************************
08/18/2025 10:37:35 - INFO -   Effective parameters:
08/18/2025 10:37:35 - INFO -     <<< amp: True
08/18/2025 10:37:35 - INFO -     <<< batch_size: 96
08/18/2025 10:37:35 - INFO -     <<< batch_size_val: 32
08/18/2025 10:37:35 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:37:35 - INFO -     <<< cache_dir: 
08/18/2025 10:37:35 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:37:35 - INFO -     <<< cross_model: cross-base
08/18/2025 10:37:35 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:37:35 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:37:35 - INFO -     <<< datatype: msvd
08/18/2025 10:37:35 - INFO -     <<< do_eval: False
08/18/2025 10:37:35 - INFO -     <<< do_lower_case: False
08/18/2025 10:37:35 - INFO -     <<< do_pretrain: False
08/18/2025 10:37:35 - INFO -     <<< do_train: True
08/18/2025 10:37:35 - INFO -     <<< epochs: 1
08/18/2025 10:37:35 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:37:35 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:37:35 - INFO -     <<< feature_framerate: 1
08/18/2025 10:37:35 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:37:35 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:37:35 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:37:35 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:37:35 - INFO -     <<< init_model: None
08/18/2025 10:37:35 - INFO -     <<< linear_patch: 2d
08/18/2025 10:37:35 - INFO -     <<< local_rank: 0
08/18/2025 10:37:35 - INFO -     <<< loose_type: True
08/18/2025 10:37:35 - INFO -     <<< lr: 0.0001
08/18/2025 10:37:35 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:37:35 - INFO -     <<< margin: 0.1
08/18/2025 10:37:35 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:37:35 - INFO -     <<< max_frames: 12
08/18/2025 10:37:35 - INFO -     <<< max_words: 32
08/18/2025 10:37:35 - INFO -     <<< n_display: 5
08/18/2025 10:37:35 - INFO -     <<< n_gpu: 1
08/18/2025 10:37:35 - INFO -     <<< n_pair: 1
08/18/2025 10:37:35 - INFO -     <<< negative_weighting: 1
08/18/2025 10:37:35 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:37:35 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:37:35 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:37:35 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:37:35 - INFO -     <<< rank: 0
08/18/2025 10:37:35 - INFO -     <<< resume_model: None
08/18/2025 10:37:35 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:37:35 - INFO -     <<< seed: 42
08/18/2025 10:37:35 - INFO -     <<< sim_header: meanP
08/18/2025 10:37:35 - INFO -     <<< slice_framepos: 0
08/18/2025 10:37:35 - INFO -     <<< task_type: retrieval
08/18/2025 10:37:35 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:37:35 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:37:35 - INFO -     <<< train_frame_order: 0
08/18/2025 10:37:35 - INFO -     <<< use_mil: False
08/18/2025 10:37:35 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:37:35 - INFO -     <<< video_dim: 1024
08/18/2025 10:37:35 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:37:35 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:37:35 - INFO -     <<< world_size: 2
08/18/2025 10:37:35 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:37:41 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:37:41 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:37:41 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:37:41 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:37:41 - WARNING -   Test retrieval by loose type.
08/18/2025 10:37:41 - WARNING -   	 embed_dim: 512
08/18/2025 10:37:41 - WARNING -   	 image_resolution: 224
08/18/2025 10:37:41 - WARNING -   	 vision_layers: 12
08/18/2025 10:37:41 - WARNING -   	 vision_width: 768
08/18/2025 10:37:41 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:37:41 - WARNING -   	 context_length: 77
08/18/2025 10:37:41 - WARNING -   	 vocab_size: 49408
08/18/2025 10:37:41 - WARNING -   	 transformer_width: 512
08/18/2025 10:37:41 - WARNING -   	 transformer_heads: 8
08/18/2025 10:37:41 - WARNING -   	 transformer_layers: 12
08/18/2025 10:37:41 - WARNING -   		 linear_patch: 2d
08/18/2025 10:37:41 - WARNING -   	 cut_top_layer: 0
08/18/2025 10:37:43 - WARNING -   	 sim_header: meanP
08/18/2025 10:37:48 - INFO -   --------------------
08/18/2025 10:37:48 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 10:37:48 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 10:37:49 - INFO -   ***** Running test *****
08/18/2025 10:37:49 - INFO -     Num examples = 27763
08/18/2025 10:37:49 - INFO -     Batch size = 32
08/18/2025 10:37:49 - INFO -     Num steps = 868
08/18/2025 10:37:49 - INFO -   ***** Running val *****
08/18/2025 10:37:49 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/18/2025 10:37:50 - INFO -   ***** Running training *****
08/18/2025 10:37:50 - INFO -     Num examples = 48774
08/18/2025 10:37:50 - INFO -     Batch size = 96
08/18/2025 10:37:50 - INFO -     Num steps = 508
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 683, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 655, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 392, in train_epoch
[rank0]:     scaler.step(optimizer)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 448, in step
[rank0]:     self.unscale_(optimizer)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank0]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank0]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank0]: ValueError: Attempting to unscale FP16 gradients.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 683, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 655, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 392, in train_epoch
[rank1]:     scaler.step(optimizer)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 448, in step
[rank1]:     self.unscale_(optimizer)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank1]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank1]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank1]: ValueError: Attempting to unscale FP16 gradients.
W0818 10:39:08.304725 140244406921024 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 31672 closing signal SIGTERM
E0818 10:39:10.573457 140244406921024 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 31671) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:39:08
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 31671)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 10:44:02.033560 139894171006784 torch/distributed/run.py:779] 
W0818 10:44:02.033560 139894171006784 torch/distributed/run.py:779] *****************************************
W0818 10:44:02.033560 139894171006784 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:44:02.033560 139894171006784 torch/distributed/run.py:779] *****************************************
Traceback (most recent call last):
  File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 23, in <module>
    from transformers import get_linear_schedule_with_warmup
ModuleNotFoundError: No module named 'transformers'
Traceback (most recent call last):
  File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 23, in <module>
    from transformers import get_linear_schedule_with_warmup
ModuleNotFoundError: No module named 'transformers'
W0818 10:44:05.104435 139894171006784 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 49930 closing signal SIGTERM
E0818 10:44:05.168861 139894171006784 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 49929) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:44:05
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 49929)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 10:45:46.493704 139949300643648 torch/distributed/run.py:779] 
W0818 10:45:46.493704 139949300643648 torch/distributed/run.py:779] *****************************************
W0818 10:45:46.493704 139949300643648 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:45:46.493704 139949300643648 torch/distributed/run.py:779] *****************************************
08/18/2025 10:45:50 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:45:50 - INFO -   Effective parameters:
08/18/2025 10:45:50 - INFO -     <<< amp: True
08/18/2025 10:45:50 - INFO -     <<< batch_size: 96
08/18/2025 10:45:50 - INFO -     <<< batch_size_val: 32
08/18/2025 10:45:50 - INFO -     <<< cache_dir: 
08/18/2025 10:45:50 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:45:50 - INFO -     <<< cross_model: cross-base
08/18/2025 10:45:50 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:45:50 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:45:50 - INFO -     <<< datatype: msvd
08/18/2025 10:45:50 - INFO -     <<< do_eval: False
08/18/2025 10:45:50 - INFO -     <<< do_lower_case: False
08/18/2025 10:45:50 - INFO -     <<< do_pretrain: False
08/18/2025 10:45:50 - INFO -     <<< do_train: True
08/18/2025 10:45:50 - INFO -     <<< epochs: 1
08/18/2025 10:45:50 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:45:50 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:45:50 - INFO -     <<< feature_framerate: 1
08/18/2025 10:45:50 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:45:50 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:45:50 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:45:50 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:45:50 - INFO -     <<< init_model: None
08/18/2025 10:45:50 - INFO -     <<< linear_patch: 2d
08/18/2025 10:45:50 - INFO -     <<< local_rank: 0
08/18/2025 10:45:50 - INFO -     <<< loose_type: True
08/18/2025 10:45:50 - INFO -     <<< lr: 0.0001
08/18/2025 10:45:50 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:45:50 - INFO -     <<< margin: 0.1
08/18/2025 10:45:50 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:45:50 - INFO -     <<< max_frames: 12
08/18/2025 10:45:50 - INFO -     <<< max_words: 32
08/18/2025 10:45:50 - INFO -     <<< n_display: 5
08/18/2025 10:45:50 - INFO -     <<< n_gpu: 1
08/18/2025 10:45:50 - INFO -     <<< n_pair: 1
08/18/2025 10:45:50 - INFO -     <<< negative_weighting: 1
08/18/2025 10:45:50 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:45:50 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:45:50 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:45:50 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:45:50 - INFO -     <<< rank: 0
08/18/2025 10:45:50 - INFO -     <<< resume_model: None
08/18/2025 10:45:50 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:45:50 - INFO -     <<< seed: 42
08/18/2025 10:45:50 - INFO -     <<< sim_header: meanP
08/18/2025 10:45:50 - INFO -     <<< slice_framepos: 0
08/18/2025 10:45:50 - INFO -     <<< task_type: retrieval
08/18/2025 10:45:50 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:45:50 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:45:50 - INFO -     <<< train_frame_order: 0
08/18/2025 10:45:50 - INFO -     <<< use_mil: False
08/18/2025 10:45:50 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:45:50 - INFO -     <<< video_dim: 1024
08/18/2025 10:45:50 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:45:50 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:45:50 - INFO -     <<< world_size: 2
08/18/2025 10:45:50 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:45:50 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:45:50 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:45:50 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:45:50 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:45:50 - WARNING -   Test retrieval by loose type.
08/18/2025 10:45:50 - WARNING -   	 embed_dim: 512
08/18/2025 10:45:50 - WARNING -   	 image_resolution: 224
08/18/2025 10:45:50 - WARNING -   	 vision_layers: 12
08/18/2025 10:45:50 - WARNING -   	 vision_width: 768
08/18/2025 10:45:50 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:45:50 - WARNING -   	 context_length: 77
08/18/2025 10:45:50 - WARNING -   	 vocab_size: 49408
08/18/2025 10:45:50 - WARNING -   	 transformer_width: 512
08/18/2025 10:45:50 - WARNING -   	 transformer_heads: 8
08/18/2025 10:45:50 - WARNING -   	 transformer_layers: 12
08/18/2025 10:45:50 - WARNING -   		 linear_patch: 2d
08/18/2025 10:45:50 - WARNING -   	 cut_top_layer: 0
08/18/2025 10:45:52 - WARNING -   	 sim_header: meanP
08/18/2025 10:45:57 - INFO -   --------------------
08/18/2025 10:45:57 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 10:45:57 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 10:45:58 - INFO -   ***** Running test *****
08/18/2025 10:45:58 - INFO -     Num examples = 27763
08/18/2025 10:45:58 - INFO -     Batch size = 32
08/18/2025 10:45:58 - INFO -     Num steps = 868
08/18/2025 10:45:58 - INFO -   ***** Running val *****
08/18/2025 10:45:58 - INFO -     Num examples = 4290
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
Video number: 1200
Total Paire: 48774
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
Video number: 1200
Total Paire: 48774
08/18/2025 10:45:59 - INFO -   ***** Running training *****
08/18/2025 10:45:59 - INFO -     Num examples = 48774
08/18/2025 10:45:59 - INFO -     Batch size = 96
08/18/2025 10:45:59 - INFO -     Num steps = 508
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 670, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 642, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 375, in train_epoch
[rank0]:     scaler.step(optimizer)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 448, in step
[rank0]:     self.unscale_(optimizer)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank0]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank0]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank0]: ValueError: Attempting to unscale FP16 gradients.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 670, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 642, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 375, in train_epoch
[rank1]:     scaler.step(optimizer)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 448, in step
[rank1]:     self.unscale_(optimizer)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank1]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank1]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank1]: ValueError: Attempting to unscale FP16 gradients.
W0818 10:47:09.077052 139949300643648 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 51337 closing signal SIGTERM
E0818 10:47:12.496530 139949300643648 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 51336) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:47:09
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 51336)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 10:51:41.121943 140262921320256 torch/distributed/run.py:779] 
W0818 10:51:41.121943 140262921320256 torch/distributed/run.py:779] *****************************************
W0818 10:51:41.121943 140262921320256 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:51:41.121943 140262921320256 torch/distributed/run.py:779] *****************************************
08/18/2025 10:51:44 - INFO -   Effective parameters:
08/18/2025 10:51:44 - INFO -     <<< amp: True
08/18/2025 10:51:44 - INFO -     <<< batch_size: 96
08/18/2025 10:51:44 - INFO -     <<< batch_size_val: 32
08/18/2025 10:51:44 - INFO -     <<< cache_dir: 
08/18/2025 10:51:44 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:51:44 - INFO -     <<< cross_model: cross-base
08/18/2025 10:51:44 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:51:44 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:51:44 - INFO -     <<< datatype: msvd
08/18/2025 10:51:44 - INFO -     <<< do_eval: False
08/18/2025 10:51:44 - INFO -     <<< do_lower_case: False
08/18/2025 10:51:44 - INFO -     <<< do_pretrain: False
08/18/2025 10:51:44 - INFO -     <<< do_train: True
08/18/2025 10:51:44 - INFO -     <<< epochs: 1
08/18/2025 10:51:44 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:51:44 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:51:44 - INFO -     <<< feature_framerate: 1
08/18/2025 10:51:44 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:51:44 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:51:44 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:51:44 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:51:44 - INFO -     <<< init_model: None
08/18/2025 10:51:44 - INFO -     <<< linear_patch: 2d
08/18/2025 10:51:44 - INFO -     <<< local_rank: 0
08/18/2025 10:51:44 - INFO -     <<< loose_type: True
08/18/2025 10:51:44 - INFO -     <<< lr: 0.0001
08/18/2025 10:51:44 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:51:44 - INFO -     <<< margin: 0.1
08/18/2025 10:51:44 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:51:44 - INFO -     <<< max_frames: 12
08/18/2025 10:51:44 - INFO -     <<< max_words: 32
08/18/2025 10:51:44 - INFO -     <<< n_display: 5
08/18/2025 10:51:44 - INFO -     <<< n_gpu: 1
08/18/2025 10:51:44 - INFO -     <<< n_pair: 1
08/18/2025 10:51:44 - INFO -     <<< negative_weighting: 1
08/18/2025 10:51:44 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:51:44 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:51:44 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:51:44 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:51:44 - INFO -     <<< rank: 0
08/18/2025 10:51:44 - INFO -     <<< resume_model: None
08/18/2025 10:51:44 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:51:44 - INFO -     <<< seed: 42
08/18/2025 10:51:44 - INFO -     <<< sim_header: meanP
08/18/2025 10:51:44 - INFO -     <<< slice_framepos: 0
08/18/2025 10:51:44 - INFO -     <<< task_type: retrieval
08/18/2025 10:51:44 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:51:44 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:51:44 - INFO -     <<< train_frame_order: 0
08/18/2025 10:51:44 - INFO -     <<< use_mil: False
08/18/2025 10:51:44 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:51:44 - INFO -     <<< video_dim: 1024
08/18/2025 10:51:44 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:51:44 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:51:44 - INFO -     <<< world_size: 2
08/18/2025 10:51:44 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:51:44 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:51:45 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:51:45 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:51:45 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:51:45 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:51:45 - WARNING -   Test retrieval by loose type.
08/18/2025 10:51:45 - WARNING -   	 embed_dim: 512
08/18/2025 10:51:45 - WARNING -   	 image_resolution: 224
08/18/2025 10:51:45 - WARNING -   	 vision_layers: 12
08/18/2025 10:51:45 - WARNING -   	 vision_width: 768
08/18/2025 10:51:45 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:51:45 - WARNING -   	 context_length: 77
08/18/2025 10:51:45 - WARNING -   	 vocab_size: 49408
08/18/2025 10:51:45 - WARNING -   	 transformer_width: 512
08/18/2025 10:51:45 - WARNING -   	 transformer_heads: 8
08/18/2025 10:51:45 - WARNING -   	 transformer_layers: 12
08/18/2025 10:51:45 - WARNING -   		 linear_patch: 2d
08/18/2025 10:51:45 - WARNING -   	 cut_top_layer: 0
W0818 10:51:54.112674 140569265997632 torch/distributed/run.py:779] 
W0818 10:51:54.112674 140569265997632 torch/distributed/run.py:779] *****************************************
W0818 10:51:54.112674 140569265997632 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:51:54.112674 140569265997632 torch/distributed/run.py:779] *****************************************
08/18/2025 10:51:57 - INFO -   Effective parameters:
08/18/2025 10:51:57 - INFO -     <<< amp: True
08/18/2025 10:51:57 - INFO -     <<< batch_size: 96
08/18/2025 10:51:57 - INFO -     <<< batch_size_val: 32
08/18/2025 10:51:57 - INFO -     <<< cache_dir: 
08/18/2025 10:51:57 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:51:57 - INFO -     <<< cross_model: cross-base
08/18/2025 10:51:57 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:51:57 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:51:57 - INFO -     <<< datatype: msvd
08/18/2025 10:51:57 - INFO -     <<< do_eval: False
08/18/2025 10:51:57 - INFO -     <<< do_lower_case: False
08/18/2025 10:51:57 - INFO -     <<< do_pretrain: False
08/18/2025 10:51:57 - INFO -     <<< do_train: True
08/18/2025 10:51:57 - INFO -     <<< epochs: 1
08/18/2025 10:51:57 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:51:57 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:51:57 - INFO -     <<< feature_framerate: 1
08/18/2025 10:51:57 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:51:57 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:51:57 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:51:57 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:51:57 - INFO -     <<< init_model: None
08/18/2025 10:51:57 - INFO -     <<< linear_patch: 2d
08/18/2025 10:51:57 - INFO -     <<< local_rank: 0
08/18/2025 10:51:57 - INFO -     <<< loose_type: True
08/18/2025 10:51:57 - INFO -     <<< lr: 0.0001
08/18/2025 10:51:57 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:51:57 - INFO -     <<< margin: 0.1
08/18/2025 10:51:57 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:51:57 - INFO -     <<< max_frames: 12
08/18/2025 10:51:57 - INFO -     <<< max_words: 32
08/18/2025 10:51:57 - INFO -     <<< n_display: 5
08/18/2025 10:51:57 - INFO -     <<< n_gpu: 1
08/18/2025 10:51:57 - INFO -     <<< n_pair: 1
08/18/2025 10:51:57 - INFO -     <<< negative_weighting: 1
08/18/2025 10:51:57 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:51:57 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:51:57 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:51:57 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:51:57 - INFO -     <<< rank: 0
08/18/2025 10:51:57 - INFO -     <<< resume_model: None
08/18/2025 10:51:57 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:51:57 - INFO -     <<< seed: 42
08/18/2025 10:51:57 - INFO -     <<< sim_header: meanP
08/18/2025 10:51:57 - INFO -     <<< slice_framepos: 0
08/18/2025 10:51:57 - INFO -     <<< task_type: retrieval
08/18/2025 10:51:57 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:51:57 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:51:57 - INFO -     <<< train_frame_order: 0
08/18/2025 10:51:57 - INFO -     <<< use_mil: False
08/18/2025 10:51:57 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:51:57 - INFO -     <<< video_dim: 1024
08/18/2025 10:51:57 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:51:57 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:51:57 - INFO -     <<< world_size: 2
08/18/2025 10:51:57 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:51:57 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:51:58 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:51:58 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:51:58 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:51:58 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:51:58 - WARNING -   Test retrieval by loose type.
08/18/2025 10:51:58 - WARNING -   	 embed_dim: 512
08/18/2025 10:51:58 - WARNING -   	 image_resolution: 224
08/18/2025 10:51:58 - WARNING -   	 vision_layers: 12
08/18/2025 10:51:58 - WARNING -   	 vision_width: 768
08/18/2025 10:51:58 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:51:58 - WARNING -   	 context_length: 77
08/18/2025 10:51:58 - WARNING -   	 vocab_size: 49408
08/18/2025 10:51:58 - WARNING -   	 transformer_width: 512
08/18/2025 10:51:58 - WARNING -   	 transformer_heads: 8
08/18/2025 10:51:58 - WARNING -   	 transformer_layers: 12
08/18/2025 10:51:58 - WARNING -   		 linear_patch: 2d
08/18/2025 10:51:58 - WARNING -   	 cut_top_layer: 0
08/18/2025 10:52:00 - WARNING -   	 sim_header: meanP
08/18/2025 10:52:04 - INFO -   --------------------
08/18/2025 10:52:04 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 10:52:04 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 10:52:05 - INFO -   ***** Running test *****
08/18/2025 10:52:05 - INFO -     Num examples = 27763
08/18/2025 10:52:05 - INFO -     Batch size = 32
08/18/2025 10:52:05 - INFO -     Num steps = 868
08/18/2025 10:52:05 - INFO -   ***** Running val *****
08/18/2025 10:52:05 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/18/2025 10:52:06 - INFO -   ***** Running training *****
08/18/2025 10:52:06 - INFO -     Num examples = 48774
08/18/2025 10:52:06 - INFO -     Batch size = 96
08/18/2025 10:52:06 - INFO -     Num steps = 508
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 672, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 644, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 370, in train_epoch
[rank0]:     scaler.unscale_(optimizer)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank0]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank0]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank0]: ValueError: Attempting to unscale FP16 gradients.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 672, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 644, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 370, in train_epoch
[rank1]:     scaler.unscale_(optimizer)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 338, in unscale_
[rank1]:     optimizer_state["found_inf_per_device"] = self._unscale_grads_(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/amp/grad_scaler.py", line 260, in _unscale_grads_
[rank1]:     raise ValueError("Attempting to unscale FP16 gradients.")
[rank1]: ValueError: Attempting to unscale FP16 gradients.
W0818 10:53:17.482287 140569265997632 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 68888 closing signal SIGTERM
E0818 10:53:20.450946 140569265997632 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 68887) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:53:17
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 68887)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 10:56:11.608751 139777131824960 torch/distributed/run.py:779] 
W0818 10:56:11.608751 139777131824960 torch/distributed/run.py:779] *****************************************
W0818 10:56:11.608751 139777131824960 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 10:56:11.608751 139777131824960 torch/distributed/run.py:779] *****************************************
08/18/2025 10:56:16 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 10:56:16 - INFO -   Effective parameters:
08/18/2025 10:56:16 - INFO -     <<< amp: True
08/18/2025 10:56:16 - INFO -     <<< batch_size: 96
08/18/2025 10:56:16 - INFO -     <<< batch_size_val: 32
08/18/2025 10:56:16 - INFO -     <<< cache_dir: 
08/18/2025 10:56:16 - INFO -     <<< coef_lr: 0.001
08/18/2025 10:56:16 - INFO -     <<< cross_model: cross-base
08/18/2025 10:56:16 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 10:56:16 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 10:56:16 - INFO -     <<< datatype: msvd
08/18/2025 10:56:16 - INFO -     <<< do_eval: False
08/18/2025 10:56:16 - INFO -     <<< do_lower_case: False
08/18/2025 10:56:16 - INFO -     <<< do_pretrain: False
08/18/2025 10:56:16 - INFO -     <<< do_train: True
08/18/2025 10:56:16 - INFO -     <<< epochs: 1
08/18/2025 10:56:16 - INFO -     <<< eval_frame_order: 0
08/18/2025 10:56:16 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 10:56:16 - INFO -     <<< feature_framerate: 1
08/18/2025 10:56:16 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 10:56:16 - INFO -     <<< freeze_layer_num: 9
08/18/2025 10:56:16 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 10:56:16 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 10:56:16 - INFO -     <<< init_model: None
08/18/2025 10:56:16 - INFO -     <<< linear_patch: 2d
08/18/2025 10:56:16 - INFO -     <<< local_rank: 0
08/18/2025 10:56:16 - INFO -     <<< loose_type: True
08/18/2025 10:56:16 - INFO -     <<< lr: 0.0001
08/18/2025 10:56:16 - INFO -     <<< lr_decay: 0.9
08/18/2025 10:56:16 - INFO -     <<< margin: 0.1
08/18/2025 10:56:16 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 10:56:16 - INFO -     <<< max_frames: 12
08/18/2025 10:56:16 - INFO -     <<< max_words: 32
08/18/2025 10:56:16 - INFO -     <<< n_display: 5
08/18/2025 10:56:16 - INFO -     <<< n_gpu: 1
08/18/2025 10:56:16 - INFO -     <<< n_pair: 1
08/18/2025 10:56:16 - INFO -     <<< negative_weighting: 1
08/18/2025 10:56:16 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 10:56:16 - INFO -     <<< num_thread_reader: 4
08/18/2025 10:56:16 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 10:56:16 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 10:56:16 - INFO -     <<< rank: 0
08/18/2025 10:56:16 - INFO -     <<< resume_model: None
08/18/2025 10:56:16 - INFO -     <<< sampled_use_mil: False
08/18/2025 10:56:16 - INFO -     <<< seed: 42
08/18/2025 10:56:16 - INFO -     <<< sim_header: meanP
08/18/2025 10:56:16 - INFO -     <<< slice_framepos: 0
08/18/2025 10:56:16 - INFO -     <<< task_type: retrieval
08/18/2025 10:56:16 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 10:56:16 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 10:56:16 - INFO -     <<< train_frame_order: 0
08/18/2025 10:56:16 - INFO -     <<< use_mil: False
08/18/2025 10:56:16 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 10:56:16 - INFO -     <<< video_dim: 1024
08/18/2025 10:56:16 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 10:56:16 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 10:56:16 - INFO -     <<< world_size: 2
08/18/2025 10:56:16 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 10:56:17 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 10:56:17 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 10:56:17 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 10:56:17 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 10:56:17 - WARNING -   Test retrieval by loose type.
08/18/2025 10:56:17 - WARNING -   	 embed_dim: 512
08/18/2025 10:56:17 - WARNING -   	 image_resolution: 224
08/18/2025 10:56:17 - WARNING -   	 vision_layers: 12
08/18/2025 10:56:17 - WARNING -   	 vision_width: 768
08/18/2025 10:56:17 - WARNING -   	 vision_patch_size: 32
08/18/2025 10:56:17 - WARNING -   	 context_length: 77
08/18/2025 10:56:17 - WARNING -   	 vocab_size: 49408
08/18/2025 10:56:17 - WARNING -   	 transformer_width: 512
08/18/2025 10:56:17 - WARNING -   	 transformer_heads: 8
08/18/2025 10:56:17 - WARNING -   	 transformer_layers: 12
08/18/2025 10:56:17 - WARNING -   		 linear_patch: 2d
08/18/2025 10:56:17 - WARNING -   	 cut_top_layer: 0
08/18/2025 10:56:18 - WARNING -   	 sim_header: meanP
08/18/2025 10:56:23 - INFO -   --------------------
08/18/2025 10:56:23 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 10:56:23 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 10:56:24 - INFO -   ***** Running test *****
08/18/2025 10:56:24 - INFO -     Num examples = 27763
08/18/2025 10:56:24 - INFO -     Batch size = 32
08/18/2025 10:56:24 - INFO -     Num steps = 868
08/18/2025 10:56:24 - INFO -   ***** Running val *****
08/18/2025 10:56:24 - INFO -     Num examples = 4290
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
Video number: 1200
Total Paire: 48774
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
Video number: 1200
Total Paire: 48774
08/18/2025 10:56:24 - INFO -   ***** Running training *****
08/18/2025 10:56:24 - INFO -     Num examples = 48774
08/18/2025 10:56:24 - INFO -     Batch size = 96
08/18/2025 10:56:24 - INFO -     Num steps = 508
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 673, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 645, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 395, in train_epoch
[rank0]:     len(train_dataloader), "-".join([str('%.9f'%itm) for itm in sorted(list(set(optimizer.get_lr())))]),
[rank0]: AttributeError: 'AdamW' object has no attribute 'get_lr'
W0818 10:57:59.021406 139777131824960 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 85192 closing signal SIGTERM
E0818 10:58:06.196766 139777131824960 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 0 (pid: 85191) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-18_10:57:59
  host      : localhost.localdomain
  rank      : 0 (local_rank: 0)
  exitcode  : 1 (pid: 85191)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0818 11:01:19.160066 140586049800000 torch/distributed/run.py:779] 
W0818 11:01:19.160066 140586049800000 torch/distributed/run.py:779] *****************************************
W0818 11:01:19.160066 140586049800000 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 11:01:19.160066 140586049800000 torch/distributed/run.py:779] *****************************************
08/18/2025 11:01:22 - INFO -   Effective parameters:
08/18/2025 11:01:22 - INFO -     <<< amp: True
08/18/2025 11:01:22 - INFO -     <<< batch_size: 96
08/18/2025 11:01:22 - INFO -     <<< batch_size_val: 32
08/18/2025 11:01:22 - INFO -     <<< cache_dir: 
08/18/2025 11:01:22 - INFO -     <<< coef_lr: 0.001
08/18/2025 11:01:22 - INFO -     <<< cross_model: cross-base
08/18/2025 11:01:22 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 11:01:22 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 11:01:22 - INFO -     <<< datatype: msvd
08/18/2025 11:01:22 - INFO -     <<< do_eval: False
08/18/2025 11:01:22 - INFO -     <<< do_lower_case: False
08/18/2025 11:01:22 - INFO -     <<< do_pretrain: False
08/18/2025 11:01:22 - INFO -     <<< do_train: True
08/18/2025 11:01:22 - INFO -     <<< epochs: 1
08/18/2025 11:01:22 - INFO -     <<< eval_frame_order: 0
08/18/2025 11:01:22 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 11:01:22 - INFO -     <<< feature_framerate: 1
08/18/2025 11:01:22 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 11:01:22 - INFO -     <<< freeze_layer_num: 9
08/18/2025 11:01:22 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 11:01:22 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 11:01:22 - INFO -     <<< init_model: None
08/18/2025 11:01:22 - INFO -     <<< linear_patch: 2d
08/18/2025 11:01:22 - INFO -     <<< local_rank: 0
08/18/2025 11:01:22 - INFO -     <<< loose_type: True
08/18/2025 11:01:22 - INFO -     <<< lr: 0.0001
08/18/2025 11:01:22 - INFO -     <<< lr_decay: 0.9
08/18/2025 11:01:22 - INFO -     <<< margin: 0.1
08/18/2025 11:01:22 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 11:01:22 - INFO -     <<< max_frames: 12
08/18/2025 11:01:22 - INFO -     <<< max_words: 32
08/18/2025 11:01:22 - INFO -     <<< n_display: 5
08/18/2025 11:01:22 - INFO -     <<< n_gpu: 1
08/18/2025 11:01:22 - INFO -     <<< n_pair: 1
08/18/2025 11:01:22 - INFO -     <<< negative_weighting: 1
08/18/2025 11:01:22 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 11:01:22 - INFO -     <<< num_thread_reader: 4
08/18/2025 11:01:22 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 11:01:22 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 11:01:22 - INFO -     <<< rank: 0
08/18/2025 11:01:22 - INFO -     <<< resume_model: None
08/18/2025 11:01:22 - INFO -     <<< sampled_use_mil: False
08/18/2025 11:01:22 - INFO -     <<< seed: 42
08/18/2025 11:01:22 - INFO -     <<< sim_header: meanP
08/18/2025 11:01:22 - INFO -     <<< slice_framepos: 0
08/18/2025 11:01:22 - INFO -     <<< task_type: retrieval
08/18/2025 11:01:22 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 11:01:22 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 11:01:22 - INFO -     <<< train_frame_order: 0
08/18/2025 11:01:22 - INFO -     <<< use_mil: False
08/18/2025 11:01:22 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 11:01:22 - INFO -     <<< video_dim: 1024
08/18/2025 11:01:22 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 11:01:22 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 11:01:22 - INFO -     <<< world_size: 2
08/18/2025 11:01:22 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 11:01:22 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 11:01:23 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 11:01:23 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 11:01:23 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 11:01:23 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 11:01:23 - WARNING -   Test retrieval by loose type.
08/18/2025 11:01:23 - WARNING -   	 embed_dim: 512
08/18/2025 11:01:23 - WARNING -   	 image_resolution: 224
08/18/2025 11:01:23 - WARNING -   	 vision_layers: 12
08/18/2025 11:01:23 - WARNING -   	 vision_width: 768
08/18/2025 11:01:23 - WARNING -   	 vision_patch_size: 32
08/18/2025 11:01:23 - WARNING -   	 context_length: 77
08/18/2025 11:01:23 - WARNING -   	 vocab_size: 49408
08/18/2025 11:01:23 - WARNING -   	 transformer_width: 512
08/18/2025 11:01:23 - WARNING -   	 transformer_heads: 8
08/18/2025 11:01:23 - WARNING -   	 transformer_layers: 12
08/18/2025 11:01:23 - WARNING -   		 linear_patch: 2d
08/18/2025 11:01:23 - WARNING -   	 cut_top_layer: 0
08/18/2025 11:01:24 - WARNING -   	 sim_header: meanP
08/18/2025 11:01:29 - INFO -   --------------------
08/18/2025 11:01:29 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 11:01:29 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 11:01:30 - INFO -   ***** Running test *****
08/18/2025 11:01:30 - INFO -     Num examples = 27763
08/18/2025 11:01:30 - INFO -     Batch size = 32
08/18/2025 11:01:30 - INFO -     Num steps = 868
08/18/2025 11:01:30 - INFO -   ***** Running val *****
08/18/2025 11:01:30 - INFO -     Num examples = 4290
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
Video number: 1200
Total Paire: 48774
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
Video number: 1200
Total Paire: 48774
08/18/2025 11:01:31 - INFO -   ***** Running training *****
08/18/2025 11:01:31 - INFO -     Num examples = 48774
08/18/2025 11:01:31 - INFO -     Batch size = 96
08/18/2025 11:01:31 - INFO -     Num steps = 508
08/18/2025 11:02:37 - INFO -   Epoch: 1/1, Step: 5/508, Lr: 0.000000010-0.000000010-0.000010000-0.000010000-0.000010000, Loss: 3.152710, Time/step: 13.246220
08/18/2025 11:03:12 - INFO -   Epoch: 1/1, Step: 10/508, Lr: 0.000000020-0.000000020-0.000020000-0.000020000-0.000020000, Loss: 3.065287, Time/step: 7.039721
08/18/2025 11:04:08 - INFO -   Epoch: 1/1, Step: 15/508, Lr: 0.000000030-0.000000030-0.000030000-0.000030000-0.000030000, Loss: 2.914307, Time/step: 11.248927
08/18/2025 11:04:44 - INFO -   Epoch: 1/1, Step: 20/508, Lr: 0.000000040-0.000000040-0.000040000-0.000040000-0.000040000, Loss: 2.669373, Time/step: 7.088899
08/18/2025 11:05:49 - INFO -   Epoch: 1/1, Step: 25/508, Lr: 0.000000050-0.000000050-0.000050000-0.000050000-0.000050000, Loss: 2.622538, Time/step: 13.152436
08/18/2025 11:06:25 - INFO -   Epoch: 1/1, Step: 30/508, Lr: 0.000000060-0.000000060-0.000060000-0.000060000-0.000060000, Loss: 2.471415, Time/step: 7.144079
08/18/2025 11:06:59 - INFO -   Epoch: 1/1, Step: 35/508, Lr: 0.000000070-0.000000070-0.000070000-0.000070000-0.000070000, Loss: 2.205312, Time/step: 6.713182
08/18/2025 11:07:29 - INFO -   Epoch: 1/1, Step: 40/508, Lr: 0.000000080-0.000000080-0.000080000-0.000080000-0.000080000, Loss: 2.179423, Time/step: 5.979340
08/18/2025 11:08:26 - INFO -   Epoch: 1/1, Step: 45/508, Lr: 0.000000090-0.000000090-0.000090000-0.000090000-0.000090000, Loss: 1.936025, Time/step: 11.516303
08/18/2025 11:08:58 - INFO -   Epoch: 1/1, Step: 50/508, Lr: 0.000000100-0.000000100-0.000100000-0.000100000-0.000100000, Loss: 2.002813, Time/step: 6.406088
08/18/2025 11:09:32 - INFO -   Epoch: 1/1, Step: 55/508, Lr: 0.000000099-0.000000099-0.000098908-0.000098908-0.000098908, Loss: 1.926921, Time/step: 6.823973
08/18/2025 11:10:07 - INFO -   Epoch: 1/1, Step: 60/508, Lr: 0.000000098-0.000000098-0.000097817-0.000097817-0.000097817, Loss: 1.835632, Time/step: 6.971324
08/18/2025 11:10:50 - INFO -   Epoch: 1/1, Step: 65/508, Lr: 0.000000097-0.000000097-0.000096725-0.000096725-0.000096725, Loss: 1.578547, Time/step: 8.648864
08/18/2025 11:11:34 - INFO -   Epoch: 1/1, Step: 70/508, Lr: 0.000000096-0.000000096-0.000095633-0.000095633-0.000095633, Loss: 1.582344, Time/step: 8.716370
08/18/2025 11:12:09 - INFO -   Epoch: 1/1, Step: 75/508, Lr: 0.000000095-0.000000095-0.000094541-0.000094541-0.000094541, Loss: 1.621259, Time/step: 7.026486
08/18/2025 11:12:44 - INFO -   Epoch: 1/1, Step: 80/508, Lr: 0.000000093-0.000000093-0.000093450-0.000093450-0.000093450, Loss: 1.421249, Time/step: 7.049694
08/18/2025 11:13:28 - INFO -   Epoch: 1/1, Step: 85/508, Lr: 0.000000092-0.000000092-0.000092358-0.000092358-0.000092358, Loss: 1.490898, Time/step: 8.790043
08/18/2025 11:14:06 - INFO -   Epoch: 1/1, Step: 90/508, Lr: 0.000000091-0.000000091-0.000091266-0.000091266-0.000091266, Loss: 1.530978, Time/step: 7.435513
08/18/2025 11:14:46 - INFO -   Epoch: 1/1, Step: 95/508, Lr: 0.000000090-0.000000090-0.000090175-0.000090175-0.000090175, Loss: 1.569486, Time/step: 8.041524
08/18/2025 11:15:17 - INFO -   Epoch: 1/1, Step: 100/508, Lr: 0.000000089-0.000000089-0.000089083-0.000089083-0.000089083, Loss: 1.558609, Time/step: 6.153230
08/18/2025 11:16:08 - INFO -   Epoch: 1/1, Step: 105/508, Lr: 0.000000088-0.000000088-0.000087991-0.000087991-0.000087991, Loss: 1.189178, Time/step: 10.376804
08/18/2025 11:16:40 - INFO -   Epoch: 1/1, Step: 110/508, Lr: 0.000000087-0.000000087-0.000086900-0.000086900-0.000086900, Loss: 1.314407, Time/step: 6.260927
08/18/2025 11:17:26 - INFO -   Epoch: 1/1, Step: 115/508, Lr: 0.000000086-0.000000086-0.000085808-0.000085808-0.000085808, Loss: 1.311300, Time/step: 9.159909
08/18/2025 11:17:57 - INFO -   Epoch: 1/1, Step: 120/508, Lr: 0.000000085-0.000000085-0.000084716-0.000084716-0.000084716, Loss: 1.210230, Time/step: 6.237241
08/18/2025 11:18:46 - INFO -   Epoch: 1/1, Step: 125/508, Lr: 0.000000084-0.000000084-0.000083624-0.000083624-0.000083624, Loss: 1.373611, Time/step: 9.951311
08/18/2025 11:19:18 - INFO -   Epoch: 1/1, Step: 130/508, Lr: 0.000000083-0.000000083-0.000082533-0.000082533-0.000082533, Loss: 1.428422, Time/step: 6.335219
08/18/2025 11:20:03 - INFO -   Epoch: 1/1, Step: 135/508, Lr: 0.000000081-0.000000081-0.000081441-0.000081441-0.000081441, Loss: 0.939012, Time/step: 8.892228
08/18/2025 11:20:36 - INFO -   Epoch: 1/1, Step: 140/508, Lr: 0.000000080-0.000000080-0.000080349-0.000080349-0.000080349, Loss: 1.172887, Time/step: 6.760678
08/18/2025 11:21:24 - INFO -   Epoch: 1/1, Step: 145/508, Lr: 0.000000079-0.000000079-0.000079258-0.000079258-0.000079258, Loss: 1.275953, Time/step: 9.592662
08/18/2025 11:21:59 - INFO -   Epoch: 1/1, Step: 150/508, Lr: 0.000000078-0.000000078-0.000078166-0.000078166-0.000078166, Loss: 1.138589, Time/step: 6.930022
08/18/2025 11:22:36 - INFO -   Epoch: 1/1, Step: 155/508, Lr: 0.000000077-0.000000077-0.000077074-0.000077074-0.000077074, Loss: 1.130051, Time/step: 7.301350
08/18/2025 11:23:06 - INFO -   Epoch: 1/1, Step: 160/508, Lr: 0.000000076-0.000000076-0.000075983-0.000075983-0.000075983, Loss: 1.174344, Time/step: 6.031504
08/18/2025 11:24:00 - INFO -   Epoch: 1/1, Step: 165/508, Lr: 0.000000075-0.000000075-0.000074891-0.000074891-0.000074891, Loss: 1.155670, Time/step: 10.892543
08/18/2025 11:24:42 - INFO -   Epoch: 1/1, Step: 170/508, Lr: 0.000000074-0.000000074-0.000073799-0.000073799-0.000073799, Loss: 1.102910, Time/step: 8.317753
08/18/2025 11:25:19 - INFO -   Epoch: 1/1, Step: 175/508, Lr: 0.000000073-0.000000073-0.000072707-0.000072707-0.000072707, Loss: 1.106280, Time/step: 7.372710
08/18/2025 11:25:57 - INFO -   Epoch: 1/1, Step: 180/508, Lr: 0.000000072-0.000000072-0.000071616-0.000071616-0.000071616, Loss: 1.167361, Time/step: 7.721727
08/18/2025 11:26:57 - INFO -   Epoch: 1/1, Step: 185/508, Lr: 0.000000071-0.000000071-0.000070524-0.000070524-0.000070524, Loss: 1.175439, Time/step: 11.960748
08/18/2025 11:27:31 - INFO -   Epoch: 1/1, Step: 190/508, Lr: 0.000000069-0.000000069-0.000069432-0.000069432-0.000069432, Loss: 1.016805, Time/step: 6.765365
08/18/2025 11:28:14 - INFO -   Epoch: 1/1, Step: 195/508, Lr: 0.000000068-0.000000068-0.000068341-0.000068341-0.000068341, Loss: 1.045537, Time/step: 8.681361
08/18/2025 11:28:50 - INFO -   Epoch: 1/1, Step: 200/508, Lr: 0.000000067-0.000000067-0.000067249-0.000067249-0.000067249, Loss: 0.948939, Time/step: 7.099414
08/18/2025 11:29:53 - INFO -   Epoch: 1/1, Step: 205/508, Lr: 0.000000066-0.000000066-0.000066157-0.000066157-0.000066157, Loss: 1.007664, Time/step: 12.571749
08/18/2025 11:30:22 - INFO -   Epoch: 1/1, Step: 210/508, Lr: 0.000000065-0.000000065-0.000065066-0.000065066-0.000065066, Loss: 1.239702, Time/step: 5.961541
08/18/2025 11:30:57 - INFO -   Epoch: 1/1, Step: 215/508, Lr: 0.000000064-0.000000064-0.000063974-0.000063974-0.000063974, Loss: 0.832368, Time/step: 6.961922
08/18/2025 11:31:31 - INFO -   Epoch: 1/1, Step: 220/508, Lr: 0.000000063-0.000000063-0.000062882-0.000062882-0.000062882, Loss: 0.872514, Time/step: 6.778145
08/18/2025 11:32:29 - INFO -   Epoch: 1/1, Step: 225/508, Lr: 0.000000062-0.000000062-0.000061790-0.000061790-0.000061790, Loss: 0.887121, Time/step: 11.530009
08/18/2025 11:33:01 - INFO -   Epoch: 1/1, Step: 230/508, Lr: 0.000000061-0.000000061-0.000060699-0.000060699-0.000060699, Loss: 1.256607, Time/step: 6.445615
08/18/2025 11:33:31 - INFO -   Epoch: 1/1, Step: 235/508, Lr: 0.000000060-0.000000060-0.000059607-0.000059607-0.000059607, Loss: 0.886856, Time/step: 6.006429
08/18/2025 11:34:05 - INFO -   Epoch: 1/1, Step: 240/508, Lr: 0.000000059-0.000000059-0.000058515-0.000058515-0.000058515, Loss: 0.919863, Time/step: 6.806155
08/18/2025 11:34:54 - INFO -   Epoch: 1/1, Step: 245/508, Lr: 0.000000057-0.000000057-0.000057424-0.000057424-0.000057424, Loss: 1.152664, Time/step: 9.713710
08/18/2025 11:35:30 - INFO -   Epoch: 1/1, Step: 250/508, Lr: 0.000000056-0.000000056-0.000056332-0.000056332-0.000056332, Loss: 0.960964, Time/step: 7.245160
08/18/2025 11:36:05 - INFO -   Epoch: 1/1, Step: 255/508, Lr: 0.000000055-0.000000055-0.000055240-0.000055240-0.000055240, Loss: 0.981953, Time/step: 7.048552
08/18/2025 11:36:48 - INFO -   Epoch: 1/1, Step: 260/508, Lr: 0.000000054-0.000000054-0.000054148-0.000054148-0.000054148, Loss: 0.880779, Time/step: 8.641165
08/18/2025 11:37:29 - INFO -   Epoch: 1/1, Step: 265/508, Lr: 0.000000053-0.000000053-0.000053057-0.000053057-0.000053057, Loss: 0.872111, Time/step: 8.219343
08/18/2025 11:38:11 - INFO -   Epoch: 1/1, Step: 270/508, Lr: 0.000000052-0.000000052-0.000051965-0.000051965-0.000051965, Loss: 0.978912, Time/step: 8.396630
08/18/2025 11:38:45 - INFO -   Epoch: 1/1, Step: 275/508, Lr: 0.000000051-0.000000051-0.000050873-0.000050873-0.000050873, Loss: 1.055183, Time/step: 6.734541
08/18/2025 11:39:37 - INFO -   Epoch: 1/1, Step: 280/508, Lr: 0.000000050-0.000000050-0.000049782-0.000049782-0.000049782, Loss: 0.776578, Time/step: 10.294326
08/18/2025 11:40:11 - INFO -   Epoch: 1/1, Step: 285/508, Lr: 0.000000049-0.000000049-0.000048690-0.000048690-0.000048690, Loss: 0.957383, Time/step: 6.938995
08/18/2025 11:40:56 - INFO -   Epoch: 1/1, Step: 290/508, Lr: 0.000000048-0.000000048-0.000047598-0.000047598-0.000047598, Loss: 0.882355, Time/step: 8.983563
08/18/2025 11:41:27 - INFO -   Epoch: 1/1, Step: 295/508, Lr: 0.000000047-0.000000047-0.000046507-0.000046507-0.000046507, Loss: 0.859070, Time/step: 6.096456
08/18/2025 11:42:17 - INFO -   Epoch: 1/1, Step: 300/508, Lr: 0.000000045-0.000000045-0.000045415-0.000045415-0.000045415, Loss: 0.726470, Time/step: 10.057160
08/18/2025 11:42:54 - INFO -   Epoch: 1/1, Step: 305/508, Lr: 0.000000044-0.000000044-0.000044323-0.000044323-0.000044323, Loss: 0.746627, Time/step: 7.438572
08/18/2025 11:43:35 - INFO -   Epoch: 1/1, Step: 310/508, Lr: 0.000000043-0.000000043-0.000043231-0.000043231-0.000043231, Loss: 0.694647, Time/step: 8.245069
08/18/2025 11:44:06 - INFO -   Epoch: 1/1, Step: 315/508, Lr: 0.000000042-0.000000042-0.000042140-0.000042140-0.000042140, Loss: 0.772807, Time/step: 6.214232
08/18/2025 11:44:59 - INFO -   Epoch: 1/1, Step: 320/508, Lr: 0.000000041-0.000000041-0.000041048-0.000041048-0.000041048, Loss: 1.076700, Time/step: 10.526103
08/18/2025 11:45:30 - INFO -   Epoch: 1/1, Step: 325/508, Lr: 0.000000040-0.000000040-0.000039956-0.000039956-0.000039956, Loss: 0.781384, Time/step: 6.167435
08/18/2025 11:46:18 - INFO -   Epoch: 1/1, Step: 330/508, Lr: 0.000000039-0.000000039-0.000038865-0.000038865-0.000038865, Loss: 0.976301, Time/step: 9.582141
08/18/2025 11:46:49 - INFO -   Epoch: 1/1, Step: 335/508, Lr: 0.000000038-0.000000038-0.000037773-0.000037773-0.000037773, Loss: 0.966661, Time/step: 6.246315
08/18/2025 11:47:37 - INFO -   Epoch: 1/1, Step: 340/508, Lr: 0.000000037-0.000000037-0.000036681-0.000036681-0.000036681, Loss: 0.868646, Time/step: 9.644702
08/18/2025 11:48:10 - INFO -   Epoch: 1/1, Step: 345/508, Lr: 0.000000036-0.000000036-0.000035590-0.000035590-0.000035590, Loss: 0.897120, Time/step: 6.566967
08/18/2025 11:48:52 - INFO -   Epoch: 1/1, Step: 350/508, Lr: 0.000000034-0.000000034-0.000034498-0.000034498-0.000034498, Loss: 0.889089, Time/step: 8.291450
08/18/2025 11:49:21 - INFO -   Epoch: 1/1, Step: 355/508, Lr: 0.000000033-0.000000033-0.000033406-0.000033406-0.000033406, Loss: 0.708901, Time/step: 5.930972
08/18/2025 11:50:16 - INFO -   Epoch: 1/1, Step: 360/508, Lr: 0.000000032-0.000000032-0.000032314-0.000032314-0.000032314, Loss: 0.588569, Time/step: 10.976209
08/18/2025 11:50:55 - INFO -   Epoch: 1/1, Step: 365/508, Lr: 0.000000031-0.000000031-0.000031223-0.000031223-0.000031223, Loss: 0.853631, Time/step: 7.680900
08/18/2025 11:51:30 - INFO -   Epoch: 1/1, Step: 370/508, Lr: 0.000000030-0.000000030-0.000030131-0.000030131-0.000030131, Loss: 0.904786, Time/step: 7.058363
08/18/2025 11:52:02 - INFO -   Epoch: 1/1, Step: 375/508, Lr: 0.000000029-0.000000029-0.000029039-0.000029039-0.000029039, Loss: 0.963020, Time/step: 6.387457
08/18/2025 11:52:58 - INFO -   Epoch: 1/1, Step: 380/508, Lr: 0.000000028-0.000000028-0.000027948-0.000027948-0.000027948, Loss: 1.003626, Time/step: 11.228790
08/18/2025 11:53:31 - INFO -   Epoch: 1/1, Step: 385/508, Lr: 0.000000027-0.000000027-0.000026856-0.000026856-0.000026856, Loss: 0.938646, Time/step: 6.541640
08/18/2025 11:54:03 - INFO -   Epoch: 1/1, Step: 390/508, Lr: 0.000000026-0.000000026-0.000025764-0.000025764-0.000025764, Loss: 1.003126, Time/step: 6.524011
08/18/2025 11:54:36 - INFO -   Epoch: 1/1, Step: 395/508, Lr: 0.000000025-0.000000025-0.000024672-0.000024672-0.000024672, Loss: 0.657307, Time/step: 6.494651
08/18/2025 11:55:32 - INFO -   Epoch: 1/1, Step: 400/508, Lr: 0.000000024-0.000000024-0.000023581-0.000023581-0.000023581, Loss: 0.769480, Time/step: 11.212761
08/18/2025 11:56:06 - INFO -   Epoch: 1/1, Step: 405/508, Lr: 0.000000022-0.000000022-0.000022489-0.000022489-0.000022489, Loss: 0.868990, Time/step: 6.760495
08/18/2025 11:56:37 - INFO -   Epoch: 1/1, Step: 410/508, Lr: 0.000000021-0.000000021-0.000021397-0.000021397-0.000021397, Loss: 0.825002, Time/step: 6.371495
08/18/2025 11:57:09 - INFO -   Epoch: 1/1, Step: 415/508, Lr: 0.000000020-0.000000020-0.000020306-0.000020306-0.000020306, Loss: 0.820271, Time/step: 6.399943
08/18/2025 11:58:07 - INFO -   Epoch: 1/1, Step: 420/508, Lr: 0.000000019-0.000000019-0.000019214-0.000019214-0.000019214, Loss: 1.071070, Time/step: 11.539568
08/18/2025 11:58:39 - INFO -   Epoch: 1/1, Step: 425/508, Lr: 0.000000018-0.000000018-0.000018122-0.000018122-0.000018122, Loss: 0.861311, Time/step: 6.282672
08/18/2025 11:59:17 - INFO -   Epoch: 1/1, Step: 430/508, Lr: 0.000000017-0.000000017-0.000017031-0.000017031-0.000017031, Loss: 1.094770, Time/step: 7.753269
08/18/2025 11:59:53 - INFO -   Epoch: 1/1, Step: 435/508, Lr: 0.000000016-0.000000016-0.000015939-0.000015939-0.000015939, Loss: 0.863345, Time/step: 7.051278
08/18/2025 12:00:44 - INFO -   Epoch: 1/1, Step: 440/508, Lr: 0.000000015-0.000000015-0.000014847-0.000014847-0.000014847, Loss: 1.143546, Time/step: 10.220194
08/18/2025 12:01:21 - INFO -   Epoch: 1/1, Step: 445/508, Lr: 0.000000014-0.000000014-0.000013755-0.000013755-0.000013755, Loss: 0.872078, Time/step: 7.411329
08/18/2025 12:01:54 - INFO -   Epoch: 1/1, Step: 450/508, Lr: 0.000000013-0.000000013-0.000012664-0.000012664-0.000012664, Loss: 0.793118, Time/step: 6.713805
08/18/2025 12:02:33 - INFO -   Epoch: 1/1, Step: 455/508, Lr: 0.000000012-0.000000012-0.000011572-0.000011572-0.000011572, Loss: 0.755699, Time/step: 7.708566
08/18/2025 12:03:16 - INFO -   Epoch: 1/1, Step: 460/508, Lr: 0.000000010-0.000000010-0.000010480-0.000010480-0.000010480, Loss: 0.679306, Time/step: 8.562503
08/18/2025 12:03:49 - INFO -   Epoch: 1/1, Step: 465/508, Lr: 0.000000009-0.000000009-0.000009389-0.000009389-0.000009389, Loss: 0.814559, Time/step: 6.697904
08/18/2025 12:04:37 - INFO -   Epoch: 1/1, Step: 470/508, Lr: 0.000000008-0.000000008-0.000008297-0.000008297-0.000008297, Loss: 0.909599, Time/step: 9.607026
08/18/2025 12:05:12 - INFO -   Epoch: 1/1, Step: 475/508, Lr: 0.000000007-0.000000007-0.000007205-0.000007205-0.000007205, Loss: 1.100141, Time/step: 7.027620
08/18/2025 12:06:16 - INFO -   Epoch: 1/1, Step: 480/508, Lr: 0.000000006-0.000000006-0.000006114-0.000006114-0.000006114, Loss: 0.822013, Time/step: 12.794426
08/18/2025 12:06:47 - INFO -   Epoch: 1/1, Step: 485/508, Lr: 0.000000005-0.000000005-0.000005022-0.000005022-0.000005022, Loss: 0.783311, Time/step: 6.203620
08/18/2025 12:07:23 - INFO -   Epoch: 1/1, Step: 490/508, Lr: 0.000000004-0.000000004-0.000003930-0.000003930-0.000003930, Loss: 0.737414, Time/step: 7.229828
08/18/2025 12:08:01 - INFO -   Epoch: 1/1, Step: 495/508, Lr: 0.000000003-0.000000003-0.000002838-0.000002838-0.000002838, Loss: 0.853031, Time/step: 7.564286
08/18/2025 12:09:00 - INFO -   Epoch: 1/1, Step: 500/508, Lr: 0.000000002-0.000000002-0.000001747-0.000001747-0.000001747, Loss: 0.828831, Time/step: 11.747141
08/18/2025 12:09:33 - INFO -   Epoch: 1/1, Step: 505/508, Lr: 0.000000001-0.000000001-0.000000655-0.000000655-0.000000655, Loss: 0.816431, Time/step: 6.638536
08/18/2025 12:10:08 - INFO -   Epoch 1/1 Finished, Train Loss: 1.200620
08/18/2025 12:10:14 - INFO -   Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0
08/18/2025 12:10:14 - INFO -   Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_opt.bin.0
08/18/2025 12:10:14 - INFO -   Eval on val dataset
08/18/2025 12:10:15 - WARNING -   Eval under the multi-sentence per video clip setting.
08/18/2025 12:10:15 - WARNING -   sentence num: 4290, video num: 100
0/1351/1352/1353/1354/1355/1356/1357/1358/1359/13510/13511/13512/13513/13514/13515/13516/13517/13518/13519/13520/13521/13522/13523/13524/13525/13526/13527/13528/13529/13530/13531/13532/13533/13534/13535/13536/13537/13538/13539/13540/13541/13542/13543/13544/13545/13546/13547/13548/13549/13550/13551/13552/13553/13554/13555/13556/13557/13558/13559/13560/13561/13562/13563/13564/13565/13566/13567/13568/13569/13570/13571/13572/13573/13574/13575/13576/13577/13578/13579/13580/13581/13582/13583/13584/13585/13586/13587/13588/13589/13590/13591/13592/13593/13594/13595/13596/13597/13598/13599/135100/135101/135102/135103/135104/135105/135106/135107/135108/135109/135110/135111/135112/135113/135114/135115/135116/135117/135118/135119/135120/135121/135122/135123/135124/135125/135126/135127/135128/135129/135130/135131/135132/135133/135134/13508/18/2025 12:24:17 - INFO -   before reshape, sim matrix size: 4290 x 100
08/18/2025 12:24:17 - INFO -   after reshape, sim matrix size: 100 x 62 x 100
08/18/2025 12:24:18 - INFO -   Text-to-Video:
08/18/2025 12:24:18 - INFO -   	>>>  R@1: 59.3 - R@5: 86.7 - R@10: 92.1 - Median R: 1.0 - Mean R: 3.9
08/18/2025 12:24:18 - INFO -   Video-to-Text:
08/18/2025 12:24:18 - INFO -   	>>>  V2T$R@1: 76.5 - V2T$R@5: 97.1 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.6
08/18/2025 12:24:18 - INFO -   The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0, the R1 is: 59.3240
/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py:321: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_state_dict = torch.load(model_file, map_location='cpu')
08/18/2025 12:24:18 - INFO -   Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0
08/18/2025 12:24:22 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 12:24:22 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 12:24:22 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 12:24:22 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 12:24:22 - WARNING -   Test retrieval by loose type.
08/18/2025 12:24:22 - WARNING -   	 embed_dim: 512
08/18/2025 12:24:22 - WARNING -   	 image_resolution: 224
08/18/2025 12:24:22 - WARNING -   	 vision_layers: 12
08/18/2025 12:24:22 - WARNING -   	 vision_width: 768
08/18/2025 12:24:22 - WARNING -   	 vision_patch_size: 32
08/18/2025 12:24:22 - WARNING -   	 context_length: 77
08/18/2025 12:24:22 - WARNING -   	 vocab_size: 49408
08/18/2025 12:24:22 - WARNING -   	 transformer_width: 512
08/18/2025 12:24:22 - WARNING -   	 transformer_heads: 8
08/18/2025 12:24:22 - WARNING -   	 transformer_layers: 12
08/18/2025 12:24:22 - WARNING -   		 linear_patch: 2d
08/18/2025 12:24:22 - WARNING -   	 cut_top_layer: 0
08/18/2025 12:24:24 - WARNING -   	 sim_header: meanP
08/18/2025 12:24:29 - INFO -   --------------------
08/18/2025 12:24:29 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/18/2025 12:24:29 - WARNING -   Eval under the multi-sentence per video clip setting.
08/18/2025 12:24:29 - WARNING -   sentence num: 27763, video num: 670
0/8681/8682/8683/8684/8685/8686/8687/8688/8689/86810/86811/86812/86813/86814/86815/86816/86817/86818/86819/86820/86821/86822/86823/86824/86825/86826/86827/86828/86829/86830/86831/86832/86833/86834/86835/86836/86837/86838/86839/86840/86841/86842/86843/86844/86845/86846/86847/86848/86849/86850/86851/86852/86853/86854/86855/86856/86857/86858/86859/86860/86861/86862/86863/86864/86865/86866/86867/86868/86869/86870/86871/86872/86873/86874/86875/86876/86877/86878/86879/86880/86881/86882/86883/86884/86885/86886/86887/86888/86889/86890/86891/86892/86893/86894/86895/86896/86897/86898/86899/868100/868101/868102/868103/868104/868105/868106/868107/868108/868109/868110/868111/868112/868113/868114/868115/868116/868117/868118/868119/868120/868121/868122/868123/868124/868125/868126/868127/868128/868129/868130/868131/868132/868133/868134/868135/868136/868137/868138/868139/868140/868141/868142/868143/868144/868145/868146/868147/868148/868149/868150/868151/868152/868153/868154/868155/868156/868157/868158/868159/868160/868161/868162/868163/868164/868165/868166/868167/868168/868169/868170/868171/868172/868173/868174/868175/868176/868177/868178/868179/868180/868181/868182/868183/868184/868185/868186/868187/868188/868189/868190/868191/868192/868193/868194/868195/868196/868197/868198/868199/868200/868201/868202/868203/868204/868205/868206/868207/868208/868209/868210/868211/868212/868213/868214/868215/868216/868217/868218/868219/868220/868221/868222/868223/868224/868225/868226/868227/868228/868229/868230/868231/868232/868233/868234/868235/868236/868237/868238/868239/868240/868241/868242/868243/868244/868245/868246/868247/868248/868249/868250/868251/868252/868253/868254/868255/868256/868257/868258/868259/868260/868261/868262/868263/868264/868265/868266/868267/868268/868269/868270/868271/868272/868273/868274/868275/868276/868277/868278/868279/868280/868281/868282/868283/868284/868285/868286/868287/868288/868289/868290/868291/868292/868293/868294/868295/868296/868297/868298/868299/868300/868301/868302/868303/868304/868305/868306/868307/868308/868309/868310/868311/868312/868313/868314/868315/868316/868317/868318/868319/868320/868321/868322/868323/868324/868325/868326/868327/868328/868329/868330/868331/868332/868333/868334/868335/868336/868337/868338/868339/868340/868341/868342/868343/868344/868345/868346/868347/868348/868349/868350/868351/868352/868353/868354/868355/868356/868357/868358/868359/868360/868361/868362/868363/868364/868365/868366/868367/868368/868369/868370/868371/868372/868373/868374/868375/868376/868377/868378/868379/868380/868381/868382/868383/868384/868385/868386/868387/868388/868389/868390/868391/868392/868393/868394/868395/868396/868397/868398/868399/868400/868401/868402/868403/868404/868405/868406/868407/868408/868409/868410/868411/868412/868413/868414/868415/868416/868417/868418/868419/868420/868421/868422/868423/868424/868425/868426/868427/868428/868429/868430/868431/868432/868433/868434/868435/868436/868437/868438/868439/868440/868441/868442/868443/868444/868445/868446/868447/868448/868449/868450/868451/868452/868453/868454/868455/868456/868457/868458/868459/868460/868461/868462/868463/868464/868465/868466/868467/868468/868469/868470/868471/868472/868473/868474/868475/868476/868477/868478/868479/868480/868481/868482/868483/868484/868485/868486/868487/868488/868489/868490/868491/868492/868493/868494/868495/868496/868497/868498/868499/868500/868501/868502/868503/868504/868505/868506/868507/868508/868509/868510/868511/868512/868513/868514/868515/868516/868517/868518/868519/868520/868521/868522/868523/868524/868525/868526/868527/868528/868529/868530/868531/868532/868533/868534/868535/868536/868537/868538/868539/868540/868541/868542/868543/868544/868545/868546/868547/868548/868549/868550/868551/868552/868553/868554/868555/868556/868557/868558/868559/868560/868561/868562/868563/868564/868565/868566/868567/868568/868569/868570/868571/868572/868573/868574/868575/868576/868577/868578/868579/868580/868581/868582/868583/868584/868585/868586/868587/868588/868589/868590/868591/868592/868593/868594/868595/868596/868597/868598/868599/868600/868601/868602/868603/868604/868605/868606/868607/868608/868609/868610/868611/868612/868613/868614/868615/868616/868617/868618/868619/868620/868621/868622/868623/868624/868625/868626/868627/868628/868629/868630/868631/868632/868633/868634/868635/868636/868637/868638/868639/868640/868641/868642/868643/868644/868645/868646/868647/868648/868649/868650/868651/868652/868653/868654/868655/868656/868657/868658/868659/868660/868661/868662/868663/868664/868665/868666/868667/868668/868669/868670/868671/868672/868673/868674/868675/868676/868677/868678/868679/868680/868681/868682/868683/868684/868685/868686/868687/868688/868689/868690/868691/868692/868693/868694/868695/868696/868697/868698/868699/868700/868701/868702/868703/868704/868705/868706/868707/868708/868709/868710/868711/868712/868713/868714/868715/868716/868717/868718/868719/868720/868721/868722/868723/868724/868725/868726/868727/868728/868729/868730/868731/868732/868733/868734/868735/868736/868737/868738/868739/868740/868741/868742/868743/868744/868745/868746/868747/868748/868749/868750/868751/868752/868753/868754/868755/868756/868757/868758/868759/868760/868761/868762/868763/868764/868765/868766/868767/868768/868769/868770/868771/868772/868773/868774/868775/868776/868777/868778/868779/868780/868781/868782/868783/868784/868785/868786/868787/868788/868789/868790/868791/868792/868793/868794/868795/868796/868797/868798/868799/868800/868801/868802/868803/868804/868805/868806/868807/868808/868809/868810/868811/868812/868813/868814/868815/868816/868817/868818/868819/868820/868821/868822/868823/868824/868825/868826/868827/868828/868829/868830/868831/868832/868833/868834/868835/868836/868837/868838/868839/868840/868841/868842/868843/868844/868845/868846/868847/868848/868849/868850/868851/868852/868853/868854/868855/868856/868857/868858/868859/868860/868861/868862/868863/868864/868865/868866/868867/86808/18/2025 13:55:11 - INFO -   before reshape, sim matrix size: 27763 x 670
08/18/2025 13:55:12 - INFO -   after reshape, sim matrix size: 670 x 81 x 670
08/18/2025 13:55:16 - INFO -   Text-to-Video:
08/18/2025 13:55:16 - INFO -   	>>>  R@1: 29.5 - R@5: 59.5 - R@10: 71.6 - Median R: 4.0 - Mean R: 21.4
08/18/2025 13:55:16 - INFO -   Video-to-Text:
08/18/2025 13:55:16 - INFO -   	>>>  V2T$R@1: 41.5 - V2T$R@5: 73.4 - V2T$R@10: 83.7 - V2T$Median R: 2.0 - V2T$Mean R: 9.2
W0818 15:23:54.213633 140482375747392 torch/distributed/run.py:779] 
W0818 15:23:54.213633 140482375747392 torch/distributed/run.py:779] *****************************************
W0818 15:23:54.213633 140482375747392 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 15:23:54.213633 140482375747392 torch/distributed/run.py:779] *****************************************
08/18/2025 15:23:57 - INFO -   Effective parameters:
08/18/2025 15:23:57 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 15:23:57 - INFO -     <<< batch_size: 96
08/18/2025 15:23:57 - INFO -     <<< batch_size_val: 32
08/18/2025 15:23:57 - INFO -     <<< cache_dir: 
08/18/2025 15:23:57 - INFO -     <<< coef_lr: 0.001
08/18/2025 15:23:57 - INFO -     <<< cross_model: cross-base
08/18/2025 15:23:57 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 15:23:57 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 15:23:57 - INFO -     <<< datatype: msvd
08/18/2025 15:23:57 - INFO -     <<< do_eval: False
08/18/2025 15:23:57 - INFO -     <<< do_lower_case: False
08/18/2025 15:23:57 - INFO -     <<< do_pretrain: False
08/18/2025 15:23:57 - INFO -     <<< do_train: True
08/18/2025 15:23:57 - INFO -     <<< epochs: 1
08/18/2025 15:23:57 - INFO -     <<< eval_frame_order: 0
08/18/2025 15:23:57 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 15:23:57 - INFO -     <<< feature_framerate: 1
08/18/2025 15:23:57 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 15:23:57 - INFO -     <<< fp16: False
08/18/2025 15:23:57 - INFO -     <<< fp16_opt_level: O1
08/18/2025 15:23:57 - INFO -     <<< freeze_layer_num: 9
08/18/2025 15:23:57 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 15:23:57 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 15:23:57 - INFO -     <<< init_model: None
08/18/2025 15:23:57 - INFO -     <<< linear_patch: 2d
08/18/2025 15:23:57 - INFO -     <<< local_rank: 0
08/18/2025 15:23:57 - INFO -     <<< loose_type: True
08/18/2025 15:23:57 - INFO -     <<< lr: 0.0001
08/18/2025 15:23:57 - INFO -     <<< lr_decay: 0.9
08/18/2025 15:23:57 - INFO -     <<< margin: 0.1
08/18/2025 15:23:57 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 15:23:57 - INFO -     <<< max_frames: 12
08/18/2025 15:23:57 - INFO -     <<< max_words: 32
08/18/2025 15:23:57 - INFO -     <<< n_display: 5
08/18/2025 15:23:57 - INFO -     <<< n_gpu: 1
08/18/2025 15:23:57 - INFO -     <<< n_pair: 1
08/18/2025 15:23:57 - INFO -     <<< negative_weighting: 1
08/18/2025 15:23:57 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 15:23:57 - INFO -     <<< num_thread_reader: 4
08/18/2025 15:23:57 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 15:23:57 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 15:23:57 - INFO -     <<< rank: 0
08/18/2025 15:23:57 - INFO -     <<< resume_model: None
08/18/2025 15:23:57 - INFO -     <<< sampled_use_mil: False
08/18/2025 15:23:57 - INFO -     <<< seed: 42
08/18/2025 15:23:57 - INFO -     <<< sim_header: meanP
08/18/2025 15:23:57 - INFO -     <<< slice_framepos: 0
08/18/2025 15:23:57 - INFO -     <<< task_type: retrieval
08/18/2025 15:23:57 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 15:23:57 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 15:23:57 - INFO -     <<< train_frame_order: 0
08/18/2025 15:23:57 - INFO -     <<< use_mil: False
08/18/2025 15:23:57 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 15:23:57 - INFO -     <<< video_dim: 1024
08/18/2025 15:23:57 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 15:23:57 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 15:23:57 - INFO -     <<< world_size: 2
08/18/2025 15:23:57 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 15:24:35 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 15:24:35 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 15:24:35 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 15:24:35 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 15:24:35 - WARNING -   Test retrieval by loose type.
08/18/2025 15:24:35 - WARNING -   	 embed_dim: 512
08/18/2025 15:24:35 - WARNING -   	 image_resolution: 224
08/18/2025 15:24:35 - WARNING -   	 vision_layers: 12
08/18/2025 15:24:35 - WARNING -   	 vision_width: 768
08/18/2025 15:24:35 - WARNING -   	 vision_patch_size: 32
08/18/2025 15:24:35 - WARNING -   	 context_length: 77
08/18/2025 15:24:35 - WARNING -   	 vocab_size: 49408
08/18/2025 15:24:35 - WARNING -   	 transformer_width: 512
08/18/2025 15:24:35 - WARNING -   	 transformer_heads: 8
08/18/2025 15:24:35 - WARNING -   	 transformer_layers: 12
08/18/2025 15:24:35 - WARNING -   		 linear_patch: 2d
08/18/2025 15:24:35 - WARNING -   	 cut_top_layer: 0
08/18/2025 15:24:38 - WARNING -   	 sim_header: meanP
08/18/2025 15:24:45 - INFO -   --------------------
08/18/2025 15:24:45 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 15:24:45 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 15:24:46 - INFO -   ***** Running test *****
08/18/2025 15:24:46 - INFO -     Num examples = 27763
08/18/2025 15:24:46 - INFO -     Batch size = 32
08/18/2025 15:24:46 - INFO -     Num steps = 868
08/18/2025 15:24:46 - INFO -   ***** Running val *****
08/18/2025 15:24:46 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/18/2025 15:24:49 - INFO -   ***** Running training *****
08/18/2025 15:24:49 - INFO -     Num examples = 48774
08/18/2025 15:24:49 - INFO -     Batch size = 96
08/18/2025 15:24:49 - INFO -     Num steps = 508
08/18/2025 15:28:19 - INFO -   Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.148196, Time/step: 41.983546
08/18/2025 15:29:40 - INFO -   Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.029150, Time/step: 16.265100
08/18/2025 15:30:41 - INFO -   Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.846721, Time/step: 12.133887
08/18/2025 15:31:17 - INFO -   Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.532522, Time/step: 7.108163
08/18/2025 15:32:19 - INFO -   Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.522690, Time/step: 12.532584
08/18/2025 15:32:57 - INFO -   Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.342144, Time/step: 7.638184
08/18/2025 15:33:24 - INFO -   Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.031947, Time/step: 5.311534
08/18/2025 15:34:00 - INFO -   Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.934312, Time/step: 7.197199
08/18/2025 15:34:59 - INFO -   Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.567915, Time/step: 11.783598
08/18/2025 15:35:32 - INFO -   Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.603093, Time/step: 6.624231
08/18/2025 15:36:09 - INFO -   Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.312273, Time/step: 7.396612
08/18/2025 15:36:52 - INFO -   Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.226677, Time/step: 8.498573
08/18/2025 15:37:52 - INFO -   Epoch: 1/1, Step: 65/508, Lr: , Loss: 0.921959, Time/step: 12.063239
08/18/2025 15:38:31 - INFO -   Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.859378, Time/step: 7.755061
08/18/2025 15:39:16 - INFO -   Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.921617, Time/step: 9.069428
08/18/2025 15:39:52 - INFO -   Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.668513, Time/step: 7.218000
08/18/2025 15:40:36 - INFO -   Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.862821, Time/step: 8.822678
08/18/2025 15:41:14 - INFO -   Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.786323, Time/step: 7.507651
08/18/2025 15:41:59 - INFO -   Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.938177, Time/step: 9.097248
08/18/2025 15:42:32 - INFO -   Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.885353, Time/step: 6.558213
08/18/2025 15:43:20 - INFO -   Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.573392, Time/step: 9.688838
08/18/2025 15:43:55 - INFO -   Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.731305, Time/step: 6.885058
08/18/2025 15:44:39 - INFO -   Epoch: 1/1, Step: 115/508, Lr: , Loss: 0.760481, Time/step: 8.739034
08/18/2025 15:45:09 - INFO -   Epoch: 1/1, Step: 120/508, Lr: , Loss: 0.655350, Time/step: 6.122901
W0818 15:48:59.779606 140305442613056 torch/distributed/run.py:779] 
W0818 15:48:59.779606 140305442613056 torch/distributed/run.py:779] *****************************************
W0818 15:48:59.779606 140305442613056 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0818 15:48:59.779606 140305442613056 torch/distributed/run.py:779] *****************************************
08/18/2025 15:49:10 - INFO -   Effective parameters:
08/18/2025 15:49:10 - INFO -     <<< amp: True
08/18/2025 15:49:10 - INFO -     <<< batch_size: 96
08/18/2025 15:49:10 - INFO -     <<< batch_size_val: 32
08/18/2025 15:49:10 - INFO -     <<< cache_dir: 
08/18/2025 15:49:10 - INFO -     <<< coef_lr: 0.001
08/18/2025 15:49:10 - INFO -     <<< cross_model: cross-base
08/18/2025 15:49:10 - INFO -     <<< cross_num_hidden_layers: 4
08/18/2025 15:49:10 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/18/2025 15:49:10 - INFO -     <<< datatype: msvd
08/18/2025 15:49:10 - INFO -     <<< do_eval: False
08/18/2025 15:49:10 - INFO -     <<< do_lower_case: False
08/18/2025 15:49:10 - INFO -     <<< do_pretrain: False
08/18/2025 15:49:10 - INFO -     <<< do_train: True
08/18/2025 15:49:10 - INFO -     <<< epochs: 1
08/18/2025 15:49:10 - INFO -     <<< eval_frame_order: 0
08/18/2025 15:49:10 - INFO -     <<< expand_msrvtt_sentences: False
08/18/2025 15:49:10 - INFO -     <<< feature_framerate: 1
08/18/2025 15:49:10 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/18/2025 15:49:10 - INFO -     <<< freeze_layer_num: 9
08/18/2025 15:49:10 - INFO -     <<< gradient_accumulation_steps: 1
08/18/2025 15:49:10 - INFO -     <<< hard_negative_rate: 0.5
08/18/2025 15:49:10 - INFO -     <<< init_model: None
08/18/2025 15:49:10 - INFO -     <<< linear_patch: 2d
08/18/2025 15:49:10 - INFO -     <<< local_rank: 0
08/18/2025 15:49:10 - INFO -     <<< loose_type: True
08/18/2025 15:49:10 - INFO -     <<< lr: 0.0001
08/18/2025 15:49:10 - INFO -     <<< lr_decay: 0.9
08/18/2025 15:49:10 - INFO -     <<< margin: 0.1
08/18/2025 15:49:10 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/18/2025 15:49:10 - INFO -     <<< max_frames: 12
08/18/2025 15:49:10 - INFO -     <<< max_words: 32
08/18/2025 15:49:10 - INFO -     <<< n_display: 5
08/18/2025 15:49:10 - INFO -     <<< n_gpu: 1
08/18/2025 15:49:10 - INFO -     <<< n_pair: 1
08/18/2025 15:49:10 - INFO -     <<< negative_weighting: 1
08/18/2025 15:49:10 - INFO -     <<< new_added_modules: ['Adapter']
08/18/2025 15:49:10 - INFO -     <<< num_thread_reader: 4
08/18/2025 15:49:10 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/18/2025 15:49:10 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/18/2025 15:49:10 - INFO -     <<< rank: 0
08/18/2025 15:49:10 - INFO -     <<< resume_model: None
08/18/2025 15:49:10 - INFO -     <<< sampled_use_mil: False
08/18/2025 15:49:10 - INFO -     <<< seed: 42
08/18/2025 15:49:10 - INFO -     <<< sim_header: meanP
08/18/2025 15:49:10 - INFO -     <<< slice_framepos: 0
08/18/2025 15:49:10 - INFO -     <<< task_type: retrieval
08/18/2025 15:49:10 - INFO -     <<< text_num_hidden_layers: 12
08/18/2025 15:49:10 - INFO -     <<< train_csv: data/.train.csv
08/18/2025 15:49:10 - INFO -     <<< train_frame_order: 0
08/18/2025 15:49:10 - INFO -     <<< use_mil: False
08/18/2025 15:49:10 - INFO -     <<< val_csv: data/.val.csv
08/18/2025 15:49:10 - INFO -     <<< video_dim: 1024
08/18/2025 15:49:10 - INFO -     <<< visual_num_hidden_layers: 12
08/18/2025 15:49:10 - INFO -     <<< warmup_proportion: 0.1
08/18/2025 15:49:10 - INFO -     <<< world_size: 2
08/18/2025 15:49:10 - INFO -   device: cuda:0 n_gpu: 2
08/18/2025 15:49:10 - INFO -   device: cuda:1 n_gpu: 2
08/18/2025 15:49:17 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 15:49:17 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 15:49:17 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 15:49:17 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 15:49:17 - WARNING -   Test retrieval by loose type.
08/18/2025 15:49:17 - WARNING -   	 embed_dim: 512
08/18/2025 15:49:17 - WARNING -   	 image_resolution: 224
08/18/2025 15:49:17 - WARNING -   	 vision_layers: 12
08/18/2025 15:49:17 - WARNING -   	 vision_width: 768
08/18/2025 15:49:17 - WARNING -   	 vision_patch_size: 32
08/18/2025 15:49:17 - WARNING -   	 context_length: 77
08/18/2025 15:49:17 - WARNING -   	 vocab_size: 49408
08/18/2025 15:49:17 - WARNING -   	 transformer_width: 512
08/18/2025 15:49:17 - WARNING -   	 transformer_heads: 8
08/18/2025 15:49:17 - WARNING -   	 transformer_layers: 12
08/18/2025 15:49:17 - WARNING -   		 linear_patch: 2d
08/18/2025 15:49:17 - WARNING -   	 cut_top_layer: 0
08/18/2025 15:49:19 - WARNING -   	 sim_header: meanP
08/18/2025 15:49:24 - INFO -   --------------------
08/18/2025 15:49:24 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/18/2025 15:49:24 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
08/18/2025 15:49:24 - WARNING -   GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
Video number: 1200
Total Paire: 48774
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/18/2025 15:49:25 - INFO -   ***** Running test *****
08/18/2025 15:49:25 - INFO -     Num examples = 27763
08/18/2025 15:49:25 - INFO -     Batch size = 32
08/18/2025 15:49:25 - INFO -     Num steps = 868
08/18/2025 15:49:25 - INFO -   ***** Running val *****
08/18/2025 15:49:25 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
08/18/2025 15:49:25 - INFO -   ***** Running training *****
08/18/2025 15:49:25 - INFO -     Num examples = 48774
08/18/2025 15:49:25 - INFO -     Batch size = 96
08/18/2025 15:49:25 - INFO -     Num steps = 508
08/18/2025 15:50:39 - INFO -   Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.161296, Time/step: 14.734303
08/18/2025 15:51:07 - INFO -   Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.082540, Time/step: 5.624541
08/18/2025 15:51:45 - INFO -   Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.929565, Time/step: 7.612914
08/18/2025 15:52:23 - INFO -   Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.625264, Time/step: 7.619840
08/18/2025 15:53:23 - INFO -   Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.635376, Time/step: 11.855392
08/18/2025 15:54:01 - INFO -   Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.479228, Time/step: 7.634237
08/18/2025 15:54:37 - INFO -   Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.242137, Time/step: 7.163515
08/18/2025 15:55:09 - INFO -   Epoch: 1/1, Step: 40/508, Lr: , Loss: 2.220784, Time/step: 6.481897
08/18/2025 15:56:11 - INFO -   Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.984467, Time/step: 12.373189
08/18/2025 15:56:40 - INFO -   Epoch: 1/1, Step: 50/508, Lr: , Loss: 2.039062, Time/step: 5.870484
08/18/2025 15:57:14 - INFO -   Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.928345, Time/step: 6.791457
08/18/2025 15:57:49 - INFO -   Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.857717, Time/step: 6.881735
08/18/2025 15:58:49 - INFO -   Epoch: 1/1, Step: 65/508, Lr: , Loss: 1.617147, Time/step: 12.089793
08/18/2025 15:59:22 - INFO -   Epoch: 1/1, Step: 70/508, Lr: , Loss: 1.572039, Time/step: 6.670845
08/18/2025 16:00:00 - INFO -   Epoch: 1/1, Step: 75/508, Lr: , Loss: 1.643481, Time/step: 7.584756
08/18/2025 16:00:38 - INFO -   Epoch: 1/1, Step: 80/508, Lr: , Loss: 1.415243, Time/step: 7.516461
08/18/2025 16:01:38 - INFO -   Epoch: 1/1, Step: 85/508, Lr: , Loss: 1.488297, Time/step: 11.909255
08/18/2025 16:02:18 - INFO -   Epoch: 1/1, Step: 90/508, Lr: , Loss: 1.509903, Time/step: 8.079270
08/18/2025 16:02:51 - INFO -   Epoch: 1/1, Step: 95/508, Lr: , Loss: 1.559367, Time/step: 6.576564
08/18/2025 16:03:27 - INFO -   Epoch: 1/1, Step: 100/508, Lr: , Loss: 1.554247, Time/step: 7.330278
08/18/2025 16:04:31 - INFO -   Epoch: 1/1, Step: 105/508, Lr: , Loss: 1.191676, Time/step: 12.742496
08/18/2025 16:05:05 - INFO -   Epoch: 1/1, Step: 110/508, Lr: , Loss: 1.323306, Time/step: 6.753156
08/18/2025 16:05:39 - INFO -   Epoch: 1/1, Step: 115/508, Lr: , Loss: 1.301656, Time/step: 6.834237
08/18/2025 16:06:19 - INFO -   Epoch: 1/1, Step: 120/508, Lr: , Loss: 1.213931, Time/step: 7.884535
08/18/2025 16:07:25 - INFO -   Epoch: 1/1, Step: 125/508, Lr: , Loss: 1.359456, Time/step: 13.332971
08/18/2025 16:08:00 - INFO -   Epoch: 1/1, Step: 130/508, Lr: , Loss: 1.447248, Time/step: 6.898051
08/18/2025 16:08:41 - INFO -   Epoch: 1/1, Step: 135/508, Lr: , Loss: 0.937022, Time/step: 8.231794
08/18/2025 16:09:16 - INFO -   Epoch: 1/1, Step: 140/508, Lr: , Loss: 1.194656, Time/step: 6.958037
08/18/2025 16:10:37 - INFO -   Epoch: 1/1, Step: 145/508, Lr: , Loss: 1.285940, Time/step: 16.189644
08/18/2025 16:11:13 - INFO -   Epoch: 1/1, Step: 150/508, Lr: , Loss: 1.123048, Time/step: 7.244070
08/18/2025 16:12:15 - INFO -   Epoch: 1/1, Step: 155/508, Lr: , Loss: 1.135038, Time/step: 12.380581
08/18/2025 16:13:13 - INFO -   Epoch: 1/1, Step: 160/508, Lr: , Loss: 1.177839, Time/step: 11.622806
08/18/2025 16:14:39 - INFO -   Epoch: 1/1, Step: 165/508, Lr: , Loss: 1.158091, Time/step: 17.283304
08/18/2025 16:15:32 - INFO -   Epoch: 1/1, Step: 170/508, Lr: , Loss: 1.115031, Time/step: 10.518164
08/18/2025 16:16:52 - INFO -   Epoch: 1/1, Step: 175/508, Lr: , Loss: 1.108800, Time/step: 16.085583
08/18/2025 16:17:33 - INFO -   Epoch: 1/1, Step: 180/508, Lr: , Loss: 1.152897, Time/step: 8.028612
08/18/2025 16:19:19 - INFO -   Epoch: 1/1, Step: 185/508, Lr: , Loss: 1.192948, Time/step: 21.156892
08/18/2025 16:20:08 - INFO -   Epoch: 1/1, Step: 190/508, Lr: , Loss: 1.026159, Time/step: 9.687581
08/18/2025 16:21:15 - INFO -   Epoch: 1/1, Step: 195/508, Lr: , Loss: 1.046593, Time/step: 13.368271
08/18/2025 16:21:56 - INFO -   Epoch: 1/1, Step: 200/508, Lr: , Loss: 0.965902, Time/step: 8.297012
08/18/2025 16:23:22 - INFO -   Epoch: 1/1, Step: 205/508, Lr: , Loss: 1.009337, Time/step: 17.248528
08/18/2025 16:24:08 - INFO -   Epoch: 1/1, Step: 210/508, Lr: , Loss: 1.237711, Time/step: 9.198287
08/18/2025 16:24:51 - INFO -   Epoch: 1/1, Step: 215/508, Lr: , Loss: 0.835739, Time/step: 8.462134
08/18/2025 16:26:02 - INFO -   Epoch: 1/1, Step: 220/508, Lr: , Loss: 0.868544, Time/step: 14.274684
08/18/2025 16:27:14 - INFO -   Epoch: 1/1, Step: 225/508, Lr: , Loss: 0.894588, Time/step: 14.318230
08/18/2025 16:27:44 - INFO -   Epoch: 1/1, Step: 230/508, Lr: , Loss: 1.260493, Time/step: 6.107132
08/18/2025 16:28:20 - INFO -   Epoch: 1/1, Step: 235/508, Lr: , Loss: 0.887641, Time/step: 7.181333
08/18/2025 16:28:59 - INFO -   Epoch: 1/1, Step: 240/508, Lr: , Loss: 0.934614, Time/step: 7.764345
08/18/2025 16:30:09 - INFO -   Epoch: 1/1, Step: 245/508, Lr: , Loss: 1.175524, Time/step: 13.952201
08/18/2025 16:30:59 - INFO -   Epoch: 1/1, Step: 250/508, Lr: , Loss: 0.983095, Time/step: 10.022588
08/18/2025 16:31:38 - INFO -   Epoch: 1/1, Step: 255/508, Lr: , Loss: 0.998955, Time/step: 7.842333
08/18/2025 16:32:43 - INFO -   Epoch: 1/1, Step: 260/508, Lr: , Loss: 0.899659, Time/step: 12.998223
08/18/2025 16:33:07 - INFO -   Epoch: 1/1, Step: 265/508, Lr: , Loss: 0.890780, Time/step: 4.885516
08/18/2025 16:34:12 - INFO -   Epoch: 1/1, Step: 270/508, Lr: , Loss: 0.993796, Time/step: 12.842804
08/18/2025 16:35:00 - INFO -   Epoch: 1/1, Step: 275/508, Lr: , Loss: 1.080081, Time/step: 9.686527
08/18/2025 16:35:38 - INFO -   Epoch: 1/1, Step: 280/508, Lr: , Loss: 0.797993, Time/step: 7.654254
08/18/2025 16:36:16 - INFO -   Epoch: 1/1, Step: 285/508, Lr: , Loss: 0.970538, Time/step: 7.551676
08/18/2025 16:37:28 - INFO -   Epoch: 1/1, Step: 290/508, Lr: , Loss: 0.900870, Time/step: 14.307963
08/18/2025 16:38:02 - INFO -   Epoch: 1/1, Step: 295/508, Lr: , Loss: 0.879392, Time/step: 6.914828
08/18/2025 16:38:42 - INFO -   Epoch: 1/1, Step: 300/508, Lr: , Loss: 0.747445, Time/step: 7.959415
08/18/2025 16:39:38 - INFO -   Epoch: 1/1, Step: 305/508, Lr: , Loss: 0.770832, Time/step: 11.263449
08/18/2025 16:40:40 - INFO -   Epoch: 1/1, Step: 310/508, Lr: , Loss: 0.716923, Time/step: 12.368832
08/18/2025 16:41:13 - INFO -   Epoch: 1/1, Step: 315/508, Lr: , Loss: 0.807159, Time/step: 6.636907
08/18/2025 16:41:47 - INFO -   Epoch: 1/1, Step: 320/508, Lr: , Loss: 1.108919, Time/step: 6.801481
08/18/2025 16:42:38 - INFO -   Epoch: 1/1, Step: 325/508, Lr: , Loss: 0.813686, Time/step: 10.092655
08/18/2025 16:43:43 - INFO -   Epoch: 1/1, Step: 330/508, Lr: , Loss: 1.005765, Time/step: 12.941993
08/18/2025 16:44:18 - INFO -   Epoch: 1/1, Step: 335/508, Lr: , Loss: 0.985402, Time/step: 7.157106
08/18/2025 16:44:58 - INFO -   Epoch: 1/1, Step: 340/508, Lr: , Loss: 0.896863, Time/step: 7.956689
08/18/2025 16:45:40 - INFO -   Epoch: 1/1, Step: 345/508, Lr: , Loss: 0.933975, Time/step: 8.433771
08/18/2025 16:46:32 - INFO -   Epoch: 1/1, Step: 350/508, Lr: , Loss: 0.913721, Time/step: 10.320879
08/18/2025 16:47:09 - INFO -   Epoch: 1/1, Step: 355/508, Lr: , Loss: 0.734031, Time/step: 7.496032
08/18/2025 16:47:49 - INFO -   Epoch: 1/1, Step: 360/508, Lr: , Loss: 0.616416, Time/step: 7.968366
08/18/2025 16:48:37 - INFO -   Epoch: 1/1, Step: 365/508, Lr: , Loss: 0.891900, Time/step: 9.597493
08/18/2025 16:49:27 - INFO -   Epoch: 1/1, Step: 370/508, Lr: , Loss: 0.948648, Time/step: 10.015306
08/18/2025 16:50:02 - INFO -   Epoch: 1/1, Step: 375/508, Lr: , Loss: 1.000873, Time/step: 6.878575
08/18/2025 16:50:49 - INFO -   Epoch: 1/1, Step: 380/508, Lr: , Loss: 1.033465, Time/step: 9.437341
08/18/2025 16:51:39 - INFO -   Epoch: 1/1, Step: 385/508, Lr: , Loss: 0.975889, Time/step: 10.050116
08/18/2025 16:52:24 - INFO -   Epoch: 1/1, Step: 390/508, Lr: , Loss: 1.018377, Time/step: 8.921432
08/18/2025 16:52:58 - INFO -   Epoch: 1/1, Step: 395/508, Lr: , Loss: 0.700123, Time/step: 6.808684
08/18/2025 16:53:42 - INFO -   Epoch: 1/1, Step: 400/508, Lr: , Loss: 0.813596, Time/step: 8.916985
08/18/2025 16:54:59 - INFO -   Epoch: 1/1, Step: 405/508, Lr: , Loss: 0.907262, Time/step: 15.350400
08/18/2025 16:55:22 - INFO -   Epoch: 1/1, Step: 410/508, Lr: , Loss: 0.872037, Time/step: 4.598830
08/18/2025 16:56:03 - INFO -   Epoch: 1/1, Step: 415/508, Lr: , Loss: 0.864839, Time/step: 8.135791
08/18/2025 16:56:43 - INFO -   Epoch: 1/1, Step: 420/508, Lr: , Loss: 1.106328, Time/step: 8.121195
08/18/2025 16:57:38 - INFO -   Epoch: 1/1, Step: 425/508, Lr: , Loss: 0.903260, Time/step: 10.925215
08/18/2025 16:58:29 - INFO -   Epoch: 1/1, Step: 430/508, Lr: , Loss: 1.147863, Time/step: 10.114933
08/18/2025 16:59:08 - INFO -   Epoch: 1/1, Step: 435/508, Lr: , Loss: 0.917610, Time/step: 7.789293
08/18/2025 16:59:47 - INFO -   Epoch: 1/1, Step: 440/508, Lr: , Loss: 1.197439, Time/step: 7.818933
08/18/2025 17:00:28 - INFO -   Epoch: 1/1, Step: 445/508, Lr: , Loss: 0.928324, Time/step: 8.251609
08/18/2025 17:01:17 - INFO -   Epoch: 1/1, Step: 450/508, Lr: , Loss: 0.853982, Time/step: 9.836006
08/18/2025 17:02:00 - INFO -   Epoch: 1/1, Step: 455/508, Lr: , Loss: 0.801849, Time/step: 8.554073
08/18/2025 17:02:33 - INFO -   Epoch: 1/1, Step: 460/508, Lr: , Loss: 0.738677, Time/step: 6.715594
08/18/2025 17:03:16 - INFO -   Epoch: 1/1, Step: 465/508, Lr: , Loss: 0.860306, Time/step: 8.466506
08/18/2025 17:04:09 - INFO -   Epoch: 1/1, Step: 470/508, Lr: , Loss: 0.952627, Time/step: 10.598613
08/18/2025 17:04:46 - INFO -   Epoch: 1/1, Step: 475/508, Lr: , Loss: 1.137571, Time/step: 7.384922
08/18/2025 17:05:34 - INFO -   Epoch: 1/1, Step: 480/508, Lr: , Loss: 0.892742, Time/step: 9.632056
08/18/2025 17:06:45 - INFO -   Epoch: 1/1, Step: 485/508, Lr: , Loss: 0.857919, Time/step: 14.289471
08/18/2025 17:07:41 - INFO -   Epoch: 1/1, Step: 490/508, Lr: , Loss: 0.812034, Time/step: 11.173456
08/18/2025 17:08:26 - INFO -   Epoch: 1/1, Step: 495/508, Lr: , Loss: 0.908324, Time/step: 9.054442
08/18/2025 17:09:13 - INFO -   Epoch: 1/1, Step: 500/508, Lr: , Loss: 0.892845, Time/step: 9.402389
08/18/2025 17:10:01 - INFO -   Epoch: 1/1, Step: 505/508, Lr: , Loss: 0.867793, Time/step: 9.504185
08/18/2025 17:10:33 - INFO -   Epoch 1/1 Finished, Train Loss: 1.220704
08/18/2025 17:10:43 - INFO -   Model saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0
08/18/2025 17:10:43 - INFO -   Optimizer saved to ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_opt.bin.0
08/18/2025 17:10:43 - INFO -   Eval on val dataset
08/18/2025 17:10:44 - WARNING -   Eval under the multi-sentence per video clip setting.
08/18/2025 17:10:44 - WARNING -   sentence num: 4290, video num: 100
0/1351/1352/1353/1354/1355/1356/1357/1358/1359/13510/13511/13512/13513/13514/13515/13516/13517/13518/13519/13520/13521/13522/13523/13524/13525/13526/13527/13528/13529/13530/13531/13532/13533/13534/13535/13536/13537/13538/13539/13540/13541/13542/13543/13544/13545/13546/13547/13548/13549/13550/13551/13552/13553/13554/13555/13556/13557/13558/13559/13560/13561/13562/13563/13564/13565/13566/13567/13568/13569/13570/13571/13572/13573/13574/13575/13576/13577/13578/13579/13580/13581/13582/13583/13584/13585/13586/13587/13588/13589/13590/13591/13592/13593/13594/13595/13596/13597/13598/13599/135100/135101/135102/135103/135104/135105/135106/135107/135108/135109/135110/135111/135112/135113/135114/135115/135116/135117/135118/135119/135120/135121/135122/135123/135124/135125/135126/135127/135128/135129/135130/135131/135132/135133/135134/13508/18/2025 17:25:09 - INFO -   before reshape, sim matrix size: 4290 x 100
08/18/2025 17:25:09 - INFO -   after reshape, sim matrix size: 100 x 62 x 100
08/18/2025 17:25:10 - INFO -   Text-to-Video:
08/18/2025 17:25:10 - INFO -   	>>>  R@1: 59.0 - R@5: 87.2 - R@10: 92.8 - Median R: 1.0 - Mean R: 3.7
08/18/2025 17:25:10 - INFO -   Video-to-Text:
08/18/2025 17:25:10 - INFO -   	>>>  V2T$R@1: 78.6 - V2T$R@5: 97.1 - V2T$R@10: 99.0 - V2T$Median R: 1.0 - V2T$Mean R: 1.6
08/18/2025 17:25:10 - INFO -   The best model is: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0, the R1 is: 58.9977
/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py:261: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.
  model_state_dict = torch.load(model_file, map_location='cpu')
08/18/2025 17:25:13 - INFO -   Model loaded from ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1/pytorch_model.bin.0
08/18/2025 17:25:19 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/18/2025 17:25:19 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/18/2025 17:25:19 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/18/2025 17:25:19 - WARNING -   Stage-One:True, Stage-Two:False
08/18/2025 17:25:19 - WARNING -   Test retrieval by loose type.
08/18/2025 17:25:19 - WARNING -   	 embed_dim: 512
08/18/2025 17:25:19 - WARNING -   	 image_resolution: 224
08/18/2025 17:25:19 - WARNING -   	 vision_layers: 12
08/18/2025 17:25:19 - WARNING -   	 vision_width: 768
08/18/2025 17:25:19 - WARNING -   	 vision_patch_size: 32
08/18/2025 17:25:19 - WARNING -   	 context_length: 77
08/18/2025 17:25:19 - WARNING -   	 vocab_size: 49408
08/18/2025 17:25:19 - WARNING -   	 transformer_width: 512
08/18/2025 17:25:19 - WARNING -   	 transformer_heads: 8
08/18/2025 17:25:19 - WARNING -   	 transformer_layers: 12
08/18/2025 17:25:19 - WARNING -   		 linear_patch: 2d
08/18/2025 17:25:19 - WARNING -   	 cut_top_layer: 0
08/18/2025 17:25:21 - WARNING -   	 sim_header: meanP
08/18/2025 17:25:26 - INFO -   --------------------
08/18/2025 17:25:26 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/18/2025 17:25:26 - WARNING -   Eval under the multi-sentence per video clip setting.
08/18/2025 17:25:26 - WARNING -   sentence num: 27763, video num: 670
0/8681/8682/868W0819 08:24:27.708320 140129439930176 torch/distributed/run.py:779] 
W0819 08:24:27.708320 140129439930176 torch/distributed/run.py:779] *****************************************
W0819 08:24:27.708320 140129439930176 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0819 08:24:27.708320 140129439930176 torch/distributed/run.py:779] *****************************************
08/19/2025 08:24:30 - INFO -   device: cuda:1 n_gpu: 2
08/19/2025 08:24:30 - INFO -   Effective parameters:
08/19/2025 08:24:30 - INFO -     <<< batch_size: 96
08/19/2025 08:24:30 - INFO -     <<< batch_size_val: 32
08/19/2025 08:24:30 - INFO -     <<< cache_dir: 
08/19/2025 08:24:30 - INFO -     <<< coef_lr: 0.001
08/19/2025 08:24:30 - INFO -     <<< cross_model: cross-base
08/19/2025 08:24:30 - INFO -     <<< cross_num_hidden_layers: 4
08/19/2025 08:24:30 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/19/2025 08:24:30 - INFO -     <<< datatype: msvd
08/19/2025 08:24:30 - INFO -     <<< do_eval: False
08/19/2025 08:24:30 - INFO -     <<< do_lower_case: False
08/19/2025 08:24:30 - INFO -     <<< do_pretrain: False
08/19/2025 08:24:30 - INFO -     <<< do_train: True
08/19/2025 08:24:30 - INFO -     <<< epochs: 1
08/19/2025 08:24:30 - INFO -     <<< eval_frame_order: 0
08/19/2025 08:24:30 - INFO -     <<< expand_msrvtt_sentences: False
08/19/2025 08:24:30 - INFO -     <<< feature_framerate: 1
08/19/2025 08:24:30 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/19/2025 08:24:30 - INFO -     <<< freeze_layer_num: 9
08/19/2025 08:24:30 - INFO -     <<< gradient_accumulation_steps: 1
08/19/2025 08:24:30 - INFO -     <<< hard_negative_rate: 0.5
08/19/2025 08:24:30 - INFO -     <<< init_model: None
08/19/2025 08:24:30 - INFO -     <<< linear_patch: 2d
08/19/2025 08:24:30 - INFO -     <<< local_rank: 0
08/19/2025 08:24:30 - INFO -     <<< loose_type: True
08/19/2025 08:24:30 - INFO -     <<< lr: 0.0001
08/19/2025 08:24:30 - INFO -     <<< lr_decay: 0.9
08/19/2025 08:24:30 - INFO -     <<< margin: 0.1
08/19/2025 08:24:30 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/19/2025 08:24:30 - INFO -     <<< max_frames: 12
08/19/2025 08:24:30 - INFO -     <<< max_words: 32
08/19/2025 08:24:30 - INFO -     <<< n_display: 5
08/19/2025 08:24:30 - INFO -     <<< n_gpu: 1
08/19/2025 08:24:30 - INFO -     <<< n_pair: 1
08/19/2025 08:24:30 - INFO -     <<< negative_weighting: 1
08/19/2025 08:24:30 - INFO -     <<< new_added_modules: ['Adapter']
08/19/2025 08:24:30 - INFO -     <<< num_thread_reader: 4
08/19/2025 08:24:30 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/19/2025 08:24:30 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/19/2025 08:24:30 - INFO -     <<< rank: 0
08/19/2025 08:24:30 - INFO -     <<< resume_model: None
08/19/2025 08:24:30 - INFO -     <<< sampled_use_mil: False
08/19/2025 08:24:30 - INFO -     <<< seed: 42
08/19/2025 08:24:30 - INFO -     <<< sim_header: meanP
08/19/2025 08:24:30 - INFO -     <<< slice_framepos: 0
08/19/2025 08:24:30 - INFO -     <<< task_type: retrieval
08/19/2025 08:24:30 - INFO -     <<< text_num_hidden_layers: 12
08/19/2025 08:24:30 - INFO -     <<< train_csv: data/.train.csv
08/19/2025 08:24:30 - INFO -     <<< train_frame_order: 0
08/19/2025 08:24:30 - INFO -     <<< use_mil: False
08/19/2025 08:24:30 - INFO -     <<< val_csv: data/.val.csv
08/19/2025 08:24:30 - INFO -     <<< video_dim: 1024
08/19/2025 08:24:30 - INFO -     <<< visual_num_hidden_layers: 12
08/19/2025 08:24:30 - INFO -     <<< warmup_proportion: 0.1
08/19/2025 08:24:30 - INFO -     <<< world_size: 2
08/19/2025 08:24:30 - INFO -   device: cuda:0 n_gpu: 2
08/19/2025 08:24:33 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/19/2025 08:24:33 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/19/2025 08:24:33 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/19/2025 08:24:33 - WARNING -   Stage-One:True, Stage-Two:False
08/19/2025 08:24:33 - WARNING -   Test retrieval by loose type.
08/19/2025 08:24:33 - WARNING -   	 embed_dim: 512
08/19/2025 08:24:33 - WARNING -   	 image_resolution: 224
08/19/2025 08:24:33 - WARNING -   	 vision_layers: 12
08/19/2025 08:24:33 - WARNING -   	 vision_width: 768
08/19/2025 08:24:33 - WARNING -   	 vision_patch_size: 32
08/19/2025 08:24:33 - WARNING -   	 context_length: 77
08/19/2025 08:24:33 - WARNING -   	 vocab_size: 49408
08/19/2025 08:24:33 - WARNING -   	 transformer_width: 512
08/19/2025 08:24:33 - WARNING -   	 transformer_heads: 8
08/19/2025 08:24:33 - WARNING -   	 transformer_layers: 12
08/19/2025 08:24:33 - WARNING -   		 linear_patch: 2d
08/19/2025 08:24:33 - WARNING -   	 cut_top_layer: 0
08/19/2025 08:24:35 - WARNING -   	 sim_header: meanP
08/19/2025 08:24:40 - INFO -   --------------------
08/19/2025 08:24:40 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/19/2025 08:24:40 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/19/2025 08:24:40 - WARNING -   GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/19/2025 08:24:41 - INFO -   ***** Running test *****
08/19/2025 08:24:41 - INFO -     Num examples = 27763
08/19/2025 08:24:41 - INFO -     Batch size = 32
08/19/2025 08:24:41 - INFO -     Num steps = 868
08/19/2025 08:24:41 - INFO -   ***** Running val *****
08/19/2025 08:24:41 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/19/2025 08:24:41 - INFO -   ***** Running training *****
08/19/2025 08:24:41 - INFO -     Num examples = 48774
08/19/2025 08:24:41 - INFO -     Batch size = 96
08/19/2025 08:24:41 - INFO -     Num steps = 508
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank0]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank0]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 401, in get_sequence_visual_output
[rank0]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 337, in get_sequence_output
[rank0]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank0]:     x = self.transformer(x)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank0]:     return self.resblocks((x, video_frame))[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank0]:     input = module(input)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank0]:     attn_output = self.attention(attn_input)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank0]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank0]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank0]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank0]:     proj = linear(q, w, b)
[rank0]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank1]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank1]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 401, in get_sequence_visual_output
[rank1]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 337, in get_sequence_output
[rank1]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank1]:     x = self.transformer(x)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank1]:     return self.resblocks((x, video_frame))[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank1]:     input = module(input)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank1]:     attn_output = self.attention(attn_input)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank1]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank1]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank1]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank1]:     proj = linear(q, w, b)
[rank1]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
W0819 08:25:53.079217 140129439930176 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 32486 closing signal SIGTERM
E0819 08:25:54.946216 140129439930176 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 32487) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_08:25:53
  host      : localhost.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 32487)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0819 08:29:33.214706 140567324096320 torch/distributed/run.py:779] 
W0819 08:29:33.214706 140567324096320 torch/distributed/run.py:779] *****************************************
W0819 08:29:33.214706 140567324096320 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0819 08:29:33.214706 140567324096320 torch/distributed/run.py:779] *****************************************
08/19/2025 08:29:36 - INFO -   device: cuda:1 n_gpu: 2
08/19/2025 08:29:36 - INFO -   Effective parameters:
08/19/2025 08:29:36 - INFO -     <<< batch_size: 96
08/19/2025 08:29:36 - INFO -     <<< batch_size_val: 32
08/19/2025 08:29:36 - INFO -     <<< cache_dir: 
08/19/2025 08:29:36 - INFO -     <<< coef_lr: 0.001
08/19/2025 08:29:36 - INFO -     <<< cross_model: cross-base
08/19/2025 08:29:36 - INFO -     <<< cross_num_hidden_layers: 4
08/19/2025 08:29:36 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/19/2025 08:29:36 - INFO -     <<< datatype: msvd
08/19/2025 08:29:36 - INFO -     <<< do_eval: False
08/19/2025 08:29:36 - INFO -     <<< do_lower_case: False
08/19/2025 08:29:36 - INFO -     <<< do_pretrain: False
08/19/2025 08:29:36 - INFO -     <<< do_train: True
08/19/2025 08:29:36 - INFO -     <<< epochs: 1
08/19/2025 08:29:36 - INFO -     <<< eval_frame_order: 0
08/19/2025 08:29:36 - INFO -     <<< expand_msrvtt_sentences: False
08/19/2025 08:29:36 - INFO -     <<< feature_framerate: 1
08/19/2025 08:29:36 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/19/2025 08:29:36 - INFO -     <<< freeze_layer_num: 9
08/19/2025 08:29:36 - INFO -     <<< gradient_accumulation_steps: 1
08/19/2025 08:29:36 - INFO -     <<< hard_negative_rate: 0.5
08/19/2025 08:29:36 - INFO -     <<< init_model: None
08/19/2025 08:29:36 - INFO -     <<< linear_patch: 2d
08/19/2025 08:29:36 - INFO -     <<< local_rank: 0
08/19/2025 08:29:36 - INFO -     <<< loose_type: True
08/19/2025 08:29:36 - INFO -     <<< lr: 0.0001
08/19/2025 08:29:36 - INFO -     <<< lr_decay: 0.9
08/19/2025 08:29:36 - INFO -     <<< margin: 0.1
08/19/2025 08:29:36 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/19/2025 08:29:36 - INFO -     <<< max_frames: 12
08/19/2025 08:29:36 - INFO -     <<< max_words: 32
08/19/2025 08:29:36 - INFO -     <<< n_display: 5
08/19/2025 08:29:36 - INFO -     <<< n_gpu: 1
08/19/2025 08:29:36 - INFO -     <<< n_pair: 1
08/19/2025 08:29:36 - INFO -     <<< negative_weighting: 1
08/19/2025 08:29:36 - INFO -     <<< new_added_modules: ['Adapter']
08/19/2025 08:29:36 - INFO -     <<< num_thread_reader: 4
08/19/2025 08:29:36 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/19/2025 08:29:36 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/19/2025 08:29:36 - INFO -     <<< rank: 0
08/19/2025 08:29:36 - INFO -     <<< resume_model: None
08/19/2025 08:29:36 - INFO -     <<< sampled_use_mil: False
08/19/2025 08:29:36 - INFO -     <<< seed: 42
08/19/2025 08:29:36 - INFO -     <<< sim_header: meanP
08/19/2025 08:29:36 - INFO -     <<< slice_framepos: 0
08/19/2025 08:29:36 - INFO -     <<< task_type: retrieval
08/19/2025 08:29:36 - INFO -     <<< text_num_hidden_layers: 12
08/19/2025 08:29:36 - INFO -     <<< train_csv: data/.train.csv
08/19/2025 08:29:36 - INFO -     <<< train_frame_order: 0
08/19/2025 08:29:36 - INFO -     <<< use_mil: False
08/19/2025 08:29:36 - INFO -     <<< val_csv: data/.val.csv
08/19/2025 08:29:36 - INFO -     <<< video_dim: 1024
08/19/2025 08:29:36 - INFO -     <<< visual_num_hidden_layers: 12
08/19/2025 08:29:36 - INFO -     <<< warmup_proportion: 0.1
08/19/2025 08:29:36 - INFO -     <<< world_size: 2
08/19/2025 08:29:36 - INFO -   device: cuda:0 n_gpu: 2
08/19/2025 08:29:37 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/19/2025 08:29:37 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/19/2025 08:29:37 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/19/2025 08:29:37 - WARNING -   Stage-One:True, Stage-Two:False
08/19/2025 08:29:37 - WARNING -   Test retrieval by loose type.
08/19/2025 08:29:37 - WARNING -   	 embed_dim: 512
08/19/2025 08:29:37 - WARNING -   	 image_resolution: 224
08/19/2025 08:29:37 - WARNING -   	 vision_layers: 12
08/19/2025 08:29:37 - WARNING -   	 vision_width: 768
08/19/2025 08:29:37 - WARNING -   	 vision_patch_size: 32
08/19/2025 08:29:37 - WARNING -   	 context_length: 77
08/19/2025 08:29:37 - WARNING -   	 vocab_size: 49408
08/19/2025 08:29:37 - WARNING -   	 transformer_width: 512
08/19/2025 08:29:37 - WARNING -   	 transformer_heads: 8
08/19/2025 08:29:37 - WARNING -   	 transformer_layers: 12
08/19/2025 08:29:37 - WARNING -   		 linear_patch: 2d
08/19/2025 08:29:37 - WARNING -   	 cut_top_layer: 0
08/19/2025 08:29:38 - WARNING -   	 sim_header: meanP
08/19/2025 08:29:43 - INFO -   --------------------
08/19/2025 08:29:43 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/19/2025 08:29:43 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/19/2025 08:29:43 - WARNING -   GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/19/2025 08:29:44 - INFO -   ***** Running test *****
08/19/2025 08:29:44 - INFO -     Num examples = 27763
08/19/2025 08:29:44 - INFO -     Batch size = 32
08/19/2025 08:29:44 - INFO -     Num steps = 868
08/19/2025 08:29:44 - INFO -   ***** Running val *****
08/19/2025 08:29:44 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/19/2025 08:29:44 - INFO -   ***** Running training *****
08/19/2025 08:29:44 - INFO -     Num examples = 48774
08/19/2025 08:29:44 - INFO -     Batch size = 96
08/19/2025 08:29:44 - INFO -     Num steps = 508
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank1]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank1]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 395, in get_sequence_visual_output
[rank1]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 331, in get_sequence_output
[rank1]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank1]:     x = self.transformer(x)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank1]:     return self.resblocks((x, video_frame))[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank1]:     input = module(input)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank1]:     attn_output = self.attention(attn_input)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank1]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank1]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank1]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank1]:     proj = linear(q, w, b)
[rank1]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank0]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank0]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 395, in get_sequence_visual_output
[rank0]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 331, in get_sequence_output
[rank0]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank0]:     x = self.transformer(x)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank0]:     return self.resblocks((x, video_frame))[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank0]:     input = module(input)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank0]:     attn_output = self.attention(attn_input)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank0]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank0]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank0]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank0]:     proj = linear(q, w, b)
[rank0]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
W0819 08:30:51.349843 140567324096320 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 51123 closing signal SIGTERM
E0819 08:30:54.318754 140567324096320 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 51124) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_08:30:51
  host      : localhost.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 51124)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0819 08:34:57.737473 140584314361664 torch/distributed/run.py:779] 
W0819 08:34:57.737473 140584314361664 torch/distributed/run.py:779] *****************************************
W0819 08:34:57.737473 140584314361664 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0819 08:34:57.737473 140584314361664 torch/distributed/run.py:779] *****************************************
08/19/2025 08:35:00 - INFO -   device: cuda:1 n_gpu: 2
08/19/2025 08:35:00 - INFO -   Effective parameters:
08/19/2025 08:35:00 - INFO -     <<< batch_size: 96
08/19/2025 08:35:00 - INFO -     <<< batch_size_val: 32
08/19/2025 08:35:00 - INFO -     <<< cache_dir: 
08/19/2025 08:35:00 - INFO -     <<< coef_lr: 0.001
08/19/2025 08:35:00 - INFO -     <<< cross_model: cross-base
08/19/2025 08:35:00 - INFO -     <<< cross_num_hidden_layers: 4
08/19/2025 08:35:00 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/19/2025 08:35:00 - INFO -     <<< datatype: msvd
08/19/2025 08:35:00 - INFO -     <<< do_eval: False
08/19/2025 08:35:00 - INFO -     <<< do_lower_case: False
08/19/2025 08:35:00 - INFO -     <<< do_pretrain: False
08/19/2025 08:35:00 - INFO -     <<< do_train: True
08/19/2025 08:35:00 - INFO -     <<< epochs: 1
08/19/2025 08:35:00 - INFO -     <<< eval_frame_order: 0
08/19/2025 08:35:00 - INFO -     <<< expand_msrvtt_sentences: False
08/19/2025 08:35:00 - INFO -     <<< feature_framerate: 1
08/19/2025 08:35:00 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/19/2025 08:35:00 - INFO -     <<< freeze_layer_num: 9
08/19/2025 08:35:00 - INFO -     <<< gradient_accumulation_steps: 1
08/19/2025 08:35:00 - INFO -     <<< hard_negative_rate: 0.5
08/19/2025 08:35:00 - INFO -     <<< init_model: None
08/19/2025 08:35:00 - INFO -     <<< linear_patch: 2d
08/19/2025 08:35:00 - INFO -     <<< local_rank: 0
08/19/2025 08:35:00 - INFO -     <<< loose_type: True
08/19/2025 08:35:00 - INFO -     <<< lr: 0.0001
08/19/2025 08:35:00 - INFO -     <<< lr_decay: 0.9
08/19/2025 08:35:00 - INFO -     <<< margin: 0.1
08/19/2025 08:35:00 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/19/2025 08:35:00 - INFO -     <<< max_frames: 12
08/19/2025 08:35:00 - INFO -     <<< max_words: 32
08/19/2025 08:35:00 - INFO -     <<< n_display: 5
08/19/2025 08:35:00 - INFO -     <<< n_gpu: 1
08/19/2025 08:35:00 - INFO -     <<< n_pair: 1
08/19/2025 08:35:00 - INFO -     <<< negative_weighting: 1
08/19/2025 08:35:00 - INFO -     <<< new_added_modules: ['Adapter']
08/19/2025 08:35:00 - INFO -     <<< num_thread_reader: 4
08/19/2025 08:35:00 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/19/2025 08:35:00 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/19/2025 08:35:00 - INFO -     <<< rank: 0
08/19/2025 08:35:00 - INFO -     <<< resume_model: None
08/19/2025 08:35:00 - INFO -     <<< sampled_use_mil: False
08/19/2025 08:35:00 - INFO -     <<< seed: 42
08/19/2025 08:35:00 - INFO -     <<< sim_header: meanP
08/19/2025 08:35:00 - INFO -     <<< slice_framepos: 0
08/19/2025 08:35:00 - INFO -     <<< task_type: retrieval
08/19/2025 08:35:00 - INFO -     <<< text_num_hidden_layers: 12
08/19/2025 08:35:00 - INFO -     <<< train_csv: data/.train.csv
08/19/2025 08:35:00 - INFO -     <<< train_frame_order: 0
08/19/2025 08:35:00 - INFO -     <<< use_mil: False
08/19/2025 08:35:00 - INFO -     <<< val_csv: data/.val.csv
08/19/2025 08:35:00 - INFO -     <<< video_dim: 1024
08/19/2025 08:35:00 - INFO -     <<< visual_num_hidden_layers: 12
08/19/2025 08:35:00 - INFO -     <<< warmup_proportion: 0.1
08/19/2025 08:35:00 - INFO -     <<< world_size: 2
08/19/2025 08:35:00 - INFO -   device: cuda:0 n_gpu: 2
08/19/2025 08:35:01 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/19/2025 08:35:01 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/19/2025 08:35:01 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/19/2025 08:35:01 - WARNING -   Stage-One:True, Stage-Two:False
08/19/2025 08:35:01 - WARNING -   Test retrieval by loose type.
08/19/2025 08:35:01 - WARNING -   	 embed_dim: 512
08/19/2025 08:35:01 - WARNING -   	 image_resolution: 224
08/19/2025 08:35:01 - WARNING -   	 vision_layers: 12
08/19/2025 08:35:01 - WARNING -   	 vision_width: 768
08/19/2025 08:35:01 - WARNING -   	 vision_patch_size: 32
08/19/2025 08:35:01 - WARNING -   	 context_length: 77
08/19/2025 08:35:01 - WARNING -   	 vocab_size: 49408
08/19/2025 08:35:01 - WARNING -   	 transformer_width: 512
08/19/2025 08:35:01 - WARNING -   	 transformer_heads: 8
08/19/2025 08:35:01 - WARNING -   	 transformer_layers: 12
08/19/2025 08:35:01 - WARNING -   		 linear_patch: 2d
08/19/2025 08:35:01 - WARNING -   	 cut_top_layer: 0
08/19/2025 08:35:03 - WARNING -   	 sim_header: meanP
08/19/2025 08:35:07 - INFO -   --------------------
08/19/2025 08:35:07 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/19/2025 08:35:07 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/19/2025 08:35:08 - WARNING -   GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/19/2025 08:35:08 - INFO -   ***** Running test *****
08/19/2025 08:35:08 - INFO -     Num examples = 27763
08/19/2025 08:35:08 - INFO -     Batch size = 32
08/19/2025 08:35:08 - INFO -     Num steps = 868
08/19/2025 08:35:08 - INFO -   ***** Running val *****
08/19/2025 08:35:08 - INFO -     Num examples = 4290
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
Video number: 1200
Total Paire: 48774
Video number: 1200
Total Paire: 48774
08/19/2025 08:35:09 - INFO -   ***** Running training *****
08/19/2025 08:35:09 - INFO -     Num examples = 48774
08/19/2025 08:35:09 - INFO -     Batch size = 96
08/19/2025 08:35:09 - INFO -     Num steps = 508
[rank1]: Traceback (most recent call last):
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank1]:     main()
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank1]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank1]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank1]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank1]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank1]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 395, in get_sequence_visual_output
[rank1]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 331, in get_sequence_output
[rank1]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank1]:     x = self.transformer(x)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank1]:     return self.resblocks((x, video_frame))[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank1]:     input = module(input)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank1]:     attn_output = self.attention(attn_input)
[rank1]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank1]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank1]:     return self._call_impl(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank1]:     return forward_call(*args, **kwargs)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank1]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank1]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank1]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank1]:     proj = linear(q, w, b)
[rank1]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
[rank0]: Traceback (most recent call last):
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 454, in <module>
[rank0]:     main()
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 433, in main
[rank0]:     tr_loss, global_step = train_epoch(epoch, args, model, train_dataloader, device, n_gpu, optimizer,
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/main_xclip.py", line 219, in train_epoch
[rank0]:     loss = model(input_ids, segment_ids, input_mask, video, res, mv, video_mask, mv_mask, proportion)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1636, in forward
[rank0]:     else self._run_ddp_forward(*inputs, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/parallel/distributed.py", line 1454, in _run_ddp_forward
[rank0]:     return self.module(*inputs, **kwargs)  # type: ignore[index]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 269, in forward
[rank0]:     (sequence_output, seq_features), visual_output = self.get_sequence_visual_output(input_ids, token_type_ids, attention_mask, 
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 395, in get_sequence_visual_output
[rank0]:     sequence_output, seq_features = self.get_sequence_output(input_ids, token_type_ids, attention_mask, shaped=True) # [bs, 1, dim], [bs, num_words, dim]
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/modeling_xclip.py", line 331, in get_sequence_output
[rank0]:     sequence_hidden, seq_features = self.clip.encode_text(input_ids, return_hidden=True)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 563, in encode_text
[rank0]:     x = self.transformer(x)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 284, in forward
[rank0]:     return self.resblocks((x, video_frame))[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/container.py", line 219, in forward
[rank0]:     input = module(input)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 270, in forward
[rank0]:     attn_output = self.attention(attn_input)
[rank0]:   File "/home/wa24301158/mywork/newX-CLIP-main/modules/module_clip.py", line 262, in attention
[rank0]:     return self.attn(x, x, x, need_weights=False, attn_mask=attn_mask_)[0]
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1553, in _wrapped_call_impl
[rank0]:     return self._call_impl(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/module.py", line 1562, in _call_impl
[rank0]:     return forward_call(*args, **kwargs)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/modules/activation.py", line 1275, in forward
[rank0]:     attn_output, attn_output_weights = F.multi_head_attention_forward(
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 5420, in multi_head_attention_forward
[rank0]:     q, k, v = _in_projection_packed(query, key, value, in_proj_weight, in_proj_bias)
[rank0]:   File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/nn/functional.py", line 4920, in _in_projection_packed
[rank0]:     proj = linear(q, w, b)
[rank0]: RuntimeError: mat1 and mat2 must have the same dtype, but got Float and Half
W0819 08:36:18.504011 140584314361664 torch/distributed/elastic/multiprocessing/api.py:858] Sending process 70527 closing signal SIGTERM
E0819 08:36:20.421016 140584314361664 torch/distributed/elastic/multiprocessing/api.py:833] failed (exitcode: 1) local_rank: 1 (pid: 70528) of binary: /home/wa24301158/conda_envs/dgl/bin/python3.9
Traceback (most recent call last):
  File "/home/wa24301158/conda_envs/dgl/bin/torchrun", line 8, in <module>
    sys.exit(main())
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/elastic/multiprocessing/errors/__init__.py", line 348, in wrapper
    return f(*args, **kwargs)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 901, in main
    run(args)
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/run.py", line 892, in run
    elastic_launch(
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 133, in __call__
    return launch_agent(self._config, self._entrypoint, list(args))
  File "/home/wa24301158/conda_envs/dgl/lib/python3.9/site-packages/torch/distributed/launcher/api.py", line 264, in launch_agent
    raise ChildFailedError(
torch.distributed.elastic.multiprocessing.errors.ChildFailedError: 
============================================================
main_xclip.py FAILED
------------------------------------------------------------
Failures:
  <NO_OTHER_FAILURES>
------------------------------------------------------------
Root Cause (first observed failure):
[0]:
  time      : 2025-08-19_08:36:18
  host      : localhost.localdomain
  rank      : 1 (local_rank: 1)
  exitcode  : 1 (pid: 70528)
  error_file: <N/A>
  traceback : To enable traceback see: https://pytorch.org/docs/stable/elastic/errors.html
============================================================
W0819 08:42:11.662732 139661533771584 torch/distributed/run.py:779] 
W0819 08:42:11.662732 139661533771584 torch/distributed/run.py:779] *****************************************
W0819 08:42:11.662732 139661533771584 torch/distributed/run.py:779] Setting OMP_NUM_THREADS environment variable for each process to be 1 in default, to avoid your system being overloaded, please further tune the variable for optimal performance in your application as needed. 
W0819 08:42:11.662732 139661533771584 torch/distributed/run.py:779] *****************************************
08/19/2025 08:42:14 - INFO -   device: cuda:1 n_gpu: 2
08/19/2025 08:42:14 - INFO -   Effective parameters:
08/19/2025 08:42:14 - INFO -     <<< batch_size: 96
08/19/2025 08:42:14 - INFO -     <<< batch_size_val: 32
08/19/2025 08:42:14 - INFO -     <<< cache_dir: 
08/19/2025 08:42:14 - INFO -     <<< coef_lr: 0.001
08/19/2025 08:42:14 - INFO -     <<< cross_model: cross-base
08/19/2025 08:42:14 - INFO -     <<< cross_num_hidden_layers: 4
08/19/2025 08:42:14 - INFO -     <<< data_path: /home/wa24301158/dataset/MSVD
08/19/2025 08:42:14 - INFO -     <<< datatype: msvd
08/19/2025 08:42:14 - INFO -     <<< do_eval: False
08/19/2025 08:42:14 - INFO -     <<< do_lower_case: False
08/19/2025 08:42:14 - INFO -     <<< do_pretrain: False
08/19/2025 08:42:14 - INFO -     <<< do_train: True
08/19/2025 08:42:14 - INFO -     <<< epochs: 1
08/19/2025 08:42:14 - INFO -     <<< eval_frame_order: 0
08/19/2025 08:42:14 - INFO -     <<< expand_msrvtt_sentences: False
08/19/2025 08:42:14 - INFO -     <<< feature_framerate: 1
08/19/2025 08:42:14 - INFO -     <<< features_path: /home/wa24301158/dataset/MSVD/msvd_hevc
08/19/2025 08:42:14 - INFO -     <<< freeze_layer_num: 9
08/19/2025 08:42:14 - INFO -     <<< gradient_accumulation_steps: 1
08/19/2025 08:42:14 - INFO -     <<< hard_negative_rate: 0.5
08/19/2025 08:42:14 - INFO -     <<< init_model: None
08/19/2025 08:42:14 - INFO -     <<< linear_patch: 2d
08/19/2025 08:42:14 - INFO -     <<< local_rank: 0
08/19/2025 08:42:14 - INFO -     <<< loose_type: True
08/19/2025 08:42:14 - INFO -     <<< lr: 0.0001
08/19/2025 08:42:14 - INFO -     <<< lr_decay: 0.9
08/19/2025 08:42:14 - INFO -     <<< margin: 0.1
08/19/2025 08:42:14 - INFO -     <<< mask_path: /home/wa24301158/dataset/MSVD/videos_hevc_info
08/19/2025 08:42:14 - INFO -     <<< max_frames: 12
08/19/2025 08:42:14 - INFO -     <<< max_words: 32
08/19/2025 08:42:14 - INFO -     <<< n_display: 5
08/19/2025 08:42:14 - INFO -     <<< n_gpu: 1
08/19/2025 08:42:14 - INFO -     <<< n_pair: 1
08/19/2025 08:42:14 - INFO -     <<< negative_weighting: 1
08/19/2025 08:42:14 - INFO -     <<< new_added_modules: ['Adapter']
08/19/2025 08:42:14 - INFO -     <<< num_thread_reader: 4
08/19/2025 08:42:14 - INFO -     <<< output_dir: ckpts3/CCVTR_msvd_vit32_32_DL_0818_teacher_1
08/19/2025 08:42:14 - INFO -     <<< pretrained_clip_name: ViT-B/32
08/19/2025 08:42:14 - INFO -     <<< rank: 0
08/19/2025 08:42:14 - INFO -     <<< resume_model: None
08/19/2025 08:42:14 - INFO -     <<< sampled_use_mil: False
08/19/2025 08:42:14 - INFO -     <<< seed: 42
08/19/2025 08:42:14 - INFO -     <<< sim_header: meanP
08/19/2025 08:42:14 - INFO -     <<< slice_framepos: 0
08/19/2025 08:42:14 - INFO -     <<< task_type: retrieval
08/19/2025 08:42:14 - INFO -     <<< text_num_hidden_layers: 12
08/19/2025 08:42:14 - INFO -     <<< train_csv: data/.train.csv
08/19/2025 08:42:14 - INFO -     <<< train_frame_order: 0
08/19/2025 08:42:14 - INFO -     <<< use_mil: False
08/19/2025 08:42:14 - INFO -     <<< val_csv: data/.val.csv
08/19/2025 08:42:14 - INFO -     <<< video_dim: 1024
08/19/2025 08:42:14 - INFO -     <<< visual_num_hidden_layers: 12
08/19/2025 08:42:14 - INFO -     <<< warmup_proportion: 0.1
08/19/2025 08:42:14 - INFO -     <<< world_size: 2
08/19/2025 08:42:14 - INFO -   device: cuda:0 n_gpu: 2
08/19/2025 08:42:15 - INFO -   loading archive file /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base
08/19/2025 08:42:15 - INFO -   Model config {
  "attention_probs_dropout_prob": 0.1,
  "hidden_act": "gelu",
  "hidden_dropout_prob": 0.1,
  "hidden_size": 512,
  "initializer_range": 0.02,
  "intermediate_size": 2048,
  "max_position_embeddings": 128,
  "num_attention_heads": 8,
  "num_hidden_layers": 4,
  "type_vocab_size": 2,
  "vocab_size": 512
}

08/19/2025 08:42:15 - INFO -   Weight doesn't exsits. /home/wa24301158/mywork/newX-CLIP-main/modules/cross-base/cross_pytorch_model.bin
08/19/2025 08:42:15 - WARNING -   Stage-One:True, Stage-Two:False
08/19/2025 08:42:15 - WARNING -   Test retrieval by loose type.
08/19/2025 08:42:15 - WARNING -   	 embed_dim: 512
08/19/2025 08:42:15 - WARNING -   	 image_resolution: 224
08/19/2025 08:42:15 - WARNING -   	 vision_layers: 12
08/19/2025 08:42:15 - WARNING -   	 vision_width: 768
08/19/2025 08:42:15 - WARNING -   	 vision_patch_size: 32
08/19/2025 08:42:15 - WARNING -   	 context_length: 77
08/19/2025 08:42:15 - WARNING -   	 vocab_size: 49408
08/19/2025 08:42:15 - WARNING -   	 transformer_width: 512
08/19/2025 08:42:15 - WARNING -   	 transformer_heads: 8
08/19/2025 08:42:15 - WARNING -   	 transformer_layers: 12
08/19/2025 08:42:15 - WARNING -   		 linear_patch: 2d
08/19/2025 08:42:15 - WARNING -   	 cut_top_layer: 0
08/19/2025 08:42:17 - WARNING -   	 sim_header: meanP
08/19/2025 08:42:21 - INFO -   --------------------
08/19/2025 08:42:21 - INFO -   Weights of XCLIP not initialized from pretrained model: 
   global_mat_weight
   word_logit_weight
   frame_logit_weight
   local_mat_weight
   frame_mat_weight
   word_mat_weight
   frame_mat_weight2
   word_mat_weight2
   teacher.motion_encoder.pos_embed
   teacher.motion_encoder.cls_token
   teacher.motion_encoder.patch_embed_i.weight
   teacher.motion_encoder.patch_embed_m.weight
   teacher.motion_encoder.patch_embed_r.weight
   teacher.motion_encoder.blocks.0.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.0.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.0.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.0.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.0.norm1_i.weight
   teacher.motion_encoder.blocks.0.norm1_m.weight
   teacher.motion_encoder.blocks.0.norm1_r.weight
   teacher.motion_encoder.blocks.0.norm2_i.weight
   teacher.motion_encoder.blocks.0.norm3_i.weight
   teacher.motion_encoder.blocks.0.ffn.w1.weight
   teacher.motion_encoder.blocks.0.ffn.w3.weight
   teacher.motion_encoder.blocks.0.ffn.w2.weight
   teacher.motion_encoder.blocks.1.self_attn_i.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_i.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_m.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_m.out_proj.weight
   teacher.motion_encoder.blocks.1.self_attn_r.in_proj_weight
   teacher.motion_encoder.blocks.1.self_attn_r.out_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.q_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.k_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.v_proj.weight
   teacher.motion_encoder.blocks.1.cross_attn.out_proj.weight
   teacher.motion_encoder.blocks.1.norm1_i.weight
   teacher.motion_encoder.blocks.1.norm1_m.weight
   teacher.motion_encoder.blocks.1.norm1_r.weight
   teacher.motion_encoder.blocks.1.norm2_i.weight
   teacher.motion_encoder.blocks.1.norm3_i.weight
   teacher.motion_encoder.blocks.1.ffn.w1.weight
   teacher.motion_encoder.blocks.1.ffn.w3.weight
   teacher.motion_encoder.blocks.1.ffn.w2.weight
   teacher.motion_encoder.norm.weight
   teacher.motion_encoder.head.weight
   teacher.motion_encoder.head.bias
   teacher.temporal_fusion.cross_attention_fusion.in_proj_weight
   teacher.temporal_fusion.cross_attention_fusion.out_proj.weight
   teacher.temporal_fusion.norm_i.weight
   teacher.temporal_fusion.norm_m.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.0.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.0.ffn.w2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.q_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.k_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.v_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.out_proj.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.attention.rotary_emb.inv_freq
   teacher.temporal_fusion.transformer_encoder_blocks.1.norm2.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w1.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w3.weight
   teacher.temporal_fusion.transformer_encoder_blocks.1.ffn.w2.weight
08/19/2025 08:42:21 - INFO -   Weights from pretrained model not used in XCLIP: 
   clip.input_resolution
   clip.context_length
   clip.vocab_size
08/19/2025 08:42:21 - WARNING -   GPU with CUDA Capability 6.0 is too old for torch.compile with triton backend. Required is >= 7.0. Skipping compilation.
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
For test, sentence number: 27763
For test, video number: 670
Video number: 670
Total Paire: 27763
Video number: 1200
Total Paire: 48774
For val, sentence number: 4290
For val, video number: 100
Video number: 100
Total Paire: 4290
08/19/2025 08:42:22 - INFO -   ***** Running test *****
08/19/2025 08:42:22 - INFO -     Num examples = 27763
08/19/2025 08:42:22 - INFO -     Batch size = 32
08/19/2025 08:42:22 - INFO -     Num steps = 868
08/19/2025 08:42:22 - INFO -   ***** Running val *****
08/19/2025 08:42:22 - INFO -     Num examples = 4290
Video number: 1200
Total Paire: 48774
08/19/2025 08:42:23 - INFO -   ***** Running training *****
08/19/2025 08:42:23 - INFO -     Num examples = 48774
08/19/2025 08:42:23 - INFO -     Batch size = 96
08/19/2025 08:42:23 - INFO -     Num steps = 508
08/19/2025 08:43:36 - INFO -   Epoch: 1/1, Step: 5/508, Lr: , Loss: 3.148196, Time/step: 14.632674
08/19/2025 08:44:07 - INFO -   Epoch: 1/1, Step: 10/508, Lr: , Loss: 3.029150, Time/step: 6.257489
08/19/2025 08:44:51 - INFO -   Epoch: 1/1, Step: 15/508, Lr: , Loss: 2.846721, Time/step: 8.747607
08/19/2025 08:45:24 - INFO -   Epoch: 1/1, Step: 20/508, Lr: , Loss: 2.532522, Time/step: 6.637057
08/19/2025 08:46:18 - INFO -   Epoch: 1/1, Step: 25/508, Lr: , Loss: 2.522690, Time/step: 10.740907
08/19/2025 08:46:57 - INFO -   Epoch: 1/1, Step: 30/508, Lr: , Loss: 2.342144, Time/step: 7.926583
08/19/2025 08:47:30 - INFO -   Epoch: 1/1, Step: 35/508, Lr: , Loss: 2.031947, Time/step: 6.624965
08/19/2025 08:48:07 - INFO -   Epoch: 1/1, Step: 40/508, Lr: , Loss: 1.934312, Time/step: 7.250966
08/19/2025 08:48:57 - INFO -   Epoch: 1/1, Step: 45/508, Lr: , Loss: 1.567915, Time/step: 10.117827
08/19/2025 08:49:31 - INFO -   Epoch: 1/1, Step: 50/508, Lr: , Loss: 1.603093, Time/step: 6.692225
08/19/2025 08:50:23 - INFO -   Epoch: 1/1, Step: 55/508, Lr: , Loss: 1.312273, Time/step: 10.428347
08/19/2025 08:51:07 - INFO -   Epoch: 1/1, Step: 60/508, Lr: , Loss: 1.226677, Time/step: 8.886742
08/19/2025 08:51:51 - INFO -   Epoch: 1/1, Step: 65/508, Lr: , Loss: 0.921959, Time/step: 8.801861
08/19/2025 08:52:23 - INFO -   Epoch: 1/1, Step: 70/508, Lr: , Loss: 0.859378, Time/step: 6.315342
08/19/2025 08:53:34 - INFO -   Epoch: 1/1, Step: 75/508, Lr: , Loss: 0.921617, Time/step: 14.223982
08/19/2025 08:54:08 - INFO -   Epoch: 1/1, Step: 80/508, Lr: , Loss: 0.668513, Time/step: 6.877268
08/19/2025 08:54:41 - INFO -   Epoch: 1/1, Step: 85/508, Lr: , Loss: 0.862821, Time/step: 6.498726
08/19/2025 08:55:14 - INFO -   Epoch: 1/1, Step: 90/508, Lr: , Loss: 0.786323, Time/step: 6.586047
08/19/2025 08:56:12 - INFO -   Epoch: 1/1, Step: 95/508, Lr: , Loss: 0.938177, Time/step: 11.688734
08/19/2025 08:56:44 - INFO -   Epoch: 1/1, Step: 100/508, Lr: , Loss: 0.885353, Time/step: 6.356236
08/19/2025 08:57:28 - INFO -   Epoch: 1/1, Step: 105/508, Lr: , Loss: 0.573392, Time/step: 8.871327
08/19/2025 08:58:00 - INFO -   Epoch: 1/1, Step: 110/508, Lr: , Loss: 0.731305, Time/step: 6.403610
